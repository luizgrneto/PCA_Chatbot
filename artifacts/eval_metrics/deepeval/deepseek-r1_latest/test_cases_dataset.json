[
    {
        "input": "O que √© MLOps?",
        "expected_output": "MLOps √© a disciplina que combina pr√°ticas de Machine Learning, DevOps e engenharia de software para gerenciar o ciclo de vida completo de modelos, desde desenvolvimento at√© monitoramento e manuten√ß√£o cont√≠nua.",
        "actual_output": "MLOps √© um conjunto de pr√°ticas e metodologias voltadas para a opera√ß√£o e gerenciamento de sistemas de machine learning (ML). Similar a DevOps, mas espec√≠fico para projetos de ML, o MLOps une o desenvolvimento de modelos de ML com suas opera√ß√µes em produ√ß√£o.\n\nEle automatiza e padroniza processos como o ciclo de vida do modelo, implanta√ß√£o, monitoramento e manuten√ß√£o, garantindo que os modelos ML funcionem de forma confi√°vel e eficiente em ambientes de produ√ß√£o.\n\nAl√©m disso, o MLOps promove a colabora√ß√£o entre equipes de cientistas de dados, engenheiros de ML e DevOps, facilitando a reprodu√ß√£o, itera√ß√£o e manuten√ß√£o dos modelos. Isso resulta em benef√≠cios como melhor escala, depura√ß√£o mais r√°pida e redu√ß√£o de riscos.",
        "retrieval_context": [
            "Unlock your future in ML Ops with Navigating ML Ops: A\nBeginner' s Bluepr int.\nOther ar ticles y ou might be int erested in:\nMast ering ML Ops: The K ey to a Successful ML Ops Car eer\nThe ML Ops Platform: R evolutionising Machine Learning Efficiency\nThe ML Ops Lifecy cle: A Concise Guide t o Streamlining AI and\nMachine Learning Pr ojects\nWhat is ML Ops? Demystif ying Machine Learning Operations11/11/25, 10:05 PM MLOps Now - What is MLOps? Demystifying Machine Learning Operations\nhttps://mlopsnow.com/blog/what-is-mlops/ 10/11\nMast ering ML Ops: ML Ops Best Practices and Challenges\nFollow me on Twitter: @huwdev ¬© MLOps 2024 - Built by Huw Fulcher11/11/25, 10:05 PM MLOps Now - What is MLOps? Demystifying Machine Learning Operations\nhttps://mlopsnow.com/blog/what-is-mlops/ 11/11",
            "accommodate the ML lifecycle. This integration ensures a streamlined and\nefficient development process, ultimately leading to more reliable AI\napplications and lower maintenance costs.\nTo summarise, ML Ops builds upon DevOps principles and customises them\nto suit the unique challenges of machine learning projects, thus enabling a\nmore seamless and efficient management of these projects throughout\ntheir lifecycle.11/11/25, 10:05 PM MLOps Now - What is MLOps? Demystifying Machine Learning Operations\nhttps://mlopsnow.com/blog/what-is-mlops/ 4/11\nFor a more in-depth look at ML Ops vs DevOps check out our other blog\npost. .\nModel Dev elopment and Deployment\nModel Cr eation\nModel creation is an essential part of the ML Ops process, focused on\ndeveloping machine learning models based on specific requirements. In\nthis phase, data engineers work together with data scientists to prepare\nand preprocess the data, performing featur e engineering to ensure the",
            "Want t o become an ML Ops mast er? Sign up t o the ML Ops Now\nnewslett er to get w eekly ML Ops insights.11/11/25, 10:04 PM MLOps Now - MLOps Best Practices and Challenges\nhttps://mlopsnow.com/blog/mlops-best-practices-and-challenges/ 6/7\nUnlock your future in ML Ops with Navigating ML Ops: A\nBeginner' s Bluepr int.\nOther ar ticles y ou might be int erested in:\nMast ering ML Ops: The K ey to a Successful ML Ops Car eer\nThe ML Ops Platform: R evolutionising Machine Learning Efficiency\nThe ML Ops Lifecy cle: A Concise Guide t o Streamlining AI and\nMachine Learning Pr ojects\nWhat is ML Ops? Demystif ying Machine Learning Operations\nMast ering ML Ops: ML Ops Best Practices and Challenges\nFollow me on Twitter: @huwdev ¬© MLOps 2024 - Built by Huw Fulcher11/11/25, 10:04 PM MLOps Now - MLOps Best Practices and Challenges\nhttps://mlopsnow.com/blog/mlops-best-practices-and-challenges/ 7/7",
            "seamless and efficient integration of ML into existing processes.\nMLOps is more than just the technical side of ML lifecycle management; it\nalso incorporates best practices and methods used in software\ndevelopment and DevOps. Bridging the gap between data scientists, ML\nengineers, and DevOps, ML Ops enables a more coordinated approach to\nML projects. T eams can more easily track, reproduce, and iterate on\nmodels, ensuring stability and performance in production environments.\nBy adopting an ML Ops approach, organisations not only position\nthemselves for better scalability and faster deployment of ML models, but\nalso optimise resources and reduce risk. As a result, businesses can\nleverage data more effectively, enhancing their decision-making processes\nand achieving better outcomes in the competitive marketplace.\nFundamentals o f ML Ops\nMachine Learning Operations\nMLOps stands for Machine L earning Oper ations . It is an IT practice that",
            "‚Ä¢ Handling concerns about model fairness and adversarial attacks.\nMLOps is a methodology for ML engineering that unifies ML system development (the ML element) with ML system \noperations (the Ops element). It advocates formalizing and (when beneficial) automating critical steps of ML system \nconstruction. MLOps provides a set of standardized processes and technology capabilities for building, deploying, \nand operationalizing ML systems rapidly and reliably.\nMLOps supports ML development and deployment in the way that DevOps and DataOps support application engi -\nneering and data engineering (analytics). The difference is that when you deploy a web service, you care about resil -\nience, queries per second, load balancing, and so on. When you deploy an ML model, you also need to worry about \nchanges in the data, changes in the model, users trying to game the system, and so on. This is what MLOps is about."
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.7555555555555555
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 0.5333333333333333
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.4339622641509434
            },
            {
                "llama3.2:latest": 0.6410256410256411
            },
            {
                "gpt-oss:20b-cloud": 0.55
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.625
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.8
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "Qual a diferen√ßa entre MlOps e Data Scientist?",
        "expected_output": "A diferen√ßa entre um Data Scientist e um ML Engineer √© que o primeiro se concentra em desenvolver modelos de aprendizado autom√°tico, enquanto o segundo se concentra na engenharia da constru√ß√£o e implanta√ß√£o desses modelos.",
        "actual_output": "A diferen√ßa entre **MLOps** e **Data Scientist** √© bem definida, embora muitas vezes esses termos sejam confundidos. Vamos esclarecer:\n\n### **MLOps (Machine Learning Operations)**  \nMLOps √© um conjunto de pr√°ticas e ferramentas voltadas para a **gest√£o e opera√ß√£o de sistemas de Machine Learning** em produ√ß√£o. √â como um \"DevOps\" para ML, focando em:  \n- **Automatiza√ß√£o** de processos como integra√ß√£o, teste, implanta√ß√£o e monitoramento de modelos.  \n- **Manuten√ß√£o** cont√≠nua dos modelos, garantindo que eles funcionem bem ao longo do tempo.  \n- **Infraestrutura** e gest√£o de dados necess√°rios para os sistemas ML.  \n\n### **Data Scientist (Cientista de Dados)**  \nData Scientist √© mais focado no **desenvolvimento e cria√ß√£o dos modelos de Machine Learning** em si. Suas principais habilidades incluem:  \n- An√°lise estat√≠stica e matem√°tica.  \n- Sele√ß√£o e treinamento de modelos.  \n- Experimenta√ß√£o e valida√ß√£o de resultados.  \n\n### **Diferen√ßa Principal**  \n- **Data Scientist** cria o modelo, enquanto **MLOps** cuida da \"vida adulta\" do modelo ‚Äî desde a implanta√ß√£o at√© o monitoramento e atualiza√ß√£o cont√≠nua.  \n\nEm resumo:  \n- **Data Scientist** ‚Üí *constr√≥i o modelo*.  \n- **MLOps** ‚Üí *garante que o modelo funcione bem no dia a dia*.  \n\nEspero ter ajudado! üòä",
        "retrieval_context": [
            "https://mlopsnow.com/blog/what-is-mlops/ 2/11\nTo understand ML Ops, it‚Äôs essential to be familiar with the development\nlifecycle of data science projects. A typical data science project consists of\nseveral stages:\n1. Data acquisition: Obtaining raw data from various sources, such as\ndatabases, sensors, or external APIs.\n2. Data pr eprocessing: Cleaning, transforming, and structuring the data\nto prepare it for analysis.\n3. Featur e engineering: Selecting the most relevant data attributes, or\n‚Äúfeatures,‚Äù and converting them into a suitable format for ML\nalgorithms.\n4. Model training: Applying ML algorithms to the preprocessed data to\ncreate a predictive model.\n5. Model ev aluation: Assessing the performance of the model and\nmaking adjustments to improve its accuracy.\n6. Model deployment: Implementing the ML model into a product,\nservice, or system.\n7. Monit oring and maint enance: Continuously monitoring the\nperformance of the ML model and updating it as needed.",
            "ML Engineer vs Data Scientist: What' s the differ ence?\nMLOps Engineer vs Data Scientist: What' s the differ ence?\nMLOps Engineer vs ML Engineer : What' s the differ ence?\nFollow me on Twitter: @huwdev ¬© MLOps 2024 - Built by Huw Fulcher11/11/25, 10:05 PM MLOps Now - ML Engineer vs Data Scientist\nhttps://mlopsnow.com/blog/ml-engineer-vs-data-scientist/ 4/4",
            "ML platform . Similarly, Xin et al. analyze ML pipelines at\nGoogle to understand typical model configurations and retraining\npatterns. Polyzotis et al. survey challenges centric to data\nmanagement for machine learning deployments. Paleyes et al. re-\nview published reports of individual ML deployments and survey\ncommon challenges . Our study instead focuses on issues across\nthe production workflow (i.e., MLOps practices and challenges) as\nopposed to individual pain-points, identified by interviewing those\nwho are are most affected by it‚Äîthe ML engineers.\nData Science and ML-Related Interview Studies. Kandel et\nal. interview data analysts at enterprises, focusing on broader\norganizational contexts like we do; however, MLOps workflows\nand challenges extend beyond data analysis. Other studies build\non Kandel et al.‚Äôs work, exploring aspects such as collaboration,\ncode practices, and tools , all centered on gen-\neral data analysis and data science, as opposed to transitioning",
            "Therefore, many businesses are investing in their data science teams and ML capabilities\nto develop predictive models that can deliver business value to their users.\nThis document is for data scientists and ML engineers who want to apply DevOps\n¬†(https://cloud.google.com/devops/) principles to ML systems (MLOps). MLOps is an ML\nengineering culture and practice that aims at unifying ML system development (Dev) and\nML system operation (Ops). Practicing MLOps means that you advocate for automation\nand monitoring at all steps of ML system construction, including integration, testing,\nreleasing, deployment and infrastructure management.\nData scientists can implement and train an ML model with predictive performance on an\noffline holdout dataset, given relevant training data for their use case. However, the real\nchallenge isn't building an ML model, the challenge is building an integrated ML system and",
            "subset of MLOps capability services.\nML development\nExperimentation is the core activity in ML development, where your data scientists can rapidly try several ideas for \ndata preparation and ML modeling. Experimentation starts when the ML use case is well defined, meaning that the \nfollowing questions have been answered:\n‚Ä¢ What is the task?\n‚Ä¢ How can we measure business impact?\n‚Ä¢ What is the evaluation metric?\nFigure 5. The ML development process\n17\n‚Ä¢ What is the relevant data?\n‚Ä¢ What are the training and serving requirements?\nExperimentation aims to arrive at an effective prototype model for the ML use case at hand. In addition to experimen -\ntation, data scientists need to formalize their ML training procedures. They do this by implementing an end-to-end \npipeline, so that the procedures can be operationalized and run in production. Figure 5 shows the process of ML \ndevelopment. \nDuring experimentation, data scientists typically perform the following steps:"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.9166666666666666
            },
            {
                "gpt-oss:20b-cloud": 0.8875
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.1590909090909091
            },
            {
                "llama3.2:latest": 0.575
            },
            {
                "gpt-oss:20b-cloud": 0.7575757575757576
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.6666666666666666
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.38461538461538464
            },
            {
                "llama3.2:latest": 0.6666666666666666
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "Qual a diferen√ßa entre MLOps e DevOps?",
        "expected_output": "DevOps foca em desenvolvimento e opera√ß√£o de software tradicional, enquanto MLOps lida tamb√©m com dados, treinamento de modelos, versionamento de experimentos e monitoramento de m√©tricas de infer√™ncia e drift.",
        "actual_output": "Okay, vamos comparar MLOps e DevOps! ü§ñ\n\nAmbos s√£o abordagens voltadas para melhorar processos de desenvolvimento e opera√ß√£o, mas focam em coisas diferentes:\n\n**MLOps** (Machine Learning Operations) √© como **DevOps** para projetos de **aprendizado de m√°quina** (ML). Ele surgiu a partir do DevOps, mas adiciona elementos espec√≠ficos para lidar com a complexidade do ML.\n\n**DevOps** foca em facilitar o desenvolvimento, teste e implanta√ß√£o de **softwares tradicionais**. Ele promove a colabora√ß√£o entre desenvolvedores e equipes de opera√ß√µes para automatizar processos e acelerar a entrega de software.\n\n**Aqui est√° a diferen√ßa principal:**\n\n1.  **√Årea de Aplica√ß√£o:**\n    *   **DevOps:** Trata de **softwares convencionais** (aplica√ß√µes web, mobile backend, etc.). O \"produto\" √© o c√≥digo.\n    *   **MLOps:** Trata de **modelos de Machine Learning**. O \"produto\" √© o modelo ML, que depende fortemente de **dados**.\n\n2.  **Elementos Espec√≠ficos do ML:**\n    *   **MLOps:** Como o pr√≥prio nome sugere, incorpora os princ√≠pios do DevOps (como CI/CD) **para o ML**. Isso inclui a automa√ß√£o de processos como:\n        *   **Gest√£o de dados:** Coleta, prepara√ß√£o, versionamento e pipelines de dados s√£o essenciais para o ML, algo que o DevOps tradicional n√£o aborda diretamente.\n        *   **Treinamento de modelos:** Execu√ß√£o automatizada de treinamento em diferentes conjuntos de dados e configura√ß√µes.\n        *   **Valida√ß√£o e Monitoramento:** Verifica√ß√£o cont√≠nua da performance do modelo em produ√ß√£o, pois modelos podem \"esquecer\" ou \"degenerar\" com novos dados (data drift). O software tradicional, por outro lado, n√£o \"degrada\" desse modo.\n        *   **Implanta√ß√£o e Gerenciamento de Modelos:** Gerenciar diferentes vers√µes de modelos ML em produ√ß√£o √© mais complexo que gerenciar aplica√ß√µes tradicionais.\n\n**Em resumo:**\n\n*   **DevOps** √© a base para **desenvolver e operar software**.\n*   **MLOps** √© a aplica√ß√£o de conceitos de DevOps **especificamente para o ciclo de vida do Machine Learning**, tratando modelos ML como produtos e incorporando a gest√£o de dados e monitoramento cont√≠nuo de performance do modelo.\n\n√â como se DevOps fosse a forma de organizar uma f√°brica de software, enquanto MLOps fosse a forma de organizar uma f√°brica de modelos de IA. Ambos usam ideias semelhantes (automa√ß√£o, colabora√ß√£o), mas a f√°brica de IA tem ingredientes especiais (dados, valida√ß√£o cont√≠nua de modelos)! üòä",
        "retrieval_context": [
            "Scalability and governance\n\nMLOps establishes a defined and scalable development process, ensuring consistency, reproducibility and governance throughout the ML lifecycle. Manual deployment and monitoring are slow and require significant human effort, hindering scalability. Without proper centralized monitoring, individual models might experience performance issues that go unnoticed, impacting overall accuracy.\n\nWhat's the relationship to DevOps?\n\nMLOps and DevOps focus on different aspects of the development process. DevOps focuses on streamlining the development, testing and deployment of traditional software applications. It emphasizes collaboration between development and operations teams to automate processes and improve software delivery speed and quality.\n\nMLOps builds upon DevOps principles and applies them to the machine learning lifecycle. It goes beyond deploying code, encompassing data management, model training, monitoring and continuous improvement.",
            "strategic in their decision-making.\nSimilarities between MLOPs and DevOps\nBoth MLOps and DevOps share a need for process automation, continuous integration, and continuous delivery .\nIt also helps to have proper testing of the code base for both MLOps and DevOps.\nIn addition, there should be adequate collaboration between software developers and those who manage the infrastructure, as well as other stakeholders.\nDissimilarities between MLOps and DevOps\nAlthough MLOps is derived from DevOps, there are subtle dif ferences between the two.\nIn MLOps, data is a necessary input for developing the machine learning model. But in DevOps, data is an output of the program, not an input.\nIn MLOPs, the model must be validated continuously in production for performance deterioration caused by new data over time. The software system does not deteriorate\nin DevOps; it is merely monitored for health maintenance purposes.",
            "What are the key elements of an effective MLOps strategy?\n\nMLOps requires skills, tools and practices to effectively manage the machine learning lifecycle. MLOps teams need a diverse skillset encompassing both technical and soft skills. They must understand the entire data science pipeline, from data preparation and model training to evaluation. Familiarity with software engineering practices like version control, CI/CD pipelines and containerization are also crucial. In addition, knowledge of DevOps principles, infrastructure management and automation tools is essential for the efficient deployment and operation of ML models.",
            "systems (MLOps). This document covers concepts to consider when setting up an MLOps\nenvironment for your data science practices, such as CI, CD, and CT in ML.\nThe following topics are discussed:11/13/25, 11:39 PM MLOps: Continuous delivery and automation pipelines in machine learning | Cloud Architecture Center | Google Cloud Do‚Ä¶\nhttps://docs.cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning 2/18\nDevOps versus MLOps\nSteps for developing ML models\nMLOps maturity levels\nMlOps for generative AI\nDevOps versus MLOps\nDevOps¬†(https://cloud.google.com/devops/) is a popular practice in developing and operating\nlarge-scale software systems. This practice provides benefits such as shortening the\ndevelopment cycles, increasing deployment velocity, and dependable releases. To achieve\nthese benefits, you introduce two concepts in the software system development:\nContinuous integration (CI)¬†(https://en.wikipedia.org/wiki/Continuous_integration)",
            "What is MLOps?\n\nMLOps, short for machine learning operations, is a set of practices designed to create an assembly line for building and running machine learning models. It helps companies automate tasks and deploy models quickly, ensuring everyone involved (data scientists, engineers, IT) can cooperate smoothly and monitor and improve models for better accuracy and performance.\n\nThe term MLops is a combination of machine learning (ML) and DevOps. The term was coined in 2015 in a paper called \"Hidden technical debt in machine learning systems,\" which outlined the challenges inherent in dealing with large volumes of data and how to use DevOps processes to instill better ML practices. Creating an MLOps process incorporates continuous integration and continuous delivery (CI/CD) methodology from DevOps to create an assembly line for each step in creating a machine learning product."
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.7555555555555555
            },
            {
                "llama3.2:latest": 0.8875
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.5121951219512195
            },
            {
                "llama3.2:latest": 0.8
            },
            {
                "gpt-oss:20b-cloud": 0.6764705882352942
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.6923076923076923
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.7272727272727273
            },
            {
                "llama3.2:latest": 0.5714285714285714
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que √© drift em modelos de machine learning?",
        "expected_output": "Drift √© a mudan√ßa estat√≠stica no comportamento dos dados ou das rela√ß√µes entre vari√°veis ao longo do tempo, resultando em perda de performance do modelo.",
        "actual_output": "Okay, vamos entender o que √© \"drift\" em modelos de machine learning!\n\nEm resumo, **drift em machine learning √© quando as caracter√≠sticas ou a distribui√ß√£o dos dados que o modelo usa para fazer previs√µes muda ao longo do tempo, e isso afeta o desempenho do modelo.**\n\nPensando de forma mais detalhada:\n\n1.  **O que √© isso?**\n    *   Imagine que voc√™ treina um modelo para prever se uma pessoa vai comprar um produto. Ele aprende com dados passados (treino).\n    *   Depois de implantado, ele usa dados novos (dados de produ√ß√£o) para fazer as previs√µes.\n    *   **Drift** acontece quando esses dados de produ√ß√£o mudam de alguma forma significativa em rela√ß√£o aos dados de treino.\n\n2.  **Tipos Comuns (especialmente mencionado no contexto):**\n    *   **Concept Drift (Distor√ß√£o de Conceito):** Este √© o mais comum. √â quando a rela√ß√£o entre as caracter√≠sticas (vari√°veis) e o resultado alvo muda. Por exemplo, um modelo de cr√©dito foi treinado com dados de uma √©poca econ√¥mica. Com o tempo, o comportamento do mercado muda, e o modelo come√ßa a dar resultados ruins porque a rela√ß√£o que aprendeu n√£o √© mais a mesma. √â como o Goldilocks: as coisas mudaram, e o modelo precisa se adaptar ou ser reajustado.\n    *   **Data Drift (Deriva de Dados):** √â quando a distribui√ß√£o estat√≠stica dos dados de entrada muda. Por exemplo, um modelo de imagem reconhecimento foi treinado com fotos tiradas sob certa luz. Se as fotos novas forem muito mais claras ou escuras, a qualidade dos dados muda, e o modelo pode come√ßar a errar mais.\n\n3.  **Por que √© um problema?**\n    *   O modelo foi constru√≠do para funcionar com os dados originais.\n    *   Se os dados mudam, o modelo pode come√ßar a dar respostas imprecisas, menos precisas ou at√© mesmo opostas √†s que eram esperadas.\n    *   Isso pode levar a decis√µes ruins, falhas no sistema ou ao \"modelo ficar obsoleto\".\n\n4.  **Como lidar com isso?**\n    *   **Monitoramento:** √â crucial verificar continuamente o desempenho do modelo e a qualidade dos dados de entrada. Se houver sinais de drift (como piora na precis√£o ou mudan√ßas nos dados), a√ß√µes devem ser tomadas.\n    *   **Retreinamento:** O modelo pode precisar ser retreinado com os dados mais recentes, especialmente quando h√° uma mudan√ßa significativa (drift).\n    *   **Robustez do Modelo:** Tentar criar modelos mais resistentes a mudan√ßas nos dados tamb√©m √© uma abordagem.\n\nEnt√£o, o drift √© uma quest√£o importante na manuten√ß√£o de modelos de machine learning em produ√ß√£o, pois os dados e o mundo real mudam com o tempo!",
        "retrieval_context": [
            "On availability of new training data: New data isn't systematically available for the ML\nsystem and instead is available on an ad hoc basis when new data is collected and\nmade available in the source databases.11/13/25, 11:39 PM MLOps: Continuous delivery and automation pipelines in machine learning | Cloud Architecture Center | Google Cloud Do‚Ä¶\nhttps://docs.cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning 12/18\nOn model performance degradation: The model is retrained when there is noticeable\nperformance degradation.\nOn significant changes in the data distributions (concept drift\n¬†(https://en.wikipedia.org/wiki/Concept_drift)). It's hard to assess the complete\nperformance of the online model, but you notice significant changes on the data\ndistributions of the features that are used to perform the prediction. These changes\nsuggest that your model has gone stale, and that needs to be retrained on fresh data.\nChallenges",
            "Monitoring and optimization\n\nIn the lifecycle of a deployed machine learning model, continuous vigilance ensures effectiveness and fairness over time. Model monitoring forms the cornerstone of this phase, involving the ongoing scrutiny of the model's performance in the production environment. This step helps identify emerging issues, such as accuracy drift, bias and concerns around fairness, which could compromise the model's utility or ethical standing. Monitoring is about overseeing the model's current performance and anticipating potential problems before they escalate.",
            "asked P17 to give an example of a natural data drift problem\ntheir company faced, and they could not think of a good\nexample. P14 also said they don‚Äôt have natural data drift\nproblems:\n5Goldilocks and the Three Bears is a popular Western fairy tale. Goldilocks, the main\ncharacter, looks for things that are not too big or not too small, things that are ‚Äújust\nright.‚Äù\nOperationalizing Machine Learning: An Interview Study\nThe model gets retrained every day, so we don‚Äôt have the\nscenario of like: Oh, our models got stale and we need to re-\ntrain it because it‚Äôs starting to make mistakes because data\nhas drifted...fortunately we‚Äôve never had to deal with [such\na] scenario. Sometimes there are bad jobs, but\nwe can always effectively roll back to a different .\nHowever, a few engineers mentioned that natural data shift\ncould cause some hand-curated features and data quality\nchecks to corrupt (P3, P6, P8). P6 discussed a histogram used",
            "object recognition, probabilities or likelihoods as embeddings). P1\ndescribed a push at their company to rely more on neural networks:\nA general trend is to try to move more into the neural\nnetwork, and to combine models wherever possible so\nthere are fewer bigger models. Then you don‚Äôt have\nthese intermediate dependencies that cause drift and\nperformance regressions...you eliminate entire classes of\nbugs and and issues by consolidating all these different\npiecemeal stacks.\n4.5.6 Organizationally Supporting ML Engineers Requires Delib-\nerate Practices. Our interviewees reported various organizational\nprocesses for sustaining models as part of their ML infrastructure.\nP6, P12, P14, P16, P18, and P19 described on-call processes for su-\npervising production ML models. For each model, at any point in\ntime, some ML engineer would be on call, or primarily responsible\nfor it. Any bug or incident observed (e.g., user complaint, pipeline",
            "Continuous monitoring of model performance for accuracy drift, bias and other potential issues plays a critical role in maintaining the effectiveness of models and preventing unexpected outcomes. Monitoring the performance and health of ML models ensures that they continue to meet the intended objectives after deployment. By proactively identifying and addressing these concerns, organizations can maintain optimal model performance, mitigate risks and adapt to changing conditions or feedback."
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.9166666666666666
            },
            {
                "gpt-oss:20b-cloud": null,
                "logs": "1 validation error for Verdicts\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"verdicts\"...s.\"\\n    }\\n  ]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid"
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.34782608695652173
            },
            {
                "llama3.2:latest": 0.6153846153846154
            },
            {
                "gpt-oss:20b-cloud": 0.48
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.6153846153846154
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.6
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que √© feature store?",
        "expected_output": "Uma feature store √© um sistema que centraliza cria√ß√£o, versionamento, armazenamento e disponibiliza√ß√£o de features para treinamento e infer√™ncia.",
        "actual_output": "Um **feature store** √© um reposit√≥rio centralizado criado para armazenar e gerenciar as **features** (caracter√≠sticas ou atributos) utilizadas nos modelos de machine learning. Suas principais fun√ß√µes s√£o:\n\n1. **Armazenamento padronizado**: Mant√©m a defini√ß√£o, armazenamento e acesso padronizados das features para treinamento e infer√™ncia.\n2. **Evitar o \"treinamento vs. infer√™ncia\"**: Garante que as features utilizadas no treinamento sejam as mesmas usadas na infer√™ncia, evitando diferen√ßas que possam comprometer o modelo.\n3. **Reutiliza√ß√£o e consist√™ncia**: Centraliza as features, permitindo que diferentes equipes e projetos usem as mesmas caracter√≠sticas sem precisar recri√°-las.\n4. **Suporte a diferentes tipos de trabalho**: Fornece recursos para servir features de forma eficiente, seja em modo batch (processamento em lote) ou real-time (com baixa lat√™ncia).\n\n### Principais benefitos:\n- **Efici√™ncia**: Economiza tempo na prepara√ß√£o de dados e engenharia de features.\n- **Consist√™ncia**: Garante que os mesmos dados utilizados no treinamento sejam usados na infer√™ncia.\n- **Reprodutibilidade**: Facilita a valida√ß√£o e o rastreamento de mudan√ßas nos dados.\n\n### Exemplo pr√°tico:\nImagine que voc√™ est√° construindo um modelo para recomendar produtos. Um *feature store* pode armazenar dados como \"hist√≥rico de compras do cliente\", \"prefer√™ncias por categorias\" e \"comportamento de navega√ß√£o\". Dessa forma, voc√™ evita ter que recriar essas features toda vez que precisar usar o modelo.",
        "retrieval_context": [
            "high-throughput batch serving and low-latency real-time serving for the feature values, and\nto support both training and serving workloads.\nThe feature store helps data scientists do the following:\nDiscover and reuse available feature sets for their entities, instead of re-creating the\nsame or similar ones.\nAvoid having similar features that have different definitions by maintaining features\nand their related metadata.\nServe up-to-date feature values from the feature store.\nAvoid training-serving skew by using the feature store as the data source for\nexperimentation, continuous training, and online serving. This approach makes sure\nthat the features used for training are the same ones used during serving:\nFor experimentation, data scientists can get an offline extract from the feature\nstore to run their experiments.\nFor continuous training, the automated ML training pipeline can fetch a batch of\nthe up-to-date feature values of the dataset that are used for the training task.",
            "repository also provides data consistency for training and inference. This helps data scientists and ML researchers \nsave time on data preparation and feature engineering, which typically take up a significant amount of their time. Key \nfunctionalities in the data and feature repository include the following:\n15\n‚Ä¢ Enable shareability, discoverability, reusability, and versioning of data assets.\n‚Ä¢ Allow real-time ingestion and low-latency serving for event streaming and online prediction workloads. \n‚Ä¢ Allow high-throughput batch ingestion and serving for extract, transform, load (ETL) processes and model \ntraining, and for scoring workloads.\n‚Ä¢ Enable feature versioning for point-in-time queries.\n‚Ä¢ Support various data modalities, including tabular data, images, and text.\nML data assets can be managed at the entity features level or at the full dataset level. For example, a feature reposi -",
            "compatibility and consistency with the prediction service API.11/13/25, 11:39 PM MLOps: Continuous delivery and automation pipelines in machine learning | Cloud Architecture Center | Google Cloud Do‚Ä¶\nhttps://docs.cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning 10/18\nIn addition to offline model validation, a newly deployed model undergoes online model\nvalidation‚Äîin a canary deployment or an A/B testing setup‚Äîbefore it serves prediction for\nthe online traffic.\nFeature store\nAn optional additional component for level 1 ML pipeline automation is a feature store. A\nfeature store is a centralized repository where you standardize the definition, storage, and\naccess of features for training and serving. A feature store needs to provide an API for both\nhigh-throughput batch serving and low-latency real-time serving for the feature values, and\nto support both training and serving workloads.",
            "store to run their experiments.\nFor continuous training, the automated ML training pipeline can fetch a batch of\nthe up-to-date feature values of the dataset that are used for the training task.\nFor online prediction, the prediction service can fetch in a batch of the feature\nvalues related to the requested entity, such as customer demographic features,\nproduct features, and current session aggregation features.\nFor online prediction and feature retrieval, the prediction service identifies the\nrelevant features for an entity. For example, if the entity is a customer, relevant\nfeatures might include age, purchase history, and browsing behavior. The service\nbatches these feature values together and retrieves all the needed features for\nthe entity at once, rather than individually. This retrieval method helps with\nefficiency, especially when you need to manage multiple entities.",
            "Data versioning plays a pivotal role in maintaining the integrity and reproducibility of data analysis. It involves tracking and managing different versions of the data, allowing for traceability of results and the ability to revert to previous states if necessary. Versioning ensures that others can replicate and verify analyses, promoting transparency and reliability in data science projects.\n\nThe concept of a feature store is then introduced as a centralized repository for storing and managing features used in model training. Feature stores promote consistency and reusability of features across different models and projects. By having a dedicated system for feature management, teams can ensure they use the most relevant and up-to-date features.\n\nModel development"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.7555555555555555
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 0.95
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.25
            },
            {
                "llama3.2:latest": 0.75
            },
            {
                "gpt-oss:20b-cloud": 0.7837837837837838
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.7
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.6153846153846154
            },
            {
                "llama3.2:latest": 0.4
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que significa servir um modelo em produ√ß√£o?",
        "expected_output": "Servir um modelo significa disponibiliz√°-lo por meio de um endpoint, job ou pipeline para receber dados e retornar previs√µes de forma escal√°vel e confi√°vel.",
        "actual_output": "**Ser um modelo em produ√ß√£o significa** disponibilizar um modelo de machine learning para funcionar em um ambiente real, onde ele pode processar dados e gerar predi√ß√µes ou decis√µes de forma autom√°tica e consistente. Isso vai al√©m de apenas criar o modelo; ele se torna parte de um sistema operacional, respondendo a requisi√ß√µes e integrando-se a processos do neg√≥cio.\n\n### Principais aspectos de um modelo em produ√ß√£o:\n1. **API para predi√ß√µes**: O modelo √© exposto via API (Application Programming Interface), permitindo que outros sistemas ou aplicativos o utilizem para fazer predi√ß√µes em tempo real.\n\n2. **Monitoramento e manuten√ß√£o**: Ap√≥s ser servido, o modelo precisa ser monitorado para garantir que ele continue funcionando bem, mesmo com mudan√ßas nos dados ou no ambiente. Isso inclui verificar m√©tricas de desempenho, atualiza√ß√µes de infraestrutura e retrainamentos peri√≥dicos.\n\n3. **Escalabilidade**: O modelo deve ser capaz de lidar com volumes crescentes de requisi√ß√µes, conforme o neg√≥cio expande.\n\n4. **Governan√ßa e seguran√ßa**: Em produ√ß√£o, √© crucial garantir que o modelo cumpra requisitos de conformidade, privacidade e seguran√ßa.\n\n---\n\n### Tipos de deploy em produ√ß√£o:\nO processo de servir um modelo pode ser feito de diferentes maneiras, dependendo da complexidade do sistema e dos requisitos do neg√≥cio:\n\n1. **Deploy autom√°tico**: O modelo √© implantado automaticamente quando o c√≥digo ou o pipeline de ML √© atualizado. Geralmente usado em ambientes de desenvolvimento ou teste.\n\n2. **Deploy semi-autom√°tico**: Requer aprova√ß√£o manual de revisores antes de ser implantado em um ambiente pr√©-produ√ß√£o.\n\n3. **Deploy manual**: O modelo √© implantado em produ√ß√£o ap√≥s testes manuais e valida√ß√µes completas.\n\n---\n\n### Por que √© importante?\nServir um modelo em produ√ß√£o √© o √∫ltimo passo do ciclo de vida do projeto de ML. Isso significa que o modelo est√° pronto para ser usado em aplica√ß√µes reais, contribuindo para a tomada de decis√µes ou para a automa√ß√£o de processos. Al√©m disso, o modelo precisa estar integrado a um sistema de **CI/CD** (Continuous Integration/Continuous Deployment) para garantir que ele seja atualizado e mantido de forma eficiente.\n\nEm resumo, **servir um modelo em produ√ß√£o** √© como colocar um carro pronto para rodar na estrada ‚Äî ele precisa estar bem constru√≠do, seguro e funcionando corretamente para atender √†s necessidades do dia a dia. üòä",
        "retrieval_context": [
            "https://mlopsnow.com/blog/what-is-mlops/ 2/11\nTo understand ML Ops, it‚Äôs essential to be familiar with the development\nlifecycle of data science projects. A typical data science project consists of\nseveral stages:\n1. Data acquisition: Obtaining raw data from various sources, such as\ndatabases, sensors, or external APIs.\n2. Data pr eprocessing: Cleaning, transforming, and structuring the data\nto prepare it for analysis.\n3. Featur e engineering: Selecting the most relevant data attributes, or\n‚Äúfeatures,‚Äù and converting them into a suitable format for ML\nalgorithms.\n4. Model training: Applying ML algorithms to the preprocessed data to\ncreate a predictive model.\n5. Model ev aluation: Assessing the performance of the model and\nmaking adjustments to improve its accuracy.\n6. Model deployment: Implementing the ML model into a product,\nservice, or system.\n7. Monit oring and maint enance: Continuously monitoring the\nperformance of the ML model and updating it as needed.",
            "‚Ä¢ Hyperparameters, including trials of automated hyperparameter tuning and model selection.\n‚Ä¢ Information about training, validation, and testing data splits that were used. \n‚Ä¢ Model evaluation metrics and the validation procedure that was used.\nIf there is no need to retrain the model on a regular basis, then the produced model at the end of the experimenta -\ntion is submitted to the model registry. The model is then ready to be reviewed, approved, and deployed to the target \n18\nserving environment. In addition, all the relevant metadata and artifacts \nthat were produced during model development are tracked in the metadata \ntracking repository.\nHowever, in most cases, ML models need to be retrained on a regular basis \nwhen new data is available or when the code changes. In this case, the \noutput of the ML development process is not the model to be deployed in \nproduction. Instead, the output is the implementation of the continuous",
            "Model development\n\nModel development is a core phase in the data science process, focusing on constructing and refining machine learning models. This phase starts with model training, where the prepared data is used to train machine learning models that use selected algorithms and frameworks. The objective is to teach the model to make accurate predictions or decisions based on the data it has been trained on.\n\nAn essential aspect of model development is maintaining and tracking experiments, which involves keeping detailed records of different model iterations, the hyperparameter configurations used and the outcomes of various experiments. Such meticulous documentation is critical for comparing different models and configurations, facilitating the identification of the most effective approaches. This process helps optimize model performance and ensures that the development process is transparent and reproducible.",
            "Following the training phase, model evaluation is conducted to assess the performance of the models on unseen data. Evaluation is critical to ensure that the models perform well in real-world scenarios. Metrics such as accuracy, precision, recall and fairness measures gauge how well the model meets the project objectives. These metrics provide a quantitative basis for comparing different models and selecting the best one for deployment. Through careful evaluation, data scientists can identify and address potential issues, such as bias or overfitting, ensuring that the final model is effective and fair.\n\nModel deployment",
            "Verifying that models meet the predictive performance targets before they are\ndeployed.\nAutomated deployment to a test environment, for example, a deployment that is\ntriggered by pushing code to the development branch.\nSemi-automated deployment to a pre-production environment, for example, a\ndeployment that is triggered by merging code to the main branch after reviewers\napprove the changes.\nManual deployment to a production environment after several successful runs of the\npipeline on the pre-production environment.\nTo summarize, implementing ML in a production environment doesn't only mean deploying\nyour model as an API for prediction. Rather, it means deploying an ML pipeline that can\nautomate the retraining and deployment of new models. Setting up a CI/CD system lets you\nautomatically test and deploy new pipeline implementations. This system lets you cope\nwith rapid changes in your data and business environment. You don't have to immediately"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.9166666666666666
            },
            {
                "gpt-oss:20b-cloud": 0.8666666666666667
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.4
            },
            {
                "llama3.2:latest": 0.4888888888888889
            },
            {
                "gpt-oss:20b-cloud": 0.4444444444444444
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.6923076923076923
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 0.9375
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.6363636363636364
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "Por que versionar modelos √© importante?",
        "expected_output": "Versionar modelos permite rastrear artefatos, reproduzir experimentos, auditar previs√µes e garantir rollback seguro.",
        "actual_output": "O versionamento de modelos √© importante porque:\n\n1. **Garante a integridade e reproduzibilidade**: Ele permite rastrear diferentes vers√µes de um modelo, garantindo que an√°lises e resultados possam ser replicados e verificados.\n\n2. **Facilita a rastreabilidade**: Ao manter registros detalhados de cada vers√£o, √© poss√≠vel entender como e por qu√™ mudan√ßas foram feitas, o que ajuda na identifica√ß√£o de problemas e na melhoria cont√≠nua.\n\n3. **Promove a transpar√™ncia e confiabilidade**: Versionamento torna o processo de desenvolvimento de modelos mais transparente, permitindo que outros (e voc√™ mesmo no futuro) compreendam e validem as decis√µes tomadas durante o desenvolvimento.",
        "retrieval_context": [
            "more expensive to analyze improvements to that model in the f uture. The cost increases when\ncorrection models are cascaded, with a model for problem A‚Ä≤‚Ä≤learned on top of m‚Ä≤\na, and so on,\nfor several slightly different test distributions. Once in place, a correction cascade can create an\nimprovement deadlock, as improving the accuracy of any indi vidual component actually leads to\nsystem-level detriments. Mitigation strategies are to aug mentmato learn the corrections directly\nwithin the same model by adding features to distinguish amon g the cases, or to accept the cost of\ncreating a separate model for A‚Ä≤.\nUndeclared Consumers. Oftentimes, a prediction from a machine learning model mais made\nwidely accessible, either at runtime or by writing to Ô¨Åles or logs that may later be consumed by\nother systems. Without access controls, some of these consu mers may be undeclared , silently using\nthe output of a given model as an input to another system. In mo re classical software engineering,",
            "Continuous Integration.¬†\nMonit oring identifies model drif t over time. Without model monitoring,\nproduction systems are flying blind. By monitoring for model drift the data\nscience team is able to proactively work rather than reactively.¬†\nTesting ensur es the accuracy and r eliability o f models. Validating both\nthe model‚Äôs predictions and the data sets used is a fundamental step in\ngreenlighting models for production.¬†\nUse A/B t esting t o identif y best models. A/B testing is sometimes\noverlooked in Machine Learning but is a great way to introduce new\nmodels. Rather than swapping models out straight away you can introduce\nthe new model alongside the old. This weighted approach allows you to\nsee the efficacy of the new model in production before committing to it.\n4. Version Contr ol\nVersion control is a significant aspect of ML Ops. It allows teams to track",
            "Data versioning plays a pivotal role in maintaining the integrity and reproducibility of data analysis. It involves tracking and managing different versions of the data, allowing for traceability of results and the ability to revert to previous states if necessary. Versioning ensures that others can replicate and verify analyses, promoting transparency and reliability in data science projects.\n\nThe concept of a feature store is then introduced as a centralized repository for storing and managing features used in model training. Feature stores promote consistency and reusability of features across different models and projects. By having a dedicated system for feature management, teams can ensure they use the most relevant and up-to-date features.\n\nModel development",
            "Model development\n\nModel development is a core phase in the data science process, focusing on constructing and refining machine learning models. This phase starts with model training, where the prepared data is used to train machine learning models that use selected algorithms and frameworks. The objective is to teach the model to make accurate predictions or decisions based on the data it has been trained on.\n\nAn essential aspect of model development is maintaining and tracking experiments, which involves keeping detailed records of different model iterations, the hyperparameter configurations used and the outcomes of various experiments. Such meticulous documentation is critical for comparing different models and configurations, facilitating the identification of the most effective approaches. This process helps optimize model performance and ensures that the development process is transparent and reproducible.",
            "Collaboration and governance\n\nCreating a streamlined and efficient workflow needs the adoption of several practices and tools, among which version control stands as a cornerstone. Using systems like Git, teams can meticulously track and manage changes in code, data and models. Fostering a collaborative environment makes it easier for team members to work together on projects and ensures that any modifications can be documented and reversed when needed. The ability to roll back to previous versions is invaluable, especially when new changes introduce errors or reduce the effectiveness of the models."
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.7555555555555555
            },
            {
                "llama3.2:latest": 0.8666666666666667
            },
            {
                "gpt-oss:20b-cloud": 0.6791666666666667
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.4666666666666667
            },
            {
                "llama3.2:latest": 0.8
            },
            {
                "gpt-oss:20b-cloud": 0.4827586206896552
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.6666666666666666
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que √© um pipeline de ML?",
        "expected_output": "√â um fluxo automatizado que engloba etapas como ingest√£o, processamento, feature engineering, treinamento, avalia√ß√£o e deploy.",
        "actual_output": "Um pipeline de ML √© um conjunto de etapas automatizadas que conduzem dados de uma origem at√© a produ√ß√£o de um modelo treinado e pronto para uso. Ele representa um fluxo de trabalho estruturado que une diferentes processos, como a coleta de dados, pr√©-processamento, engenharia de caracter√≠sticas, treinamento do modelo, valida√ß√£o e implanta√ß√£o.\n\nEm termos mais simples, um pipeline de ML √© como um \"tubo\" que conduz dados de uma etapa para outra, garantindo que o modelo seja constru√≠do de forma eficiente e repet√≠vel. Ele inclui desde a prepara√ß√£o dos dados at√© a monitoriza√ß√£o do modelo em produ√ß√£o.",
        "retrieval_context": [
            "flow:\n1. The core activity during this ML development phase is experimentation. As data scientists and ML research -\ners prototype model architectures and training routines, they create labeled datasets, and they use features \nand other reusable ML artifacts that are governed through the data and model management process. The \nFigure 3. The MLOps process\n9\nprimary output of this process is a formalized training procedure, which includes data preprocessing, model \narchitecture, and model training settings. \n2. If the ML system requires continuous training (repeated retraining of the model), the training procedure is \noperationalized as a training pipeline. This requires a CI/CD routine to build, test, and deploy the pipeline to \nthe target execution environment.\n3. The continuous training pipeline is executed repeatedly based on retraining triggers, and it produces a model \nas output. The model is retrained as new data becomes available, or if model performance decay is detected.",
            "aging these pipelines, detecting errors and recovering fro m failures are all difÔ¨Åcult and costly .\nTesting such pipelines often requires expensive end-to-en d integration tests. All of this adds to\ntechnical debt of a system and makes further innovation more costly.\nPipeline jungles can only be avoided by thinking holistical ly about data collection and feature ex-\ntraction. The clean-slate approach of scrapping a pipeline jungle and redesigning from the ground\nup is indeed a major investment of engineering effort, but on e that can dramatically reduce ongoing\ncosts and speed further innovation.\nGlue code and pipeline jungles are symptomatic of integrati on issues that may have a root cause in\noverly separated ‚Äúresearch‚Äù and ‚Äúengineering‚Äù roles. When M L packages are developed in an ivory-\ntower setting, the result may appear like black boxes to the t eams that employ them in practice. A\nhybrid research approach where engineers and researchers a re embedded together on the same teams",
            "https://mlopsnow.com/blog/what-is-mlops/ 2/11\nTo understand ML Ops, it‚Äôs essential to be familiar with the development\nlifecycle of data science projects. A typical data science project consists of\nseveral stages:\n1. Data acquisition: Obtaining raw data from various sources, such as\ndatabases, sensors, or external APIs.\n2. Data pr eprocessing: Cleaning, transforming, and structuring the data\nto prepare it for analysis.\n3. Featur e engineering: Selecting the most relevant data attributes, or\n‚Äúfeatures,‚Äù and converting them into a suitable format for ML\nalgorithms.\n4. Model training: Applying ML algorithms to the preprocessed data to\ncreate a predictive model.\n5. Model ev aluation: Assessing the performance of the model and\nmaking adjustments to improve its accuracy.\n6. Model deployment: Implementing the ML model into a product,\nservice, or system.\n7. Monit oring and maint enance: Continuously monitoring the\nperformance of the ML model and updating it as needed.",
            "Optimising Data Pipeline and Model P erformance\nOptimising performance of data pipelines and models is a crucial aspect\nof ML Ops. T o address performance-related challenges, organisations can:\nImplement robust data preprocessing techniques to clean and\ntransform input data efficiently.\nUse automated feature engineering to select relevant features,\nreducing the risk of overfitting and enhancing overall model\nperformance.\nImplement real-time monitoring and alerting systems for pipelines\nand models, facilitating prompt identification and resolution of issues.\nMLOps Ar chitectur e and T ools\nMLOps Ar chitectur e and Design\nThe ML Ops architecture comprises several components, including data\ncollection , data pr ep, model training, validation, and deployment. A well-11/11/25, 10:05 PM MLOps Now - What is MLOps? Demystifying Machine Learning Operations\nhttps://mlopsnow.com/blog/what-is-mlops/ 7/11\ndesigned architecture ensures smooth collaboration between different",
            "Takeaway. The MLOps anti-patterns described in this section re-\nveal that ML engineering, as a field, is changing faster than educa-\ntional resources can keep up. We see this as opportunities for new\nresources, such as classroom material (e.g., textbooks, courses) to\nprescribe the right engineering practices and rigor for the highly\nexperimental discipline that is production ML, and automated doc-\numentation assistance for ML pipelines in organizations.\n5.3 Characterizing the ‚ÄúMLOps Stack‚Äù for Tool\nBuilders\nMLOps tool builders may be interested in an organization of the\ndozens of tools, libraries, and services MLEs use to run ML and\ndata processing pipelines. Although multiple MLEs reported hav-\ning to ‚Äúglue‚Äù open-source solutions together and having to build\n‚Äúhomegrown‚Äù infrastructure as part of their work (P1, P2, P5, P6,\nP10, P12), an analysis of the various deployments reveals that tools\nShreya Shankar‚àó, Rolando Garcia‚àó, Joseph M. Hellerstein, Aditya G. Parameswaran\nRun Layer"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.6787301587301586
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 0.3333333333333333
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.18181818181818182
            },
            {
                "llama3.2:latest": 0.7407407407407407
            },
            {
                "gpt-oss:20b-cloud": 0.59375
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.7142857142857143
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.7142857142857143
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que √© reprodutibilidade em ML?",
        "expected_output": "Capacidade de reproduzir os mesmos resultados usando o mesmo c√≥digo, dados e par√¢metros.",
        "actual_output": "A reprodutibilidade em ML (Machine Learning) √© um conceito fundamental que se refere √† capacidade de **reproduzir os mesmos resultados de um experimento ou treinamento de modelo**, utilizando os mesmos dados, algoritmos e par√¢metros, em diferentes ambientes ou em outro momento.\n\nIsso √© crucial porque garante que:\n\n1.  **Resultados s√£o confi√°veis:** Se um modelo obt√©m 95% de precis√£o em um conjunto de dados, deve ser poss√≠vel obter a mesma precis√£o repetidamente.\n2.  **Valida√ß√£o e compara√ß√£o s√£o justas:** √â poss√≠vel comparar modelos ou t√©cnicas de forma justa, sabendo que as condi√ß√µes s√£o id√™nticas.\n3.  **Reparo e depura√ß√£o s√£o poss√≠veis:** Se um modelo falha, √© mais f√°cil identificar onde o erro ocorreu se os experimentos forem reprodut√≠veis.\n\nFatores que contribuem para a reprodutibilidade em ML incluem:\n\n*   **Defini√ß√£o clara dos dados:** Uso de conjuntos de dados espec√≠ficos e versionamento.\n*   **Reprodutibilidade do pr√©-processamento:** Passos de pr√©-processamento consistentes e documentados.\n*   **Seed fixa:** Para opera√ß√µes estoc√°sticas (como inicializa√ß√£o de pesos ou shuffling de dados).\n*   **Ambiente consistente:** Uso de bibliotecas, vers√µes e configura√ß√µes do sistema operacional id√™nticas.\n*   **Documenta√ß√£o detalhada:** Anota√ß√£o de todos os par√¢metros, decis√µes e passos do experimento.\n\nInfelizmente, o contexto fornecido n√£o cont√©m informa√ß√µes sobre o conceito de reprodutibilidade em ML.",
        "retrieval_context": [
            "https://mlopsnow.com/blog/what-is-mlops/ 2/11\nTo understand ML Ops, it‚Äôs essential to be familiar with the development\nlifecycle of data science projects. A typical data science project consists of\nseveral stages:\n1. Data acquisition: Obtaining raw data from various sources, such as\ndatabases, sensors, or external APIs.\n2. Data pr eprocessing: Cleaning, transforming, and structuring the data\nto prepare it for analysis.\n3. Featur e engineering: Selecting the most relevant data attributes, or\n‚Äúfeatures,‚Äù and converting them into a suitable format for ML\nalgorithms.\n4. Model training: Applying ML algorithms to the preprocessed data to\ncreate a predictive model.\n5. Model ev aluation: Assessing the performance of the model and\nmaking adjustments to improve its accuracy.\n6. Model deployment: Implementing the ML model into a product,\nservice, or system.\n7. Monit oring and maint enance: Continuously monitoring the\nperformance of the ML model and updating it as needed.",
            "Building an ML-enabled system\nBuilding an ML-enabled system is a multifaceted undertaking that combines data engineering, ML engineering, and \napplication engineering tasks, as shown in figure 1.\nData engineering involves ingesting, integrating, curating, and refining data to facilitate a broad spectrum of opera -\ntional tasks, data analytics tasks, and ML tasks. Data engineering can be crucial to the success of the analytics and \nML initiatives. If an organization does not have robust data engineering processes and technologies, it might not be \nset up for success with downstream business intelligence, advanced analytics, or ML projects.\nML models are built and deployed in production using curated data that is usually created by the data engineering \nteam. The models do not operate in silos; they are components of, and support, a large range of application systems, \nsuch as business intelligence systems, line of business applications, process control systems, and embedded sys -",
            "‚Ä¢ Support various data modalities, including tabular data, images, and text.\nML data assets can be managed at the entity features level or at the full dataset level. For example, a feature reposi -\ntory might contain an entity called customer, which includes features like age group, postal code, and gender. On the \nother hand, a dataset repository might include a customer churn dataset, which includes features from the customer \nand product entities, as well as purchase- and web-activity event logs.\nML metadata and artifact tracking\nVarious types of ML artifacts are produced in different processes of the MLOps lifecycle, including descriptive \nstatistics and data schemas, trained models, and evaluation results. ML metadata is the information about these \nartifacts, including their location, types, properties, and associations to experiments and runs. The ML metadata and",
            "subset of MLOps capability services.\nML development\nExperimentation is the core activity in ML development, where your data scientists can rapidly try several ideas for \ndata preparation and ML modeling. Experimentation starts when the ML use case is well defined, meaning that the \nfollowing questions have been answered:\n‚Ä¢ What is the task?\n‚Ä¢ How can we measure business impact?\n‚Ä¢ What is the evaluation metric?\nFigure 5. The ML development process\n17\n‚Ä¢ What is the relevant data?\n‚Ä¢ What are the training and serving requirements?\nExperimentation aims to arrive at an effective prototype model for the ML use case at hand. In addition to experimen -\ntation, data scientists need to formalize their ML training procedures. They do this by implementing an end-to-end \npipeline, so that the procedures can be operationalized and run in production. Figure 5 shows the process of ML \ndevelopment. \nDuring experimentation, data scientists typically perform the following steps:",
            "rather than ML-specific metrics alone like MAP (P5, P7, P15, P16,\nP11, P17, P18, P19). The need to evaluate product-critical metrics\nstemmed from close collaboration with other stakeholders, such\nas product managers and business operators. P11 felt that a key\nreason many ML projects fail is that they don‚Äôt measure metrics\nthat will yield the organization value:\nTying to the business‚Äôs KPIs (key\nperformance indicators) is really important. But it‚Äôs a\nprocess‚Äîyou need to figure out what are, and\nfrankly I think that‚Äôs how people should be doing AI. It\n like: hey, let‚Äôs do these experiments and\nget cool numbers and show off these nice precision-recall\ncurves to our bosses and call it a day. It should be like:\nhey, let‚Äôs actually show the same business metrics that\neveryone else is held accountable to to our bosses at the\nend of the day.\nSince product-specific metrics are, by definition, different for\ndifferent ML models, it was important for engineers to treat choos-"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.7555555555555555
            },
            {
                "gpt-oss:20b-cloud": 0
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.3684210526315789
            },
            {
                "llama3.2:latest": 0.775
            },
            {
                "gpt-oss:20b-cloud": 0.0
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.6363636363636364
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 0.9166666666666666
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.7272727272727273
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que √© CI/CD aplicado a ML?",
        "expected_output": "CI/CD em ML automatiza testes, valida√ß√£o, empacotamento e implanta√ß√£o de modelos, garantindo ciclos mais r√°pidos e confi√°veis.",
        "actual_output": "Ol√°! Vamos entender o que √© CI/CD aplicado a ML.\n\nNo contexto do Machine Learning (ML), o CI/CD (Continuous Integration/Continuous Delivery) √© adaptado para lidar com a entrega de modelos de forma eficiente e confi√°vel. Aqui est√° como ele se difere um pouco do CI/CD tradicional:\n\n1. **CI (Continuous Integration)** no ML:\n   - N√£o √© s√≥ sobre testar e validar c√≥digo, mas tamb√©m dados, esquemas de dados e modelos.\n   - Isso significa que, al√©m de verificar se o c√≥digo est√° funcionando, o CI deve garantir que os dados est√£o adequados e que o modelo est√° gerando resultados esperados.\n\n2. **CD (Continuous Delivery)** no ML:\n   - No ML, o CD vai al√©m de um pacote de software ou um servi√ßo. Ele se refere a um sistema (como um pipeline de treinamento de ML) que automaticamente entrega um servi√ßo (como um servi√ßo de predi√ß√£o).\n   - Isso significa que, ap√≥s o modelo ser treinado e validado, ele pode ser implantado automaticamente para servir predi√ß√µes.\n\n3. **CT (Continuous Training)**:\n   - √â uma propriedade nova e √∫nica do ML, que se refere √† retrainagem autom√°tica e √† entrega cont√≠nua dos modelos. Isso √© importante porque modelos podem precisar de atualiza√ß√µes frequentes √† medida que os dados mudam.\n\nEm resumo, no ML, o CI/CD √© uma abordagem mais abrangente que inclui a valida√ß√£o de dados e modelos, al√©m da entrega autom√°tica de pipelines de treinamento e predi√ß√£o.\n\nSe voc√™ quiser explorar mais sobre MLOps ou como implementar essas pr√°ticas, posso te ajudar! üòä",
        "retrieval_context": [
            "testing, integration testing, and continuous delivery of the software module or the package.\nHowever, in ML, there are a few notable differences:\nCI is no longer only about testing and validating code and components, but also\ntesting and validating data, data schemas, and models.\nCD is no longer about a single software package or a service, but a system (an ML\ntraining pipeline) that should automatically deploy another service (model prediction\nservice).\nCT is a new property, unique to ML systems, that's concerned with automatically\nretraining and serving the models.\nThe following section discusses the typical steps for training and evaluating an ML model\nto serve as a prediction service.\nData science steps for ML\nIn any ML project, after you define the business use case and establish the success criteria,\nthe process of delivering an ML model to production involves the following steps. These\nsteps can be completed manually or can be completed by an automatic pipeline.",
            "and automate themes from Google‚Äôs AI Adoption Framework. The decision about whether (or to which degree) to \nadopt each of these processes and capabilities in your organization depends on your business context. For exam -\nple, you must determine the business value that the framework creates when compared to the cost of purchasing or \nbuilding capabilities (for example, the cost in engineering hours).\nOverview of MLOps lifecycle \nand core capabilities\nDespite the growing recognition of AI/ML as a crucial pillar of digital transformation, successful deployments and \neffective operations are a bottleneck for getting value from AI. Only one in two organizations has moved beyond \npilots and proofs of concept. Moreover, 72% of a cohort of organizations that began AI pilots before 2019 have not \nbeen able to deploy even a single application in production.1 Algorithmia‚Äôs survey of the state of enterprise machine",
            "achieved by a shift in team culture. Recognizing, prioritiz ing, and rewarding this effort is important\nfor the long term health of successful ML teams.\nAcknowledgments\nThis paper owes much to the important lessons learned day to d ay in a culture that values both\ninnovative ML research and strong engineering practice. Ma ny colleagues have helped shape our\nthoughts here, and the beneÔ¨Åt of accumulated folk wisdom can not be overstated. We would like\nto speciÔ¨Åcally recognize the following: Roberto Bayardo, L uis Cobo, Sharat Chikkerur, Jeff Dean,\nPhilip Henderson, Arnar Mar Hrafnkelsson, Ankur Jain, Joe K ovac, Jeremy Kubica, H. Brendan\nMcMahan, Satyaki Mahalanabis, Lan Nie, Michael Pohl, Abdul Salem, Sajid Siddiqi, Ricky Shan,\nAlan Skelly, Cory Williams, and Andrew Young.\nA short version of this paper was presented at the SE4ML works hop in 2014 in Montreal, Canada.\n8",
            "rather than ML-specific metrics alone like MAP (P5, P7, P15, P16,\nP11, P17, P18, P19). The need to evaluate product-critical metrics\nstemmed from close collaboration with other stakeholders, such\nas product managers and business operators. P11 felt that a key\nreason many ML projects fail is that they don‚Äôt measure metrics\nthat will yield the organization value:\nTying to the business‚Äôs KPIs (key\nperformance indicators) is really important. But it‚Äôs a\nprocess‚Äîyou need to figure out what are, and\nfrankly I think that‚Äôs how people should be doing AI. It\n like: hey, let‚Äôs do these experiments and\nget cool numbers and show off these nice precision-recall\ncurves to our bosses and call it a day. It should be like:\nhey, let‚Äôs actually show the same business metrics that\neveryone else is held accountable to to our bosses at the\nend of the day.\nSince product-specific metrics are, by definition, different for\ndifferent ML models, it was important for engineers to treat choos-",
            "team. Each role contributes significantly to the success of AI and ML\nprojects but has unique skill sets and areas of focus. By understanding and\nappreciating these distinctions, organizations can more effectively allocate\nresources, encourage collaboration, and drive innovation in their machine\nlearning initiatives.\nIf you‚Äôre looking to become an ML Engineer check out our article about\nthe 5 skills y ou need t o be successful.\nWant t o become an ML Ops mast er? Sign up t o the ML Ops Now\nnewslett er to get w eekly ML Ops insights.\nUnlock your future in ML Ops with Navigating ML Ops: A\nBeginner' s Bluepr int.\nOther ar ticles y ou might be int erested in:\nYour Ultimat e Machine Learning Engineer R oadmap\n3 ways t o learn ML Ops quickly\nML Engineer vs Data Scientist: What' s the differ ence?\nMLOps Engineer vs Data Scientist: What' s the differ ence?\nMLOps Engineer vs ML Engineer : What' s the differ ence?"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.9166666666666666
            },
            {
                "gpt-oss:20b-cloud": null,
                "logs": "1 validation error for Verdicts\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"verdicts\"...no\"\\n    }\\n  ]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid"
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.0967741935483871
            },
            {
                "llama3.2:latest": 0.45714285714285713
            },
            {
                "gpt-oss:20b-cloud": 0.18181818181818182
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.75
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.8
            },
            {
                "llama3.2:latest": 0.3333333333333333
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "Por que monitorar um modelo em produ√ß√£o?",
        "expected_output": "Para detectar quedas de performance, drifts, falhas de pipeline, mudan√ßas de distribui√ß√£o e problemas de lat√™ncia ou disponibilidade.",
        "actual_output": "Monitorar um modelo em produ√ß√£o √© essencial porque, sem ele, os sistemas est√£o \"voando cegos\". O monitoramento identifica o **drift do modelo** ao longo do tempo, permitindo que a equipe de data science atue de forma **proativa** em vez de reativa. Al√©m disso, ele garante a **valida√ß√£o da precis√£o e confiabilidade** do modelo antes de coloc√°-lo em produ√ß√£o e ajuda a introduzir novos modelos atrav√©s de **A/B testing**, permitindo testar efic√°cia antes de adot√°-los. O monitoramento tamb√©m assegura a **continuidade do neg√≥cio** e a **efic√°cia preditiva** do modelo, evitando problemas inesperados e mantendo a performance otimizada.",
        "retrieval_context": [
            "Continuous Integration.¬†\nMonit oring identifies model drif t over time. Without model monitoring,\nproduction systems are flying blind. By monitoring for model drift the data\nscience team is able to proactively work rather than reactively.¬†\nTesting ensur es the accuracy and r eliability o f models. Validating both\nthe model‚Äôs predictions and the data sets used is a fundamental step in\ngreenlighting models for production.¬†\nUse A/B t esting t o identif y best models. A/B testing is sometimes\noverlooked in Machine Learning but is a great way to introduce new\nmodels. Rather than swapping models out straight away you can introduce\nthe new model alongside the old. This weighted approach allows you to\nsee the efficacy of the new model in production before committing to it.\n4. Version Contr ol\nVersion control is a significant aspect of ML Ops. It allows teams to track",
            "dation system has on click-throughs and on conversation rates. The results of online experimentation should be \nintegrated with the model registry capability to facilitate the decision about releasing the model to production. Online \nexperimentation enhances the reliability of your ML releases by helping you decide to discard ill-performing models \nand to promote well-performing ones. Key functionalities in online experimentation include the following:\n‚Ä¢ Support canary and shadow deployments.\n‚Ä¢ Support traffic splitting and A/B tests.\n‚Ä¢ Support multi-armed bandit (MAB) tests.\nModel monitoring\nThe model monitoring capability lets you track the efficiency and effectiveness of the deployed models in production \nto ensure predictive quality and business continuity. This capability informs you if your models are stale and need to \nbe investigated and updated. Key functionalities in model monitoring include the following:",
            "‚Ä¢ Hyperparameters, including trials of automated hyperparameter tuning and model selection.\n‚Ä¢ Information about training, validation, and testing data splits that were used. \n‚Ä¢ Model evaluation metrics and the validation procedure that was used.\nIf there is no need to retrain the model on a regular basis, then the produced model at the end of the experimenta -\ntion is submitted to the model registry. The model is then ready to be reviewed, approved, and deployed to the target \n18\nserving environment. In addition, all the relevant metadata and artifacts \nthat were produced during model development are tracked in the metadata \ntracking repository.\nHowever, in most cases, ML models need to be retrained on a regular basis \nwhen new data is available or when the code changes. In this case, the \noutput of the ML development process is not the model to be deployed in \nproduction. Instead, the output is the implementation of the continuous",
            "Continuous monitoring of model performance for accuracy drift, bias and other potential issues plays a critical role in maintaining the effectiveness of models and preventing unexpected outcomes. Monitoring the performance and health of ML models ensures that they continue to meet the intended objectives after deployment. By proactively identifying and addressing these concerns, organizations can maintain optimal model performance, mitigate risks and adapt to changing conditions or feedback.",
            "Monitoring and optimization\n\nIn the lifecycle of a deployed machine learning model, continuous vigilance ensures effectiveness and fairness over time. Model monitoring forms the cornerstone of this phase, involving the ongoing scrutiny of the model's performance in the production environment. This step helps identify emerging issues, such as accuracy drift, bias and concerns around fairness, which could compromise the model's utility or ethical standing. Monitoring is about overseeing the model's current performance and anticipating potential problems before they escalate."
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.7555555555555555
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 0.8875
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.4375
            },
            {
                "llama3.2:latest": 0.7222222222222222
            },
            {
                "gpt-oss:20b-cloud": 0.5
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.75
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.6666666666666666
            },
            {
                "llama3.2:latest": 0.6
            },
            {
                "gpt-oss:20b-cloud": null,
                "logs": "1 validation error for Claims\n  Invalid JSON: expected `,` or `]` at line 1 column 745 [type=json_invalid, input_value='{\"claims\":[\"Monitorar um...erformance otimizada.\"}', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid"
            }
        ]
    },
    {
        "input": "O que √© um model registry?",
        "expected_output": "Reposit√≥rio central para armazenar vers√µes de modelos, m√©tricas e hist√≥rico.",
        "actual_output": "Um **model registry** √© uma capacidade t√©cnica centralizada do MLOps (Machine Learning Operations) que gerencia o ciclo de vida dos modelos de machine learning. Seu principal objetivo √© garantir a qualidade, governan√ßa e disponibilidade dos modelos em produ√ß√£o.\n\n### Principais fun√ß√µes do model registry:\n1. **Registro e organiza√ß√£o**: Cadastra e organiza os modelos treinados e implantados.\n2. **Rastreamento de vers√µes**: Mant√©m hist√≥rico de vers√µes dos modelos, permitindo atualiza√ß√µes controladas.\n3. **Armazenamento de metadados**: Salva informa√ß√µes como depend√™ncias, documenta√ß√£o e m√©tricas de desempenho.\n4. **Governan√ßa**: Auxilia na aprova√ß√£o, libera√ß√£o e rollback de modelos, garantindo conformidade e qualidade.\n\n### Benef√≠cios:\n- Garante a **disponibilidade** e **reprodu√ß√£o** dos modelos em produ√ß√£o.\n- Facilita a **descoberta** e **manuten√ß√£o** dos modelos.\n- Integra-se com outras partes do MLOps, como o **model evaluation** e o **model serving**, para garantir que apenas modelos aprovados e validados sejam implantados.\n\nEm resumo, o model registry √© essencial para gerenciar, controlar e documentar os modelos de ML de forma eficiente, especialmente em ambientes de produ√ß√£o.",
        "retrieval_context": [
            "processing, model training, model evaluation, model serving, online experimentation, model monitoring, ML pipeline, \nand model registry. Finally, two cross-cutting capabilities that enable integration and interaction are an ML metadata \nand artifact repository and an ML dataset and feature repository.\nFigure 4. Core MLOps technical capabilities\n11\nThe following sections outline the characteristics of each of the MLOps capabilities.\nExperimentation \nThe experimentation capability lets your data scientists and ML researchers collaboratively perform exploratory data \nanalysis, create prototype model architectures, and implement training routines. An ML environment should also let \nthem write modular, reusable, and testable source code that is version controlled. Key functionalities in experimenta -\ntion include the following:\n‚Ä¢ Provide notebook environments that are integrated with version control tools like Git.",
            "‚Ä¢ Hyperparameters, including trials of automated hyperparameter tuning and model selection.\n‚Ä¢ Information about training, validation, and testing data splits that were used. \n‚Ä¢ Model evaluation metrics and the validation procedure that was used.\nIf there is no need to retrain the model on a regular basis, then the produced model at the end of the experimenta -\ntion is submitted to the model registry. The model is then ready to be reviewed, approved, and deployed to the target \n18\nserving environment. In addition, all the relevant metadata and artifacts \nthat were produced during model development are tracked in the metadata \ntracking repository.\nHowever, in most cases, ML models need to be retrained on a regular basis \nwhen new data is available or when the code changes. In this case, the \noutput of the ML development process is not the model to be deployed in \nproduction. Instead, the output is the implementation of the continuous",
            "following:\n‚Ä¢ Register, organize, track, and version your trained and deployed ML models.\n‚Ä¢ Store model metadata and runtime dependencies for deployability.\n‚Ä¢ Maintain model documentation and reporting‚Äîfor example, using model cards .\n‚Ä¢ Integrate with the model evaluation and deployment capability and track online and offline evaluation metrics \nfor the models.\n‚Ä¢ Govern the model launching process: review, approve, release, and roll back. These decisions are based on a \nnumber of offline performance and fairness metrics and on online experimentation results.\nDataset and feature repository\nThe dataset and feature repository capability lets you unify the definition and the storage of the ML data assets. \nHaving a central repository of fresh, high-quality data assets enables shareability, discoverability, and reusability. The \nrepository also provides data consistency for training and inference. This helps data scientists and ML researchers",
            "dation system has on click-throughs and on conversation rates. The results of online experimentation should be \nintegrated with the model registry capability to facilitate the decision about releasing the model to production. Online \nexperimentation enhances the reliability of your ML releases by helping you decide to discard ill-performing models \nand to promote well-performing ones. Key functionalities in online experimentation include the following:\n‚Ä¢ Support canary and shadow deployments.\n‚Ä¢ Support traffic splitting and A/B tests.\n‚Ä¢ Support multi-armed bandit (MAB) tests.\nModel monitoring\nThe model monitoring capability lets you track the efficiency and effectiveness of the deployed models in production \nto ensure predictive quality and business continuity. This capability informs you if your models are stale and need to \nbe investigated and updated. Key functionalities in model monitoring include the following:",
            "‚Ä¢ Trigger pipelines on demand, on a schedule, or in response to specified events.\n‚Ä¢ Enable local interactive execution for debugging during ML development.\n‚Ä¢ Integrate with the ML metadata tracking capability to capture pipeline execution parameters and to produce \nartifacts.\n‚Ä¢ Provide a set of built-in components for common ML tasks and also allow custom components.\n‚Ä¢ Run on different environments, including local machines and scalable cloud platforms.\n‚Ä¢ Optionally, provide GUI-based tools for designing and building pipelines.\nModel registry\nThe model registry capability lets you govern the lifecycle of the ML models in a central repository. This ensures the \nquality of the production models and enables model discovery. Key functionalities in the model registry include the \nfollowing:\n‚Ä¢ Register, organize, track, and version your trained and deployed ML models.\n‚Ä¢ Store model metadata and runtime dependencies for deployability."
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.9166666666666666
            },
            {
                "gpt-oss:20b-cloud": 0.8928571428571428
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.40425531914893614
            },
            {
                "llama3.2:latest": 0.40816326530612246
            },
            {
                "gpt-oss:20b-cloud": 0.3157894736842105
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.7
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.8181818181818182
            },
            {
                "llama3.2:latest": 0.6666666666666666
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que √© data validation em MLOps?",
        "expected_output": "√â o processo automatizado de verificar integridade, schema, ranges e consist√™ncia de dados antes de alimentar o pipeline de ML.",
        "actual_output": "**Data validation em MLOps** √© o processo de verificar a qualidade, integridade e conformidade dos dados antes de utiliz√°-los em modelos de machine learning ou sistemas de produ√ß√£o. Esse passo √© crucial para garantir que os dados estejam livres de erros, consistentes e adequados ao contexto do projeto, evitando problemas como previs√µes imprecisas ou falhas no sistema.\n\nNo MLOps, a data validation √© realizada em diferentes est√°gios, como durante a coleta, limpeza e prepara√ß√£o dos dados, e tamb√©m em ciclos de valida√ß√£o cont√≠nua durante o despliegue e monitoramento. Ferramentas de MLOps, como experiment tracking e pipelines automatizados, podem ser usadas para validar dados de forma eficiente.\n\nAl√©m disso, a data validation ajuda a identificar e corrigir problemas antes que os dados passem para o modelo, garantindo que o sistema final atenda √†s necessidades dos usu√°rios e mantenha a qualidade das previs√µes.",
        "retrieval_context": [
            "this phase, data engineers work together with data scientists to prepare\nand preprocess the data, performing featur e engineering to ensure the\ndata has the right format and structure.\nDuring model creation, various data pipelines are developed, enabling the\nsmooth flow of information between the different stages of the machine\nlearning process. T ools such as data engineering platforms can be used to\ndesign, test and maintain these pipelines.\nModel T raining\nOnce the model has been created, it is trained using a suitable dataset.\nModel training is an iterative process that involves feeding data into the\nmodel for it to learn and make predictions. The model is continually\nadjusted, and its performance is evaluated against a validation dataset to\nfine-tune its accuracy and effectiveness.\nSeveral techniques can be applied during the model training phase,\nincluding hyperparameter optimisation, cross-validation, and\nregularisation. Utilising the right combination of these methods helps",
            "needs to support data scientists. Without their feedback, the system won‚Äôt\nmeet user needs. R egular feedback sessions with data scientists can\nstimulate open communication and improve outcomes.¬†\nUse collaboration t ools t o help communication. Collaboration tools assist\nin enhancing communication and project management, mitigating\ntechnical debt, and facilitating continuous delivery.\n2. Aut omating Pr ocesses\nAutomation is a cornerstone of ML Ops, improving efficiency and accuracy.\nHere are some best practices for it:11/11/25, 10:04 PM MLOps Now - MLOps Best Practices and Challenges\nhttps://mlopsnow.com/blog/mlops-best-practices-and-challenges/ 2/7\nAutomat e data collection, cleaning and pr eparation. One significant\naspect of automation in ML Ops is the handling of data. Automating\ncollection, cleaning, and preparation improves efficiency drastically when\nimproving models. It‚Äôs also an important practice for ensuring data\nvalidation and managing new data.",
            "data collection and processing, experimentation, evaluation and de-\nployment, and monitoring and response, as shown in Figure 1. Sev-\neral research papers and companies have proposed tools to accom-\nplish various tasks in the workflow, such as data pre-processing [ 22,\n58,60] and experiment tracking . Crankshaw et al. stud-\nied the problem of model deployment and low-latency prediction\nserving . With regards to validating changes in production sys-\ntems, some researchers have studied CI (Continuous Integration) for\nML and proposed preliminary solutions‚Äîfor example, ease.ml/ci\nstreamlines data management and proposes unit tests for overfit-\nting , Garg et al. survey different MLOps tools , and some\npapers introduce tools to perform validation and monitoring in\nproduction ML pipelines .\nMLOps Challenges. Sculley et al. were early proponents that pro-\nduction ML systems raise special challenges and can be hard tomaintain over time, based on their experience at Google . Since",
            "different failure modes in the offline validation phase‚Äîe.g., perfor-\nmance drops in subpopulations users might care deeply about‚ÄîlikeP11 did, P8 offered a reactive strategy of spawning a new dataset\nfor each observed live failure: ‚ÄúEvery gets into\nthe same queue, and 3 of us sit down once a week and go through\nthe queue...then our collect more data.‚Äù This\nnew dataset was then used in the offline validation phase in future\niterations of the production ML lifecycle.\nWhile processes to dynamically update the validation datasets\nranged from human-in-the-loop to frequent synthetic data con-\nstruction (P6), we found that higher-stakes applications of ML (e.g.,\nautonomous vehicles), created separate teams to manage the dy-\nnamic evaluation process. P1 said:\nWe had to move away from only aggregate metrics like\nMAP towards the ability to curate scenarios of interest,\nand then validate model performance on them specifi-\ncally. So, as an example, you can‚Äôt hit pedestrians, right.",
            "https://mlopsnow.com/blog/what-is-mlops/ 2/11\nTo understand ML Ops, it‚Äôs essential to be familiar with the development\nlifecycle of data science projects. A typical data science project consists of\nseveral stages:\n1. Data acquisition: Obtaining raw data from various sources, such as\ndatabases, sensors, or external APIs.\n2. Data pr eprocessing: Cleaning, transforming, and structuring the data\nto prepare it for analysis.\n3. Featur e engineering: Selecting the most relevant data attributes, or\n‚Äúfeatures,‚Äù and converting them into a suitable format for ML\nalgorithms.\n4. Model training: Applying ML algorithms to the preprocessed data to\ncreate a predictive model.\n5. Model ev aluation: Assessing the performance of the model and\nmaking adjustments to improve its accuracy.\n6. Model deployment: Implementing the ML model into a product,\nservice, or system.\n7. Monit oring and maint enance: Continuously monitoring the\nperformance of the ML model and updating it as needed."
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.9166666666666666
            },
            {
                "gpt-oss:20b-cloud": 0.5
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.4489795918367347
            },
            {
                "llama3.2:latest": 0.8333333333333334
            },
            {
                "gpt-oss:20b-cloud": 0.3870967741935484
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 1.0
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.4
            },
            {
                "llama3.2:latest": 0.5555555555555556
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "Diferen√ßa entre batch inference e online inference?",
        "expected_output": "Batch inference processa grandes volumes de dados periodicamente, enquanto online inference responde em tempo real para requisi√ß√µes individuais.",
        "actual_output": "A **batch inference** e a **online inference** s√£o duas abordagens diferentes para aplicar modelos de machine learning, principalmente em termos de como e quando os dados s√£o processados. Vamos entender as diferen√ßas principais:\n\n### Batch Inference (Inf√©rno em Lotes)\n- **Processamento em Lotes**: Neste m√©todo, os dados s√£o processados em lotes grandes de uma vez. O modelo recebe um conjunto de dados (batch) e gera as previs√µes para todos os registros desse lote.\n- **Uso Comum**: Geralmente utilizado para tarefas de treinamento cont√≠nuo ou previs√£o em lote, onde os dados s√£o coletados ao longo do tempo e processados em uma √∫nica execu√ß√£o.\n- **Exemplo**: Um sistema de recomenda√ß√£o que processa os dados de todos os usu√°rios no final do dia para gerar recomenda√ß√µes para o pr√≥ximo dia.\n\n### Online Inference (Inf√©rno Online)\n- **Processamento em Tempo Real**: Neste m√©todo, o modelo processa os dados um de cada vez, fornecidos de forma cont√≠nua. Cada solicita√ß√£o de entrada √© processada individualmente e a previs√£o √© gerada imediatamente.\n- **Uso Comum**: Utilizado em sistemas que precisam de respostas em tempo real, como recomenda√ß√µes em tempo real ou previs√µes baseadas em dados atualizados.\n- **Exemplo**: Um sistema de recomenda√ß√£o que gera uma recomenda√ß√£o imediatamente quando um usu√°rio faz uma a√ß√£o espec√≠fica, como clicar em um produto.\n\n### Principais Diferen√ßas\n1. **Tempo de Processamento**:\n   - **Batch**: O processamento ocorre em intervalos de tempo definidos.\n   - **Online**: O processamento ocorre em tempo real, com cada solicita√ß√£o sendo tratada individualmente.\n\n2. **Volume de Dados**:\n   - **Batch**: Trata grandes volumes de dados de uma vez.\n   - **Online**: Trata pequenas quantidades de dados, uma solicita√ß√£o de cada vez.\n\n3. **Recursos**:\n   - **Batch**: Pode ser mais eficiente para grandes volumes de dados, utilizando recursos em lote.\n   - **Online**: Requer menor poder de processamento por solicita√ß√£o, mas com alta disponibilidade.\n\n4. **Atualiza√ß√£o dos Modelos**:\n   - **Batch**: Os modelos s√£o atualizados periodicamente, ap√≥s o processamento do lote.\n   - **Online**: Os modelos podem ser atualizados em tempo real, conforme novos dados s√£o processados.\n\n### Conclus√£o\nA escolha entre batch e online inference depende das necessidades espec√≠ficas do sistema. Se voc√™ precisa de previs√µes r√°pidas e em tempo real, a **online inference** √© a melhor op√ß√£o. Por outro lado, se voc√™ est√° lidando com grandes volumes de dados e precisa de processamento eficiente, a **batch inference** pode ser mais adequada.\n\nEspero que isso tenha esclarecido suas d√∫vidas! üòä",
        "retrieval_context": [
            "store to run their experiments.\nFor continuous training, the automated ML training pipeline can fetch a batch of\nthe up-to-date feature values of the dataset that are used for the training task.\nFor online prediction, the prediction service can fetch in a batch of the feature\nvalues related to the requested entity, such as customer demographic features,\nproduct features, and current session aggregation features.\nFor online prediction and feature retrieval, the prediction service identifies the\nrelevant features for an entity. For example, if the entity is a customer, relevant\nfeatures might include age, purchase history, and browsing behavior. The service\nbatches these feature values together and retrieves all the needed features for\nthe entity at once, rather than individually. This retrieval method helps with\nefficiency, especially when you need to manage multiple entities.",
            "‚Ä¢ Enable composite prediction routines, where multiple models are invoked hierarchically or simultaneously \nbefore the results are aggregated, in addition to any required pre- or post-processing routines.\n‚Ä¢ Allow efficient use of ML inference accelerators with autoscaling to match spiky workloads and to balance \n13\ncost with latency.\n‚Ä¢ Support model explainability using techniques like feature attributions for a given model prediction.\n‚Ä¢ Support logging of prediction serving requests and responses for analysis. \nOnline experimentation\nThe online experimentation capability lets you understand how newly trained models perform in production settings \ncompared to the current models (if any) before you release the new model to production. For example, using a small \nsubset of the serving population, you use online experimentation to understand the impact that a new recommen -\ndation system has on click-throughs and on conversation rates. The results of online experimentation should be",
            "dation system has on click-throughs and on conversation rates. The results of online experimentation should be \nintegrated with the model registry capability to facilitate the decision about releasing the model to production. Online \nexperimentation enhances the reliability of your ML releases by helping you decide to discard ill-performing models \nand to promote well-performing ones. Key functionalities in online experimentation include the following:\n‚Ä¢ Support canary and shadow deployments.\n‚Ä¢ Support traffic splitting and A/B tests.\n‚Ä¢ Support multi-armed bandit (MAB) tests.\nModel monitoring\nThe model monitoring capability lets you track the efficiency and effectiveness of the deployed models in production \nto ensure predictive quality and business continuity. This capability informs you if your models are stale and need to \nbe investigated and updated. Key functionalities in model monitoring include the following:",
            "behavior of a given model before it is released. These feedba ck loops can take different forms, but\nthey are all more difÔ¨Åcult to detect and address if they occur gradually over time, as may be the case\nwhen models are updated infrequently.\nDirect Feedback Loops. A model may directly inÔ¨Çuence the selection of its own future training\ndata. It is common practice to use standard supervised algor ithms, although the theoretically correct\nsolution would be to use bandit algorithms. The problem here is that bandit algorithms (such as\ncontextual bandits ) do not necessarily scale well to the size of action spaces typically required for\nreal-world problems. It is possible to mitigate these effec ts by using some amount of randomization\n, or by isolating certain parts of data from being inÔ¨Çuenc ed by a given model.\nHidden Feedback Loops. Direct feedback loops are costly to analyze, but at least the y pose a",
            "stractions to support ML systems. Zheng recently made a comp elling comparison of the state ML\nabstractions to the state of database technology , maki ng the point that nothing in the machine\nlearning literature comes close to the success of the relati onal database as a basic abstraction. What\nis the right interface to describe a stream of data, or a model , or a prediction?\nFor distributed learning in particular, there remains a lac k of widely accepted abstractions. It could\nbe argued that the widespread use of Map-Reduce in machine le arning was driven by the void of\nstrong distributed learning abstractions. Indeed, one of t he few areas of broad agreement in recent\nyears appears to be that Map-Reduce is a poor abstraction for iterative ML algorithms.\n5\nThe parameter-server abstraction seems much more robust, b ut there are multiple competing speci-\nÔ¨Åcations of this basic idea . The lack of standard abst ractions makes it all too easy to blur the\nlines between components."
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.9166666666666666
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.13157894736842105
            },
            {
                "llama3.2:latest": 0.64
            },
            {
                "gpt-oss:20b-cloud": 0.12121212121212122
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.7142857142857143
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.5454545454545454
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que √© feature drift?",
        "expected_output": "√â a mudan√ßa na distribui√ß√£o estat√≠stica das features usadas pelo modelo ao longo do tempo.",
        "actual_output": "**Resposta:**\n\nO **feature drift** √© um fen√¥meno relacionado √† mudan√ßa no comportamento ou na distribui√ß√£o das **features** (caracter√≠sticas ou vari√°veis) utilizadas pelos modelos de machine learning ao longo do tempo. Ele ocorre quando as features que alimentam o modelo mudam de forma significativa em rela√ß√£o aos dados usados durante o treinamento, fazendo com que o modelo comece a fazer predi√ß√µes menos precisas.\n\n### Por que √© importante?\n- **Performance do modelo:** Se as features mudam, o modelo pode come√ßar a dar resultados imprecisos.\n- **Reentrenamento:** Para manter a precis√£o, √© comum reentrenar os modelos periodicamente para adapt√°-los √†s novas features.\n\n### Exemplos de como o feature drift pode acontecer:\n1. **Legacy Features:** Uma feature antiga que j√° n√£o √© mais relevante pode permanecer no modelo por muito tempo, sem que se note que ela est√° redundante.\n2. **Bundled Features:** Adicionar um grupo de features ao modelo sem analisar se todas s√£o realmente √∫teis pode trazer caracter√≠sticas desnecess√°rias.\n3. **Efeito de Correla√ß√£o:** Duas features fortemente correlacionadas podem causar problemas se uma delas for mais inst√°vel ou menos confi√°vel que a outra.\n\n### Consequ√™ncias:\n- **Perda de confiabilidade:** O modelo pode come√ßar a dar resultados ruins se as features mudarem sem que isso seja detectado.\n- **Custos operacionais:** Corrigir problemas de feature drift pode ser custoso, especialmente se o modelo estiver em produ√ß√£o.\n\n### Como evitar ou detectar:\n- **Monitoramento:** √â essencial verificar periodicamente se as features est√£o mantendo a mesma distribui√ß√£o dos dados de treino.\n- **Reentrenamento:** Atualizar o modelo com os novos dados e features regularmente.\n\nEm resumo, o **feature drift** √© uma mudan√ßa nas features que afeta a performance do modelo, e √© algo que precisa ser monitorado e gerenciado para garantir que o modelo continue preciso e √∫til.",
        "retrieval_context": [
            "Data versioning plays a pivotal role in maintaining the integrity and reproducibility of data analysis. It involves tracking and managing different versions of the data, allowing for traceability of results and the ability to revert to previous states if necessary. Versioning ensures that others can replicate and verify analyses, promoting transparency and reliability in data science projects.\n\nThe concept of a feature store is then introduced as a centralized repository for storing and managing features used in model training. Feature stores promote consistency and reusability of features across different models and projects. By having a dedicated system for feature management, teams can ensure they use the most relevant and up-to-date features.\n\nModel development",
            "some products. A year later, the code that stops populating t he database with the old numbers is\ndeleted. This will not be a good day for the maintainers of the ML system.\nUnderutilized data dependencies can creep into a model in se veral ways.\n‚Ä¢Legacy Features. The most common case is that a feature Fis included in a model early in\nits development. Over time, Fis made redundant by new features but this goes undetected.\n‚Ä¢Bundled Features. Sometimes, a group of features is evaluated and found to be be neÔ¨Åcial.\nBecause of deadline pressures or similar effects, all the fe atures in the bundle are added to\nthe model together, possibly including features that add li ttle or no value.\n‚Ä¢«´-Features. As machine learning researchers, it is tempting to improve m odel accuracy\neven when the accuracy gain is very small or when the complexi ty overhead might be high.\n‚Ä¢Correlated Features. Often two features are strongly correlated, but one is more d irectly",
            "Consider the following examples. Feature Awas incorrectly logged from 9/14 to 9/17. Feature Bis\nnot available on data before 10/7. The code used to compute fe atureChas to change for data before\nand after 11/1 because of changes to the logging format. Feat ureDis not available in production, so\na substitute features D‚Ä≤andD‚Ä≤‚Ä≤must be used when querying the model in a live setting. If feat ure\nZis used, then jobs for training must be given extra memory due to lookup tables or they will train\ninefÔ¨Åciently. Feature Qprecludes the use of feature Rbecause of latency constraints.\nAll this messiness makes conÔ¨Åguration hard to modify correc tly, and hard to reason about. How-\never, mistakes in conÔ¨Åguration can be costly, leading to ser ious loss of time, waste of computing\nresources, or production issues. This leads us to articulat e the following principles of good conÔ¨Ågu-\nration systems:\n‚Ä¢It should be easy to specify a conÔ¨Åguration as a small change f rom a previous conÔ¨Åguration.",
            "participants cited Covid as an example, but there are other\n(better) everyday instances of unnatural data drift. P6 de-\nscribed a bug where users had inconsistent definitions of the\nsame word, complicating the deployment of a service to a\nnew user. P7 mentioned a bug where data from users in a\ncertain geographic region arrived more sporadically than\nusual. P10 discussed a bug where the format of raw data was\noccasionally corrupted: ‚ÄúTables didn‚Äôt always have headers\nin the same place, even though they were the same tables.‚Äù\n‚Ä¢Natural data drift: Surprisingly, participants didn‚Äôt seem\ntoo worried about slower, expected natural data drift over\ntime‚Äîthey noted that frequent model retrains solved this\nproblem (P6, P7, P8, P12, P15, P16, P17). As an anecdote, we\nasked P17 to give an example of a natural data drift problem\ntheir company faced, and they could not think of a good\nexample. P14 also said they don‚Äôt have natural data drift\nproblems:",
            "asked P17 to give an example of a natural data drift problem\ntheir company faced, and they could not think of a good\nexample. P14 also said they don‚Äôt have natural data drift\nproblems:\n5Goldilocks and the Three Bears is a popular Western fairy tale. Goldilocks, the main\ncharacter, looks for things that are not too big or not too small, things that are ‚Äújust\nright.‚Äù\nOperationalizing Machine Learning: An Interview Study\nThe model gets retrained every day, so we don‚Äôt have the\nscenario of like: Oh, our models got stale and we need to re-\ntrain it because it‚Äôs starting to make mistakes because data\nhas drifted...fortunately we‚Äôve never had to deal with [such\na] scenario. Sometimes there are bad jobs, but\nwe can always effectively roll back to a different .\nHowever, a few engineers mentioned that natural data shift\ncould cause some hand-curated features and data quality\nchecks to corrupt (P3, P6, P8). P6 discussed a histogram used"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.7555555555555555
            },
            {
                "llama3.2:latest": 0.9166666666666666
            },
            {
                "gpt-oss:20b-cloud": 0.4777777777777777
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.21052631578947367
            },
            {
                "llama3.2:latest": 0.7567567567567568
            },
            {
                "gpt-oss:20b-cloud": 0.19444444444444445
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.5833333333333334
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.6363636363636364
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que √© concept drift?",
        "expected_output": "√â quando a rela√ß√£o entre as features e a vari√°vel alvo muda, tornando o modelo menos eficaz.",
        "actual_output": "Concept drift (deslocamento de conceito) √© um fen√¥meno em sistemas de machine learning onde as rela√ß√µes entre os dados de entrada e os valores alvo mudam ao longo do tempo, fazendo com que o modelo, treinado com dados hist√≥ricos, comece a dar respostas menos precisas. √â como se os dados \"fossem diferentes\" sem que o modelo perceba, o que pode acontecer naturalmente ou devido a mudan√ßas externas.\n\nNo contexto, menciona que o concept drift √© dif√≠cil de detectar e pode levar a uma degrada√ß√£o da performance do modelo. Alguns entrevistados at√© disseram que nunca tiveram problemas com isso, mas √© importante monitor√°-lo para garantir que o modelo continue funcionando bem.",
        "retrieval_context": [
            "some products. A year later, the code that stops populating t he database with the old numbers is\ndeleted. This will not be a good day for the maintainers of the ML system.\nUnderutilized data dependencies can creep into a model in se veral ways.\n‚Ä¢Legacy Features. The most common case is that a feature Fis included in a model early in\nits development. Over time, Fis made redundant by new features but this goes undetected.\n‚Ä¢Bundled Features. Sometimes, a group of features is evaluated and found to be be neÔ¨Åcial.\nBecause of deadline pressures or similar effects, all the fe atures in the bundle are added to\nthe model together, possibly including features that add li ttle or no value.\n‚Ä¢«´-Features. As machine learning researchers, it is tempting to improve m odel accuracy\neven when the accuracy gain is very small or when the complexi ty overhead might be high.\n‚Ä¢Correlated Features. Often two features are strongly correlated, but one is more d irectly",
            "object recognition, probabilities or likelihoods as embeddings). P1\ndescribed a push at their company to rely more on neural networks:\nA general trend is to try to move more into the neural\nnetwork, and to combine models wherever possible so\nthere are fewer bigger models. Then you don‚Äôt have\nthese intermediate dependencies that cause drift and\nperformance regressions...you eliminate entire classes of\nbugs and and issues by consolidating all these different\npiecemeal stacks.\n4.5.6 Organizationally Supporting ML Engineers Requires Delib-\nerate Practices. Our interviewees reported various organizational\nprocesses for sustaining models as part of their ML infrastructure.\nP6, P12, P14, P16, P18, and P19 described on-call processes for su-\npervising production ML models. For each model, at any point in\ntime, some ML engineer would be on call, or primarily responsible\nfor it. Any bug or incident observed (e.g., user complaint, pipeline",
            "participants cited Covid as an example, but there are other\n(better) everyday instances of unnatural data drift. P6 de-\nscribed a bug where users had inconsistent definitions of the\nsame word, complicating the deployment of a service to a\nnew user. P7 mentioned a bug where data from users in a\ncertain geographic region arrived more sporadically than\nusual. P10 discussed a bug where the format of raw data was\noccasionally corrupted: ‚ÄúTables didn‚Äôt always have headers\nin the same place, even though they were the same tables.‚Äù\n‚Ä¢Natural data drift: Surprisingly, participants didn‚Äôt seem\ntoo worried about slower, expected natural data drift over\ntime‚Äîthey noted that frequent model retrains solved this\nproblem (P6, P7, P8, P12, P15, P16, P17). As an anecdote, we\nasked P17 to give an example of a natural data drift problem\ntheir company faced, and they could not think of a good\nexample. P14 also said they don‚Äôt have natural data drift\nproblems:",
            "asked P17 to give an example of a natural data drift problem\ntheir company faced, and they could not think of a good\nexample. P14 also said they don‚Äôt have natural data drift\nproblems:\n5Goldilocks and the Three Bears is a popular Western fairy tale. Goldilocks, the main\ncharacter, looks for things that are not too big or not too small, things that are ‚Äújust\nright.‚Äù\nOperationalizing Machine Learning: An Interview Study\nThe model gets retrained every day, so we don‚Äôt have the\nscenario of like: Oh, our models got stale and we need to re-\ntrain it because it‚Äôs starting to make mistakes because data\nhas drifted...fortunately we‚Äôve never had to deal with [such\na] scenario. Sometimes there are bad jobs, but\nwe can always effectively roll back to a different .\nHowever, a few engineers mentioned that natural data shift\ncould cause some hand-curated features and data quality\nchecks to corrupt (P3, P6, P8). P6 discussed a histogram used",
            "On availability of new training data: New data isn't systematically available for the ML\nsystem and instead is available on an ad hoc basis when new data is collected and\nmade available in the source databases.11/13/25, 11:39 PM MLOps: Continuous delivery and automation pipelines in machine learning | Cloud Architecture Center | Google Cloud Do‚Ä¶\nhttps://docs.cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning 12/18\nOn model performance degradation: The model is retrained when there is noticeable\nperformance degradation.\nOn significant changes in the data distributions (concept drift\n¬†(https://en.wikipedia.org/wiki/Concept_drift)). It's hard to assess the complete\nperformance of the online model, but you notice significant changes on the data\ndistributions of the features that are used to perform the prediction. These changes\nsuggest that your model has gone stale, and that needs to be retrained on fresh data.\nChallenges"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.8541666666666666
            },
            {
                "gpt-oss:20b-cloud": 0.2
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.34375
            },
            {
                "llama3.2:latest": 0.8
            },
            {
                "gpt-oss:20b-cloud": 0.3939393939393939
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.8333333333333334
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 0.8888888888888888
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.7142857142857143
            },
            {
                "llama3.2:latest": 0.6
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "Para que serve o MLflow?",
        "expected_output": "Gerenciar experimentos, registrar m√©tricas e armazenar modelos.",
        "actual_output": "O **MLflow** √© uma plataforma open source projetada para simplificar o desenvolvimento e gerenciamento de pipelines de machine learning. Suas principais fun√ß√µes s√£o:\n\n1. **Orquestra√ß√£o de pipelines**: Ajuda a gerenciar e automatizar o fluxo completo do desenvolvimento de modelos, desde a experimenta√ß√£o at√© a implanta√ß√£o em produ√ß√£o.\n\n2. **Acelera√ß√£o do desenvolvimento**: Fornece uma interface unificada para experimentar, registrar, reutilizar e implantar modelos, facilitando o trabalho dos cientistas de dados e engenheiros de ML.\n\n3. **Reprodutibilidade e colabora√ß√£o**: Garante que experimentos e modelos possam ser facilmente compartilhados e reutilizados por diferentes equipes.\n\nAl√©m disso, como √© uma ferramenta open source, √© amplamente utilizada por empresas e desenvolvedores para construir solu√ß√µes personalizadas de MLOps. üåü",
        "retrieval_context": [
            "https://mlopsnow.com/blog/what-is-mlops/ 7/11\ndesigned architecture ensures smooth collaboration between different\nteams and streamlines the entire machine learning lifecycle.\nData Collection and Data Pr ep play a critical role in the ML Ops\narchitecture. T o build accurate and reliable machine learning models, it is\nessential to have high-quality data from various sources. Data engineers\nand data scientists work together, leveraging tools like Google Cloud\nStorage and BigQuer y to collect, store, and preprocess the data, making it\nsuitable for model training.\nOrchestration and T esting T ools\nOrchestration in ML Ops involves managing and automating the end-to-\nend machine learning pipeline. It plays a significant role in simplifying\ncomplex workflows and facilitating collaboration. Some widely-used\norchestration tools include Kubeflow , Apache Air flow, and MLflow .\nIn addition to orchestration tools, ML Ops practices also focus on testing",
            "workflow must be capable of scaling seamlessly. The infrastructure\nsupporting ML applications ought to be elastic, adjusting resources based\non the workload automatically.\nOptimisation plays a crucial role in maximising the performance and\nefficiency of machine learning models. T echniques such as hyperparameter\ntuning, model pruning, and implementation of efficient algorithms enable\nthe creation of highly performant models without sacrificing accuracy.\nAdditionally, monitoring tools can provide insights into model11/11/25, 9:50 PM MLOps Now - The MLOps Platform: Revolutionising Machine Learning Efficiency\nhttps://mlopsnow.com/blog/mlops-platforms-revolutionising-machine-learning/ 5/10\nperformance, allowing for proactive optimisation to maintain satisfactory\nresults.\nLeveraging Open Sour ce Tools and\nFramew orks\nMLflow and Kubeflow\nLeveraging open source tools and frameworks such as MLflow and\nKubeflow can play a significant role in accelerating the development and",
            "8\n‚Ä¢ Prediction serving is about serving the model that is deployed in production for inference.\n‚Ä¢ Continuous monitoring is about monitoring the effectiveness and efficiency of a deployed model.\n‚Ä¢ Data and model management is a central, cross-cutting function for governing ML artifacts to support audit -\nability, traceability, and compliance. Data and model management can also promote shareability, reusability, \nand discoverability of ML assets.\nMLOps: An end-to-end workflow\nFigure 3 shows a simplified but canonical flow for how the MLOps processes interact with each other, focusing on \nhigh-level flow of control and on key inputs and outputs.\nThis is not a waterfall workflow that has to sequentially pass through all the processes. The processes can be \nskipped, or the flow can repeat a given phase or a subsequence of the processes. The diagram shows the following \nflow:\n1. The core activity during this ML development phase is experimentation. As data scientists and ML research -",
            "on Kandel et al.‚Äôs work, exploring aspects such as collaboration,\ncode practices, and tools , all centered on gen-\neral data analysis and data science, as opposed to transitioning\nworkflows in ML to production. Many ML-related interview stud-\nies focus on a single tool, task, or challenge in the workflow‚Äîfor\nexample, AutoML , data iteration , model training ,\nminimizing bias in ML models , and building infras-\ntructure for ML pipelines . Sambasivan et al. study data\nquality issues during machine learning, as opposed to challenges\nin MLOps. Other ML-related interview studies focus on specific\napplications of ML, such as medicine , customer service ,\nand interview processing . Some interview studies report on\nsoftware engineering practices for ML development; however, they\nfocus only on a few applications and primarily on engineering, not\noperational, challenges . Our interview study aims to be both\nbroad and focused: we consider many applications and companies,",
            "Takeaway. The MLOps anti-patterns described in this section re-\nveal that ML engineering, as a field, is changing faster than educa-\ntional resources can keep up. We see this as opportunities for new\nresources, such as classroom material (e.g., textbooks, courses) to\nprescribe the right engineering practices and rigor for the highly\nexperimental discipline that is production ML, and automated doc-\numentation assistance for ML pipelines in organizations.\n5.3 Characterizing the ‚ÄúMLOps Stack‚Äù for Tool\nBuilders\nMLOps tool builders may be interested in an organization of the\ndozens of tools, libraries, and services MLEs use to run ML and\ndata processing pipelines. Although multiple MLEs reported hav-\ning to ‚Äúglue‚Äù open-source solutions together and having to build\n‚Äúhomegrown‚Äù infrastructure as part of their work (P1, P2, P5, P6,\nP10, P12), an analysis of the various deployments reveals that tools\nShreya Shankar‚àó, Rolando Garcia‚àó, Joseph M. Hellerstein, Aditya G. Parameswaran\nRun Layer"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.7555555555555555
            },
            {
                "llama3.2:latest": 0.9166666666666666
            },
            {
                "gpt-oss:20b-cloud": null,
                "logs": "1 validation error for Verdicts\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"verdicts\"...\".\"\\n    }\\n  ]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid"
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.5277777777777778
            },
            {
                "llama3.2:latest": 0.6
            },
            {
                "gpt-oss:20b-cloud": 0.0967741935483871
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.7142857142857143
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": null,
                "logs": "1 validation error for Statements\n  Invalid JSON: expected `,` or `]` at line 1 column 844 [type=json_invalid, input_value='{\"statements\":[\"MLflow ...sonalizadas de MLOps.\"}', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid"
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.5555555555555556
            },
            {
                "llama3.2:latest": 0.6666666666666666
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que √© canary deployment em ML?",
        "expected_output": "√â uma estrat√©gia onde uma nova vers√£o do modelo √© implantada para uma pequena parcela do tr√°fego para validar performance antes do rollout completo.",
        "actual_output": "Okay, vamos entender o que √© **canary deployment** em Machine Learning!\n\nPelo contexto, podemos ver que o canary deployment est√° relacionado ao processo de **deploy** de modelos ML. Especificamente, o texto menciona:\n\n> \"production deployment of a new version of an ML model usually goes through A/B testing or **online experiments** before the model is promoted to serve all the prediction request traffic.\"\n\nE mais tarde:\n\n> \"For example, many organizations start by focusing on the processes for ML development, model deployment, and prediction serving. [...] continuous training and continuous monitoring might not be necessary if they are piloting a relatively small number of ML systems.\"\n\nEmbora o texto n√£o defina explicitamente \"canary deployment\", o conceito de **teste gradual** antes de promover o modelo para todo o tr√°fego √© exatamente o que caracteriza o canary deployment.\n\n**Ent√£o, o canary deployment em ML √©:**\n\nUm m√©todo de **implanta√ß√£o** onde uma nova vers√£o de um modelo ML (ou qualquer software) √© lan√ßada para servir apenas uma **pequena parte** do tr√°fego (como 5% ou 10%) inicialmente, em vez de 100%.\n\n**Por que √© usado?**\n\n*   **Redu√ß√£o de riscos:** Se algo der errado com a nova vers√£o, o impacto √© limitado apenas para aquele grupo de usu√°rios (ou tr√°fego) espec√≠fico.\n*   **Monitoramento precoce:** √â poss√≠vel monitorar o desempenho e a estabilidade da nova vers√£o em um ambiente de produ√ß√£o controlado antes de exp√¥-la a todo o tr√°fego.\n*   **Promo√ß√£o gradual:** Se a nova vers√£o funcionar bem, ela pode ser gradualmente exposta para um maior percentual de tr√°fego at√© dominar completamente o servi√ßo.\n\n√â uma pr√°tica comum no desenvolvimento de software tradicional e tamb√©m aplicada no ML para garantir que as novas vers√µes de modelos funcionem bem antes de afetar todos os usu√°rios.",
        "retrieval_context": [
            "Deployment refers to the prediction service: The process is concerned only with\ndeploying the trained model as a prediction service (for example, a microservice with\na REST API), rather than deploying the entire ML system.\nLack of active performance monitoring: The process doesn't track or log the model\npredictions and actions, which are required in order to detect model performance\ndegradation and other model behavioral drifts.\nThe engineering team might have their own complex setup for API configuration, testing,\nand deployment, including security, regression, and load and canary testing. In addition,\nproduction deployment of a new version of an ML model usually goes through A/B testing\nor online experiments before the model is promoted to serve all the prediction request\ntraffic.\nChallenges\nMLOps level 0 is common in many businesses that are beginning to apply ML to their use\ncases. This manual, data-scientist-driven process might be sufficient when models are",
            "seamless and efficient integration of ML into existing processes.\nMLOps is more than just the technical side of ML lifecycle management; it\nalso incorporates best practices and methods used in software\ndevelopment and DevOps. Bridging the gap between data scientists, ML\nengineers, and DevOps, ML Ops enables a more coordinated approach to\nML projects. T eams can more easily track, reproduce, and iterate on\nmodels, ensuring stability and performance in production environments.\nBy adopting an ML Ops approach, organisations not only position\nthemselves for better scalability and faster deployment of ML models, but\nalso optimise resources and reduce risk. As a result, businesses can\nleverage data more effectively, enhancing their decision-making processes\nand achieving better outcomes in the competitive marketplace.\nFundamentals o f ML Ops\nMachine Learning Operations\nMLOps stands for Machine L earning Oper ations . It is an IT practice that",
            "from their investments in ML. Capgemini Research noted that the top three challenges faced by organizations in \nachieving deployments at scale are lack of mid- to senior-level talent, lack of change-management processes, and \nlack of strong governance models for achieving scale.\nThe common theme in these and other studies is that ML systems cannot be built in an ad hoc manner, isolated from \nother IT initiatives like DataOps and DevOps. They also cannot be built without adopting and applying sound software \nengineering practices, while taking into account the factors that make operationalizing ML different from operational -\nizing other types of software.\nOrganizations need an automated and streamlined ML process. This process does not just help the organization \nsuccessfully deploy ML models in production. It also helps manage risk when organizations scale the number of",
            "ly, they can be created by combining vendor tools that each are best suited to particular tasks, developed as custom \nservices, or created as a combination of these approaches.\nIn most cases, the processes are deployed in stages rather than all at once in a single deployment. An organization‚Äôs \nplan for adopting these processes and capabilities should align with business priorities and with the organization‚Äôs \ntechnical and skills maturity. For example, many organizations start by focusing on the processes for ML develop -\nment, model deployment, and prediction serving. For these organizations, continuous training and continuous moni -\ntoring might not be necessary if they are piloting a relatively small number of ML systems.\nFigure 4 shows the core set of technical capabilities that are generally required for MLOps. They are abstracted as \nfunctional components that can have many-to-many mappings to specific products and technologies.\n10",
            "tween tasks, the key artifacts created by the tasks, and the relationship of tasks to other upstream and downstream \nprocesses. In this section, you learn about concrete details of tasks like running a continuous training pipeline, de -\nploying a model, and monitoring predictive performance of the model.\n16\nMLOps processes take place on an integrated ML platform that has the required development and operations capa -\nbilities (described later). Infrastructure engineers can provision this type of platform in different environments (like \ndevelopment, test, staging, and production) using configuration management and infrastructure-as-code (IaC) tools \nlike Terraform . Each environment is configured with its own set of required compute resources, data access, and \nsubset of MLOps capability services.\nML development\nExperimentation is the core activity in ML development, where your data scientists can rapidly try several ideas for"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.9166666666666666
            },
            {
                "llama3.2:latest": 0.8875
            },
            {
                "gpt-oss:20b-cloud": null,
                "logs": "1 validation error for Verdicts\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"verdicts\"...no\"\\n    }\\n  ]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid"
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.1111111111111111
            },
            {
                "llama3.2:latest": 0.6470588235294118
            },
            {
                "gpt-oss:20b-cloud": 0.03125
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.7142857142857143
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 0.9
            }
        ]
    },
    {
        "input": "O que √© pipeline de retreinamento de modelo?",
        "expected_output": "Pipeline autom√°tico que treina novamente o modelo com novos dados ou drift.",
        "actual_output": "Um **pipeline de retreinamento de modelo** √© um conjunto estruturado de etapas que permite atualizar e recriar um modelo de machine learning (ML) existente com novos dados ou ajustes nos par√¢metros. Esse processo √© essencial para manter a precis√£o e efic√°cia do modelo √† medida que as condi√ß√µes ou os dados mudam ao longo do tempo.\n\nNo contexto apresentado, o retreinamento faz parte do ciclo de vida do ML, que inclui:\n\n1. **Desenvolvimento e experimenta√ß√£o**: Onde os modelos s√£o inicialmente criados e testados.\n2. **Integra√ß√£o cont√≠nua**: Onde o c√≥digo e os componentes do modelo s√£o validados e preparados para implanta√ß√£o.\n3. **Entrega cont√≠nua**: Onde o modelo √© implantado e disponibilizado para uso em produ√ß√£o.\n4. **Gest√£o do modelo**: Onde o modelo √© monitorado, atualizado e registrado para garantir sua qualidade e disponibilidade.\n\nO pipeline de retreinamento √© especialmente √∫til para garantir que os modelos continuem relevantes e eficientes, mesmo com o passar do tempo e a entrada de novos dados.",
        "retrieval_context": [
            "1. Development and experimentation: You iteratively try out new ML algorithms and new\nmodeling where the experiment steps are orchestrated. The output of this stage is the\nsource code of the ML pipeline steps that are then pushed to a source repository.\n2. Pipeline continuous integration: You build source code and run various tests. The\noutputs of this stage are pipeline components (packages, executables, and artifacts)\nto be deployed in a later stage.\n3. Pipeline continuous delivery: You deploy the artifacts produced by the CI stage to the\ntarget environment. The output of this stage is a deployed pipeline with the new\nimplementation of the model.\n4. Automated triggering: The pipeline is automatically executed in production based on a\nschedule or in response to a trigger. The output of this stage is a trained model that is\npushed to the model registry.\n5. Model continuous delivery: You serve the trained model as a prediction service for the",
            "P10 mentioned that there were parts of a pipeline that no one\ntouched because it was already running in production, and the\nprincipal developer who knew most about it had left the company.\nP16 said that ‚Äúmost of the, like, actual models were trained before\n time.‚Äù P14 described a ‚Äúpipeline jungle‚Äù that was difficult to\nmaintain:\nYou end up with this pipeline jungle where everything‚Äôs\nsuper entangled, and it‚Äôs really hard to make changes,\nbecause just to make one single change, you have to\nhold so much context in your brain. You‚Äôre trying to\nthink about like, okay this one change is gonna affect\nthis system which affects this system, [which\ncreates]...the pipeline got to the point where it was very\ndifficult to make even simple changes.\nWhile writing down institutional knowledge can be straightfor-\nward to do once, P6 discussed that in the ML setting, they learn\nfaster than they can document; moreover, people don‚Äôt want to\nread so many different versions of documentation:",
            "aging these pipelines, detecting errors and recovering fro m failures are all difÔ¨Åcult and costly .\nTesting such pipelines often requires expensive end-to-en d integration tests. All of this adds to\ntechnical debt of a system and makes further innovation more costly.\nPipeline jungles can only be avoided by thinking holistical ly about data collection and feature ex-\ntraction. The clean-slate approach of scrapping a pipeline jungle and redesigning from the ground\nup is indeed a major investment of engineering effort, but on e that can dramatically reduce ongoing\ncosts and speed further innovation.\nGlue code and pipeline jungles are symptomatic of integrati on issues that may have a root cause in\noverly separated ‚Äúresearch‚Äù and ‚Äúengineering‚Äù roles. When M L packages are developed in an ivory-\ntower setting, the result may appear like black boxes to the t eams that employ them in practice. A\nhybrid research approach where engineers and researchers a re embedded together on the same teams",
            "‚Ä¢ Trigger pipelines on demand, on a schedule, or in response to specified events.\n‚Ä¢ Enable local interactive execution for debugging during ML development.\n‚Ä¢ Integrate with the ML metadata tracking capability to capture pipeline execution parameters and to produce \nartifacts.\n‚Ä¢ Provide a set of built-in components for common ML tasks and also allow custom components.\n‚Ä¢ Run on different environments, including local machines and scalable cloud platforms.\n‚Ä¢ Optionally, provide GUI-based tools for designing and building pipelines.\nModel registry\nThe model registry capability lets you govern the lifecycle of the ML models in a central repository. This ensures the \nquality of the production models and enables model discovery. Key functionalities in the model registry include the \nfollowing:\n‚Ä¢ Register, organize, track, and version your trained and deployed ML models.\n‚Ä¢ Store model metadata and runtime dependencies for deployability.",
            "this phase, data engineers work together with data scientists to prepare\nand preprocess the data, performing featur e engineering to ensure the\ndata has the right format and structure.\nDuring model creation, various data pipelines are developed, enabling the\nsmooth flow of information between the different stages of the machine\nlearning process. T ools such as data engineering platforms can be used to\ndesign, test and maintain these pipelines.\nModel T raining\nOnce the model has been created, it is trained using a suitable dataset.\nModel training is an iterative process that involves feeding data into the\nmodel for it to learn and make predictions. The model is continually\nadjusted, and its performance is evaluated against a validation dataset to\nfine-tune its accuracy and effectiveness.\nSeveral techniques can be applied during the model training phase,\nincluding hyperparameter optimisation, cross-validation, and\nregularisation. Utilising the right combination of these methods helps"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.9166666666666666
            },
            {
                "gpt-oss:20b-cloud": 0.75
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.2
            },
            {
                "llama3.2:latest": 0.72
            },
            {
                "gpt-oss:20b-cloud": 0.5294117647058824
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.5555555555555556
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que caracteriza um bom ambiente de MLOps?",
        "expected_output": "Um ambiente com versionamento consistente, pipelines automatizados, monitoramento cont√≠nuo, logging estruturado, valida√ß√£o de dados e processos reprodut√≠veis.",
        "actual_output": "Um bom ambiente de MLOps √© caracterizado por:\n\n1.  **Infraestrutura Adequada:** Possuir uma plataforma integrada que ofere√ßa as capacidades necess√°rias de desenvolvimento e opera√ß√£o para suportar todo o ciclo de vida do ML. Isso inclui recursos de computa√ß√£o apropriados, acesso aos dados e servi√ßos de capacidades MLOps espec√≠ficas, configur√°veis para diferentes ambientes (desenvolvimento, teste, produ√ß√£o).\n\n2.  **Funcionalidades Essenciais:**\n    *   **Gest√£o de Experimentos:** Registrar e rastrear facilmente os experimentos de ci√™ncia de dados, modelos e resultados.\n    *   **Integra√ß√£o Cont√≠nua/Entrega Cont√≠nua (CI/CD):** Permitir a automa√ß√£o do pipeline de treinamento, valida√ß√£o e implanta√ß√£o do modelo.\n    *   **Implanta√ß√£o de Modelos:** Facilitar a coloca√ß√£o do modelo em produ√ß√£o de forma controlada e confi√°vel.\n    *   **Monitoramento:** Oferecer ferramentas para acompanhar continuamente o desempenho do modelo em produ√ß√£o e detectar degrada√ß√µes.\n    *   **Reprodutibilidade:** Garantir que experimentos e implanta√ß√µes possam ser reproduzidos consistentemente.\n    *   **Gest√£o de Vers√µes:** Controlar vers√µes de dados, modelos e c√≥digo.\n    *   **Automa√ß√£o:** Automatizar processos cr√≠ticos para aumentar a efici√™ncia e reduzir erros.\n\n3.  **Benef√≠cios:**\n    *   **Produtividade:** Libera cientistas de dados para focar no desenvolvimento de modelos, enquanto equipes de opera√ß√µes lidam com aspectos operacionais.\n    *   **Confian√ßa:** Garante que modelos s√£o implantados e monitorados de forma confi√°vel e segura.\n    *   **Agilidade:** Acelera o ciclo de desenvolvimento, teste e implanta√ß√£o de modelos.\n    *   **Reprodutibilidade e Confiabilidade:** Assegura que processos sejam consistentes e menos propensos a falhas.\n\nEm resumo, um bom ambiente MLOps une desenvolvimento e opera√ß√£o de forma integrada e automatizada, fornecendo as ferramentas e processos necess√°rios para construir, implantar e manter modelos de machine learning de forma eficiente, confi√°vel e em larga escala.",
        "retrieval_context": [
            "tween tasks, the key artifacts created by the tasks, and the relationship of tasks to other upstream and downstream \nprocesses. In this section, you learn about concrete details of tasks like running a continuous training pipeline, de -\nploying a model, and monitoring predictive performance of the model.\n16\nMLOps processes take place on an integrated ML platform that has the required development and operations capa -\nbilities (described later). Infrastructure engineers can provision this type of platform in different environments (like \ndevelopment, test, staging, and production) using configuration management and infrastructure-as-code (IaC) tools \nlike Terraform . Each environment is configured with its own set of required compute resources, data access, and \nsubset of MLOps capability services.\nML development\nExperimentation is the core activity in ML development, where your data scientists can rapidly try several ideas for",
            "6. Model deployment: Implementing the ML model into a product,\nservice, or system.\n7. Monit oring and maint enance: Continuously monitoring the\nperformance of the ML model and updating it as needed.\nMLOps helps manage the lifecycle of data science projects and ensures\nthat best practices are followed at each stage. This allows data scientists to\nfocus on their core tasks while IT professionals handle operational aspects,\ncreating a more effective and efficient workflow.\nFor a more in-depth comparison of Data Science and ML Ops check out\nour other blog post.\nMLOps and DevOps\nThe DevOps P aradigm\nDevOps is a combination of development (Dev) and operations (Ops)\npractices, aimed at unifying software development and IT operations. The\nprimary goal of DevOps is to reduce the time taken from code changes to\noperational deployment. This is achieved by embracing automation for11/11/25, 10:05 PM MLOps Now - What is MLOps? Demystifying Machine Learning Operations",
            "https://mlopsnow.com/blog/what-is-mlops/ 2/11\nTo understand ML Ops, it‚Äôs essential to be familiar with the development\nlifecycle of data science projects. A typical data science project consists of\nseveral stages:\n1. Data acquisition: Obtaining raw data from various sources, such as\ndatabases, sensors, or external APIs.\n2. Data pr eprocessing: Cleaning, transforming, and structuring the data\nto prepare it for analysis.\n3. Featur e engineering: Selecting the most relevant data attributes, or\n‚Äúfeatures,‚Äù and converting them into a suitable format for ML\nalgorithms.\n4. Model training: Applying ML algorithms to the preprocessed data to\ncreate a predictive model.\n5. Model ev aluation: Assessing the performance of the model and\nmaking adjustments to improve its accuracy.\n6. Model deployment: Implementing the ML model into a product,\nservice, or system.\n7. Monit oring and maint enance: Continuously monitoring the\nperformance of the ML model and updating it as needed.",
            "‚Ä¢ Handling concerns about model fairness and adversarial attacks.\nMLOps is a methodology for ML engineering that unifies ML system development (the ML element) with ML system \noperations (the Ops element). It advocates formalizing and (when beneficial) automating critical steps of ML system \nconstruction. MLOps provides a set of standardized processes and technology capabilities for building, deploying, \nand operationalizing ML systems rapidly and reliably.\nMLOps supports ML development and deployment in the way that DevOps and DataOps support application engi -\nneering and data engineering (analytics). The difference is that when you deploy a web service, you care about resil -\nience, queries per second, load balancing, and so on. When you deploy an ML model, you also need to worry about \nchanges in the data, changes in the model, users trying to game the system, and so on. This is what MLOps is about.",
            "such as business intelligence systems, line of business applications, process control systems, and embedded sys -\ntems. Integrating an ML model into an application is a critical task that involves making sure first that the deployed \nmodel is used effectively by the applications, and then monitoring model performance. In addition to this, you should \nalso collect and monitor relevant business KPIs (for example, click-through rate, revenue uplift, and user experience). \nThis information helps you understand the impact of the ML model on the business and adapt accordingly.\nFigure 1. The relationship of data engineering, ML engineering, and app engineering\n7\nThe MLOps lifecycle\nThe MLOps lifecycle encompasses seven integrated and iterative processes, as shown in figure 2.\nThe processes can consist of the following:\n‚Ä¢ ML development concerns experimenting and developing a robust and reproducible model training proce -"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.8261904761904761
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.24390243902439024
            },
            {
                "llama3.2:latest": 0.6666666666666666
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.6666666666666666
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.16666666666666666
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que √© um modelo de machine learning?",
        "expected_output": "Um modelo de machine learning √© um algoritmo treinado com dados para fazer previs√µes ou classifica√ß√µes.",
        "actual_output": "Um modelo de machine learning √© um algoritmo ou conjunto de regras que o computador aprende a partir de dados, permitindo que ele fa√ßa previs√µes ou tome decis√µes sem ser programado explicitamente para cada caso. \n\nPor exemplo, um modelo de machine learning pode ser treinado para reconhecer imagens, analisar sentimentos em textos ou prever vendas futuras. O processo de cria√ß√£o desses modelos envolve v√°rias etapas, como a prepara√ß√£o dos dados, a escolha de algoritmos e o treinamento do modelo, conforme descrito no contexto. \n\nSe precisar de mais detalhes sobre como os modelos s√£o desenvolvidos ou treinados, posso ajudar! üòä",
        "retrieval_context": [
            "\"MLOps emphasizes the comprehensive management of the machine learning model lifecycle, covering everything from deploying models into production environments to monitoring their performance. When necessary, models are updated to ensure that they continue to function effectively. The goal is to streamline the deployment process, guarantee models operate at their peak efficiency and foster an environment of continuous improvement. By focusing on these areas, MLOps ensures that machine learning models meet the immediate needs of their applications and adapt over time to maintain relevance and effectiveness in changing conditions.\n\nWhile ML focuses on the technical creation of models, MLOps focuses on the practical implementation and ongoing management of those models in a real-world setting.",
            "Model development\n\nModel development is a core phase in the data science process, focusing on constructing and refining machine learning models. This phase starts with model training, where the prepared data is used to train machine learning models that use selected algorithms and frameworks. The objective is to teach the model to make accurate predictions or decisions based on the data it has been trained on.\n\nAn essential aspect of model development is maintaining and tracking experiments, which involves keeping detailed records of different model iterations, the hyperparameter configurations used and the outcomes of various experiments. Such meticulous documentation is critical for comparing different models and configurations, facilitating the identification of the most effective approaches. This process helps optimize model performance and ensures that the development process is transparent and reproducible.",
            "more expensive to analyze improvements to that model in the f uture. The cost increases when\ncorrection models are cascaded, with a model for problem A‚Ä≤‚Ä≤learned on top of m‚Ä≤\na, and so on,\nfor several slightly different test distributions. Once in place, a correction cascade can create an\nimprovement deadlock, as improving the accuracy of any indi vidual component actually leads to\nsystem-level detriments. Mitigation strategies are to aug mentmato learn the corrections directly\nwithin the same model by adding features to distinguish amon g the cases, or to accept the cost of\ncreating a separate model for A‚Ä≤.\nUndeclared Consumers. Oftentimes, a prediction from a machine learning model mais made\nwidely accessible, either at runtime or by writing to Ô¨Åles or logs that may later be consumed by\nother systems. Without access controls, some of these consu mers may be undeclared , silently using\nthe output of a given model as an input to another system. In mo re classical software engineering,",
            "this phase, data engineers work together with data scientists to prepare\nand preprocess the data, performing featur e engineering to ensure the\ndata has the right format and structure.\nDuring model creation, various data pipelines are developed, enabling the\nsmooth flow of information between the different stages of the machine\nlearning process. T ools such as data engineering platforms can be used to\ndesign, test and maintain these pipelines.\nModel T raining\nOnce the model has been created, it is trained using a suitable dataset.\nModel training is an iterative process that involves feeding data into the\nmodel for it to learn and make predictions. The model is continually\nadjusted, and its performance is evaluated against a validation dataset to\nfine-tune its accuracy and effectiveness.\nSeveral techniques can be applied during the model training phase,\nincluding hyperparameter optimisation, cross-validation, and\nregularisation. Utilising the right combination of these methods helps",
            "https://mlopsnow.com/blog/what-is-mlops/ 2/11\nTo understand ML Ops, it‚Äôs essential to be familiar with the development\nlifecycle of data science projects. A typical data science project consists of\nseveral stages:\n1. Data acquisition: Obtaining raw data from various sources, such as\ndatabases, sensors, or external APIs.\n2. Data pr eprocessing: Cleaning, transforming, and structuring the data\nto prepare it for analysis.\n3. Featur e engineering: Selecting the most relevant data attributes, or\n‚Äúfeatures,‚Äù and converting them into a suitable format for ML\nalgorithms.\n4. Model training: Applying ML algorithms to the preprocessed data to\ncreate a predictive model.\n5. Model ev aluation: Assessing the performance of the model and\nmaking adjustments to improve its accuracy.\n6. Model deployment: Implementing the ML model into a product,\nservice, or system.\n7. Monit oring and maint enance: Continuously monitoring the\nperformance of the ML model and updating it as needed."
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5888888888888889
            },
            {
                "llama3.2:latest": 0.9166666666666666
            },
            {
                "gpt-oss:20b-cloud": 0.5333333333333333
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.25
            },
            {
                "llama3.2:latest": 0.7272727272727273
            },
            {
                "gpt-oss:20b-cloud": 0.28
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.7
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 0.9
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.75
            },
            {
                "llama3.2:latest": 0.25
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que √© treinamento de modelo?",
        "expected_output": "Treinamento √© o processo de ajustar os par√¢metros do modelo usando dados rotulados ou n√£o rotulados.",
        "actual_output": "O **treinamento de modelo** √© o processo de ensinar um modelo de machine learning a fazer previs√µes ou decis√µes com base em dados que ele recebe. Durante esse processo, o modelo aprende padr√µes e rela√ß√µes nos dados para, depois de treinado, ser capaz de lidar com novas informa√ß√µes de forma eficiente e precisa.",
        "retrieval_context": [
            "more expensive to analyze improvements to that model in the f uture. The cost increases when\ncorrection models are cascaded, with a model for problem A‚Ä≤‚Ä≤learned on top of m‚Ä≤\na, and so on,\nfor several slightly different test distributions. Once in place, a correction cascade can create an\nimprovement deadlock, as improving the accuracy of any indi vidual component actually leads to\nsystem-level detriments. Mitigation strategies are to aug mentmato learn the corrections directly\nwithin the same model by adding features to distinguish amon g the cases, or to accept the cost of\ncreating a separate model for A‚Ä≤.\nUndeclared Consumers. Oftentimes, a prediction from a machine learning model mais made\nwidely accessible, either at runtime or by writing to Ô¨Åles or logs that may later be consumed by\nother systems. Without access controls, some of these consu mers may be undeclared , silently using\nthe output of a given model as an input to another system. In mo re classical software engineering,",
            "Continuous Integration.¬†\nMonit oring identifies model drif t over time. Without model monitoring,\nproduction systems are flying blind. By monitoring for model drift the data\nscience team is able to proactively work rather than reactively.¬†\nTesting ensur es the accuracy and r eliability o f models. Validating both\nthe model‚Äôs predictions and the data sets used is a fundamental step in\ngreenlighting models for production.¬†\nUse A/B t esting t o identif y best models. A/B testing is sometimes\noverlooked in Machine Learning but is a great way to introduce new\nmodels. Rather than swapping models out straight away you can introduce\nthe new model alongside the old. This weighted approach allows you to\nsee the efficacy of the new model in production before committing to it.\n4. Version Contr ol\nVersion control is a significant aspect of ML Ops. It allows teams to track",
            "Continuous monitoring of model performance for accuracy drift, bias and other potential issues plays a critical role in maintaining the effectiveness of models and preventing unexpected outcomes. Monitoring the performance and health of ML models ensures that they continue to meet the intended objectives after deployment. By proactively identifying and addressing these concerns, organizations can maintain optimal model performance, mitigate risks and adapt to changing conditions or feedback.",
            "Model development\n\nModel development is a core phase in the data science process, focusing on constructing and refining machine learning models. This phase starts with model training, where the prepared data is used to train machine learning models that use selected algorithms and frameworks. The objective is to teach the model to make accurate predictions or decisions based on the data it has been trained on.\n\nAn essential aspect of model development is maintaining and tracking experiments, which involves keeping detailed records of different model iterations, the hyperparameter configurations used and the outcomes of various experiments. Such meticulous documentation is critical for comparing different models and configurations, facilitating the identification of the most effective approaches. This process helps optimize model performance and ensures that the development process is transparent and reproducible.",
            "Following the training phase, model evaluation is conducted to assess the performance of the models on unseen data. Evaluation is critical to ensure that the models perform well in real-world scenarios. Metrics such as accuracy, precision, recall and fairness measures gauge how well the model meets the project objectives. These metrics provide a quantitative basis for comparing different models and selecting the best one for deployment. Through careful evaluation, data scientists can identify and address potential issues, such as bias or overfitting, ensuring that the final model is effective and fair.\n\nModel deployment"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.3873015873015873
            },
            {
                "llama3.2:latest": 0.9166666666666666
            },
            {
                "gpt-oss:20b-cloud": 0
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.25
            },
            {
                "llama3.2:latest": 0.8787878787878788
            },
            {
                "gpt-oss:20b-cloud": 0.06060606060606061
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.75
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.6666666666666666
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "Por que separar treino e teste?",
        "expected_output": "A separa√ß√£o evita overfitting e garante avalia√ß√£o imparcial da performance.",
        "actual_output": "Ol√°! Separar os dados de treino e teste √© uma pr√°tica essencial em aprendizado de m√°quina por uma raz√£o muito importante: **evitar que o modelo \"aprenda\" apenas para os dados que j√° viu e fique super especializado neles, prejudicando sua capacidade de generalizar para novos dados**.\n\nImagine que voc√™ est√° treinando um modelo para classificar animais. Se voc√™ usar apenas gatos e cachorros para treinamento e teste, ele pode aprender todas as caracter√≠sticas espec√≠ficas desses animais. Mas quando voc√™ apresentar uma foto de um p√°ssaro, ele pode falhar, porque nunca teve contato com esse tipo de imagem durante o treino.\n\nAqui est√° o problema:\n\n1.  **Overfitting (Ajuste Excessivo):** Se o modelo for treinado e avaliado na mesma base de dados, ele pode decorar os padr√µes dessa base espec√≠fica, capturando ru√≠dos e peculiaridades irrelevantes. Isso faz com que ele se desempenhe muito bem na base de treino, mas mal na base de teste (que √© diferente, mas que representaria dados reais que o modelo vai encontrar no futuro).\n\n2.  **Perda de Generaliza√ß√£o:** O objetivo do aprendizado de m√°quina √© criar modelos que funcionem bem em situa√ß√µes novas e desconhecidas. A base de teste √© como o mundo real se apresenta para o modelo ap√≥s o treino. Separar os dados garante que voc√™ tenha uma avalia√ß√£o honesta da capacidade do modelo de lidar com novas informa√ß√µes.\n\n**Em resumo:** Separa-se treino e teste para garantir que o modelo possa **generalizar** bem, ou seja, performar bem em dados que ele nunca viu antes, refletindo seu desempenho no mundo real. √â uma forma de simular o que aconteceria com o modelo quando ele estiver realmente em produ√ß√£o, ajudando a prevenir o \"debt\" de monitoramento e teste mencionado no contexto, j√° que voc√™ precisa ter certeza que o modelo continua funcionando corretamente quando os dados mudam (model drift).",
        "retrieval_context": [
            ", in which thresholds are learned via simple evaluation on heldout validation data.\nMonitoring and Testing. Unit testing of individual components and end-to-end tests of running\nsystems are valuable, but in the face of a changing world such tests are not sufÔ¨Åcient to provide\nevidence that a system is working as intended. Comprehensiv e live monitoring of system behavior\nin real time combined with automated response is critical fo r long-term system reliability.\nThe key question is: what to monitor? Testable invariants ar e not always obvious given that many\nML systems are intended to adapt over time. We offer the follo wing starting points.\n‚Ä¢Prediction Bias. In a system that is working as intended, it should usually be t he case that\nthe distribution of predicted labels is equal to the distrib ution of observed labels. This is\nby no means a comprehensive test, as it can be met by a null mode l that simply predicts",
            "a given language, especially when that language has a conven ient library or syntax for the\ntask at hand. However, using multiple languages often incre ases the cost of effective testing\nand can increase the difÔ¨Åculty of transferring ownership to other individuals.\n‚Ä¢Prototype Smell. It is convenient to test new ideas in small scale via prototyp es. How-\never, regularly relying on a prototyping environment may be an indicator that the full-scale\nsystem is brittle, difÔ¨Åcult to change, or could beneÔ¨Åt from i mproved abstractions and inter-\nfaces. Maintaining a prototyping environment carries its o wn cost, and there is a signiÔ¨Åcant\ndanger that time pressures may encourage a prototyping syst em to be used as a production\nsolution. Additionally, results found at small scale rarel y reÔ¨Çect the reality at full scale.\n6 ConÔ¨Åguration Debt\nAnother potentially surprising area where debt can accumul ate is in the conÔ¨Åguration of machine",
            "Continuous Integration.¬†\nMonit oring identifies model drif t over time. Without model monitoring,\nproduction systems are flying blind. By monitoring for model drift the data\nscience team is able to proactively work rather than reactively.¬†\nTesting ensur es the accuracy and r eliability o f models. Validating both\nthe model‚Äôs predictions and the data sets used is a fundamental step in\ngreenlighting models for production.¬†\nUse A/B t esting t o identif y best models. A/B testing is sometimes\noverlooked in Machine Learning but is a great way to introduce new\nmodels. Rather than swapping models out straight away you can introduce\nthe new model alongside the old. This weighted approach allows you to\nsee the efficacy of the new model in production before committing to it.\n4. Version Contr ol\nVersion control is a significant aspect of ML Ops. It allows teams to track",
            "testing, integration testing, and continuous delivery of the software module or the package.\nHowever, in ML, there are a few notable differences:\nCI is no longer only about testing and validating code and components, but also\ntesting and validating data, data schemas, and models.\nCD is no longer about a single software package or a service, but a system (an ML\ntraining pipeline) that should automatically deploy another service (model prediction\nservice).\nCT is a new property, unique to ML systems, that's concerned with automatically\nretraining and serving the models.\nThe following section discusses the typical steps for training and evaluating an ML model\nto serve as a prediction service.\nData science steps for ML\nIn any ML project, after you define the business use case and establish the success criteria,\nthe process of delivering an ML model to production involves the following steps. These\nsteps can be completed manually or can be completed by an automatic pipeline.",
            "Continuous integration\nIn this setup, the pipeline and its components are built, tested, and packaged when new\ncode is committed or pushed to the source code repository. Besides building packages,\ncontainer images, and executables, the CI process can include the following tests:\nUnit testing your feature engineering logic.\nUnit testing the different methods implemented in your model. For example, you have\na function that accepts a categorical data column and you encode the function as a\none-hot¬†(https://en.wikipedia.org/wiki/One-hot) feature.\nTesting that your model training converges (that is, the loss of your model goes down\nby iterations and overfits¬†(https://en.wikipedia.org/wiki/Overfitting) a few sample records).\nTesting that your model training doesn't produce NaN¬†(https://en.wikipedia.org/wiki/NaN)\nvalues due to dividing by zero or manipulating small or large values.\nTesting that each component in the pipeline produces the expected artifacts."
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 0.7
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.391304347826087
            },
            {
                "llama3.2:latest": 0.6578947368421053
            },
            {
                "gpt-oss:20b-cloud": 0.05714285714285714
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.6428571428571429
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 0.95
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.6666666666666666
            },
            {
                "llama3.2:latest": 0.4166666666666667
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que √© infer√™ncia?",
        "expected_output": "Infer√™ncia √© o uso de um modelo treinado para gerar previs√µes em novos dados.",
        "actual_output": "Nenhuma informa√ß√£o dispon√≠vel no contexto.",
        "retrieval_context": [
            "Ô¨Åcations of this basic idea . The lack of standard abst ractions makes it all too easy to blur the\nlines between components.\nCommon Smells. In software engineering, a design smell may indicate an underlying problem in\na component or system . We identify a few ML system smells, not hard-and-fast rules, but as\nsubjective indicators.\n‚Ä¢Plain-Old-Data Type Smell. The rich information used and produced by ML systems is\nall to often encoded with plain data types like raw Ô¨Çoats and i ntegers. In a robust system,\na model parameter should know if it is a log-odds multiplier o r a decision threshold, and a\nprediction should know various pieces of information about the model that produced it and\nhow it should be consumed.\n‚Ä¢Multiple-Language Smell. It is often tempting to write a particular piece of a system in\na given language, especially when that language has a conven ient library or syntax for the\ntask at hand. However, using multiple languages often incre ases the cost of effective testing",
            "a given language, especially when that language has a conven ient library or syntax for the\ntask at hand. However, using multiple languages often incre ases the cost of effective testing\nand can increase the difÔ¨Åculty of transferring ownership to other individuals.\n‚Ä¢Prototype Smell. It is convenient to test new ideas in small scale via prototyp es. How-\never, regularly relying on a prototyping environment may be an indicator that the full-scale\nsystem is brittle, difÔ¨Åcult to change, or could beneÔ¨Åt from i mproved abstractions and inter-\nfaces. Maintaining a prototyping environment carries its o wn cost, and there is a signiÔ¨Åcant\ndanger that time pressures may encourage a prototyping syst em to be used as a production\nsolution. Additionally, results found at small scale rarel y reÔ¨Çect the reality at full scale.\n6 ConÔ¨Åguration Debt\nAnother potentially surprising area where debt can accumul ate is in the conÔ¨Åguration of machine",
            "privilege of working with real data in development. This privilege comes\nwith a lot of responsibility.\nUse encr yption t o protect data in transit and at r est. Encrypting data\nensures that the data sets used in the training pipeline are secure, even if\nleaked.¬†\nAccess contr ols to contr ol access t o data and models. An extra layer on\ntop of encryption is access controls to ensure only relevant users can view\ndata and models.¬†\nRegular audits t o confirm compliance with r egulations such as GDPR and\nHIPAA. When working with customer data it is imperative that it is done in\nan ethical manner. Liase with your Cyber Security and Ethics teams to\nensure you meet these requirements.¬†\nChallenges\nWhile it is crucial to follow best practices in ML Ops, it is also important to\nunderstand and prepare for the challenges\n1. Data Management\nOne of the most common challenges in ML Ops is data management.¬†\nMLOps Engineers aren‚Äôt directly responsible for ingestion and processing",
            "https://mlopsnow.com/blog/what-is-mlops/ 2/11\nTo understand ML Ops, it‚Äôs essential to be familiar with the development\nlifecycle of data science projects. A typical data science project consists of\nseveral stages:\n1. Data acquisition: Obtaining raw data from various sources, such as\ndatabases, sensors, or external APIs.\n2. Data pr eprocessing: Cleaning, transforming, and structuring the data\nto prepare it for analysis.\n3. Featur e engineering: Selecting the most relevant data attributes, or\n‚Äúfeatures,‚Äù and converting them into a suitable format for ML\nalgorithms.\n4. Model training: Applying ML algorithms to the preprocessed data to\ncreate a predictive model.\n5. Model ev aluation: Assessing the performance of the model and\nmaking adjustments to improve its accuracy.\n6. Model deployment: Implementing the ML model into a product,\nservice, or system.\n7. Monit oring and maint enance: Continuously monitoring the\nperformance of the ML model and updating it as needed.",
            "ful and reasonable for why things should be good, but\nthe most defining characteristic of [my most productive\ncolleague] is that he has the highest pace of experi-\nmentation out of anyone. He‚Äôs always running exper-\niments, always trying everything. I think this is rel-\natively common‚Äîpeople just try everything and then\nbackfit some nice-sounding explanation for why it works.\nWe wondered, why was it even necessary to have an expla-\nnation for why something worked? Why not simply accept that,\nunlike in software, we may not have elegant, principled reasons for\nsuccessful ML experiments? P2 hypothesized that such retrofitted\nexplanations could guide future experiment ideas over a longer hori-\nzon. Alternatively, P7 mentioned that their customers sometimes\ndemanded explanations for certain predictions:\nDo I know why? No idea. I have to convince people that,\nokay, we try our best. We try to correlations.\nWe try to similarities. Why is it different? I\nhave to make conjectures."
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.7555555555555555
            },
            {
                "gpt-oss:20b-cloud": 0
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.2682926829268293
            },
            {
                "llama3.2:latest": 0.5897435897435898
            },
            {
                "gpt-oss:20b-cloud": 0.0
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 1.0
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 0.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 1.0
            },
            {
                "llama3.2:latest": 0.0
            },
            {
                "gpt-oss:20b-cloud": 0.0
            }
        ]
    },
    {
        "input": "O que √© um dataset?",
        "expected_output": "Dataset √© uma cole√ß√£o estruturada de dados utilizada para treinamento, valida√ß√£o ou teste.",
        "actual_output": "Um **dataset** √© um conjunto estruturado de dados usado para treinar, validar e testar modelos de machine learning. No contexto apresentado, ele √© gerenciado como parte do *dataset and feature repository*, que permite unificar a defini√ß√£o e armazenamento de dados de ML. \n\nAlguns pontos importantes sobre datasets, conforme o contexto:\n- S√£o armazenados em reposit√≥rios centralizados, garantindo consist√™ncia e qualidade dos dados.\n- Podem incluir diferentes tipos de dados, como dados tabulares, imagens e texto.\n- Exemplos: um dataset pode conter dados de clientes (churn, comportamento de compra) ou logs de eventos web.\n\nEssa centraliza√ß√£o facilita a reutiliza√ß√£o, compartilhamento e governan√ßa dos dados no ciclo de vida do ML.",
        "retrieval_context": [
            "privilege of working with real data in development. This privilege comes\nwith a lot of responsibility.\nUse encr yption t o protect data in transit and at r est. Encrypting data\nensures that the data sets used in the training pipeline are secure, even if\nleaked.¬†\nAccess contr ols to contr ol access t o data and models. An extra layer on\ntop of encryption is access controls to ensure only relevant users can view\ndata and models.¬†\nRegular audits t o confirm compliance with r egulations such as GDPR and\nHIPAA. When working with customer data it is imperative that it is done in\nan ethical manner. Liase with your Cyber Security and Ethics teams to\nensure you meet these requirements.¬†\nChallenges\nWhile it is crucial to follow best practices in ML Ops, it is also important to\nunderstand and prepare for the challenges\n1. Data Management\nOne of the most common challenges in ML Ops is data management.¬†\nMLOps Engineers aren‚Äôt directly responsible for ingestion and processing",
            "this phase, data engineers work together with data scientists to prepare\nand preprocess the data, performing featur e engineering to ensure the\ndata has the right format and structure.\nDuring model creation, various data pipelines are developed, enabling the\nsmooth flow of information between the different stages of the machine\nlearning process. T ools such as data engineering platforms can be used to\ndesign, test and maintain these pipelines.\nModel T raining\nOnce the model has been created, it is trained using a suitable dataset.\nModel training is an iterative process that involves feeding data into the\nmodel for it to learn and make predictions. The model is continually\nadjusted, and its performance is evaluated against a validation dataset to\nfine-tune its accuracy and effectiveness.\nSeveral techniques can be applied during the model training phase,\nincluding hyperparameter optimisation, cross-validation, and\nregularisation. Utilising the right combination of these methods helps",
            "following:\n‚Ä¢ Register, organize, track, and version your trained and deployed ML models.\n‚Ä¢ Store model metadata and runtime dependencies for deployability.\n‚Ä¢ Maintain model documentation and reporting‚Äîfor example, using model cards .\n‚Ä¢ Integrate with the model evaluation and deployment capability and track online and offline evaluation metrics \nfor the models.\n‚Ä¢ Govern the model launching process: review, approve, release, and roll back. These decisions are based on a \nnumber of offline performance and fairness metrics and on online experimentation results.\nDataset and feature repository\nThe dataset and feature repository capability lets you unify the definition and the storage of the ML data assets. \nHaving a central repository of fresh, high-quality data assets enables shareability, discoverability, and reusability. The \nrepository also provides data consistency for training and inference. This helps data scientists and ML researchers",
            "repository also provides data consistency for training and inference. This helps data scientists and ML researchers \nsave time on data preparation and feature engineering, which typically take up a significant amount of their time. Key \nfunctionalities in the data and feature repository include the following:\n15\n‚Ä¢ Enable shareability, discoverability, reusability, and versioning of data assets.\n‚Ä¢ Allow real-time ingestion and low-latency serving for event streaming and online prediction workloads. \n‚Ä¢ Allow high-throughput batch ingestion and serving for extract, transform, load (ETL) processes and model \ntraining, and for scoring workloads.\n‚Ä¢ Enable feature versioning for point-in-time queries.\n‚Ä¢ Support various data modalities, including tabular data, images, and text.\nML data assets can be managed at the entity features level or at the full dataset level. For example, a feature reposi -",
            "‚Ä¢ Support various data modalities, including tabular data, images, and text.\nML data assets can be managed at the entity features level or at the full dataset level. For example, a feature reposi -\ntory might contain an entity called customer, which includes features like age group, postal code, and gender. On the \nother hand, a dataset repository might include a customer churn dataset, which includes features from the customer \nand product entities, as well as purchase- and web-activity event logs.\nML metadata and artifact tracking\nVarious types of ML artifacts are produced in different processes of the MLOps lifecycle, including descriptive \nstatistics and data schemas, trained models, and evaluation results. ML metadata is the information about these \nartifacts, including their location, types, properties, and associations to experiments and runs. The ML metadata and"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.7555555555555555
            },
            {
                "gpt-oss:20b-cloud": 0.5
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.275
            },
            {
                "llama3.2:latest": 0.6428571428571429
            },
            {
                "gpt-oss:20b-cloud": 0.17073170731707318
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.6666666666666666
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 0.4444444444444444
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.6666666666666666
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "Por que usar pipelines em ML?",
        "expected_output": "Pipelines organizam e automatizam as etapas do ciclo de vida de ML, garantindo reprodutibilidade e escalabilidade.",
        "actual_output": "Pipelines no ML s√£o essenciais porque ajudam a organizar e automatizar processos complexos, como a coleta de dados, pr√©-processamento, treinamento e implanta√ß√£o de modelos. Eles garantem que as etapas sejam repet√≠veis e menos suscet√≠veis a erros humanos. Al√©m disso, facilitam a documenta√ß√£o e integra√ß√£o cont√≠nua, evitando que pipelines se tornem \"jungles\" desordenadas e dif√≠ceis de manter.",
        "retrieval_context": [
            "ward to do once, P6 discussed that in the ML setting, they learn\nfaster than they can document; moreover, people don‚Äôt want to\nread so many different versions of documentation:\nThere are people in the team, myself included, that\nhave been on it for several years now, and so there‚Äôs\nsome institutional knowledge embodied on the team\nthat sometimes gets written down. But you know, even\nwhen it does get written down, maybe you will read\nthem, but then, they kind of disappear to the ether.\nFinally, P17 realized that poorly documented pipelines forced\nthem to treat pipelines as black boxes: ‚ÄúSome of our models are\npretty old and not well documented, so I don‚Äôt have great expec-\ntations for what they should be doing.‚Äù Without intuition for how\npipelines should perform, practitioner productivity can be stunted.\nTakeaway. The MLOps anti-patterns described in this section re-\nveal that ML engineering, as a field, is changing faster than educa-",
            "aging these pipelines, detecting errors and recovering fro m failures are all difÔ¨Åcult and costly .\nTesting such pipelines often requires expensive end-to-en d integration tests. All of this adds to\ntechnical debt of a system and makes further innovation more costly.\nPipeline jungles can only be avoided by thinking holistical ly about data collection and feature ex-\ntraction. The clean-slate approach of scrapping a pipeline jungle and redesigning from the ground\nup is indeed a major investment of engineering effort, but on e that can dramatically reduce ongoing\ncosts and speed further innovation.\nGlue code and pipeline jungles are symptomatic of integrati on issues that may have a root cause in\noverly separated ‚Äúresearch‚Äù and ‚Äúengineering‚Äù roles. When M L packages are developed in an ivory-\ntower setting, the result may appear like black boxes to the t eams that employ them in practice. A\nhybrid research approach where engineers and researchers a re embedded together on the same teams",
            "Takeaway. The MLOps anti-patterns described in this section re-\nveal that ML engineering, as a field, is changing faster than educa-\ntional resources can keep up. We see this as opportunities for new\nresources, such as classroom material (e.g., textbooks, courses) to\nprescribe the right engineering practices and rigor for the highly\nexperimental discipline that is production ML, and automated doc-\numentation assistance for ML pipelines in organizations.\n5.3 Characterizing the ‚ÄúMLOps Stack‚Äù for Tool\nBuilders\nMLOps tool builders may be interested in an organization of the\ndozens of tools, libraries, and services MLEs use to run ML and\ndata processing pipelines. Although multiple MLEs reported hav-\ning to ‚Äúglue‚Äù open-source solutions together and having to build\n‚Äúhomegrown‚Äù infrastructure as part of their work (P1, P2, P5, P6,\nP10, P12), an analysis of the various deployments reveals that tools\nShreya Shankar‚àó, Rolando Garcia‚àó, Joseph M. Hellerstein, Aditya G. Parameswaran\nRun Layer",
            "ing environments varies depending on standards that are established in a \ngiven organization. Most organizations have at least one testing environ -\nment before production; some have more.\nThe specifics of the pipeline deployment process depend on the technol -\nogy that is used to implement the pipeline. With some no-code solutions, \ndata scientists and ML engineers don‚Äôt handle or even see the details.\nAlternatively, if you use a code-first technology to have more flexibility and \ncontrol over the ML pipelines, ML engineers can deploy the pipeline using \nstandard CI/CD processes and tools. This approach is what the diagram \ndepicts. The diagram shows a standard CI/CD workflow, which consists of \nthese stages:\n1. In the CI stage, the source code is unit-tested, and the training pipe -\nline is built and integration-tested. Any artifacts that are \ncreated by the build are stored in an artifact repository.\nFigure 6. The training operationalization process\nTraining \nOperationalization",
            "tion 4.4, we discuss organizational efforts to effectively evaluate\nmodels. Finally, in Section 4.5, we investigate the hacks ML engi-\nneers use to sustain high performance in productions ML pipelines.\n4.1 Tasks in the Production ML Lifecycle\nWe characterized ML engineers‚Äô workflows into four high-level\ntasks, each of which employ a wide variety of tools. We briefly\ndescribe each task in turn, and elaborate on them as they arise in\nour findings below.\nData Collection and Labeling. Data collection spans sourcing\nnew data, wrangling data from sources into a centralized reposi-\ntory, and cleaning data. Data labeling can be outsourced (e.g., Me-\nchanical Turk) or performed in-house with teams of annotators.\nSince descriptions and interview studies of data collection, anal-\nysis, wrangling and labeling activities can be found in related pa-\npers , we focus our summary of findings on the other\nthree tasks.\nFeature Engineering and Model Experimentation. ML engi-"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 0
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.2777777777777778
            },
            {
                "llama3.2:latest": 0.78125
            },
            {
                "gpt-oss:20b-cloud": 0.4857142857142857
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.9
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.5714285714285714
            },
            {
                "llama3.2:latest": 0.8333333333333334
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que s√£o m√©tricas de avalia√ß√£o?",
        "expected_output": "S√£o medidas num√©ricas que indicam a qualidade do modelo, como accuracy, F1 e RMSE.",
        "actual_output": "M√©tricas de avalia√ß√£o s√£o indicadores quantitativos usados para medir e verificar o desempenho de um modelo de Machine Learning em tarefas espec√≠ficas. Elas ajudam a verificar se o modelo est√° atingindo os objetivos do projeto e funcionando bem em situa√ß√µes reais.\n\nNo contexto fornecido, essas m√©tricas podem ser divididas em duas categorias principais:\n\n1.  **M√©tricas T√©cnicas:** S√£o usadas para avaliar o desempenho do modelo em tarefas de aprendizado de m√°quina, como classifica√ß√£o ou regress√£o. Exemplos mencionados s√£o:\n    *   **Acur√°cia (Accuracy):** Quanto o modelo est√° correto em suas predi√ß√µes.\n    *   **Precis√£o (Precision):** Quanto as predi√ß√µes positivas do modelo est√£o corretas.\n    *   **Recall (Sensibilidade):** Quanto o modelo consegue identificar os casos positivos reais.\n    *   **F1 Score:** Uma m√©dia harmonica da precis√£o e recall, √∫til quando essas duas m√©tricas s√£o importantes.\n    *   **Medidas de Fairness (Justi√ßa):** Indicadores que verificam se o modelo n√£o apresenta discrimina√ß√£o em rela√ß√£o a grupos espec√≠ficos.\n\n2.  **M√©tricas de Neg√≥cio:** S√£o cruciais para mostrar o valor do modelo para a organiza√ß√£o. Elas devem ser alinhadas com os KPIs (indicadores-chave de desempenho) da empresa e refletir o impacto real do modelo. O contexto enfatiza que √© importante:\n    *   Evitar focar apenas em m√©tricas t√©cnicas (como MAP, P5, P7, etc.).\n    *   Escolher m√©tricas que demonstram o valor organizacional. Por exemplo, uma m√©trica de neg√≥cio poderia ser a porcentagem de clientes satisfeitos ou o aumento nas vendas atribu√≠do ao modelo.\n    *   A escolha dessas m√©tricas deve ser feita explicitamente em conjunto com outros stakeholders (gerentes de produto, operadores de neg√≥cio) para garantir que as m√©tricas certas sejam utilizadas e que demonstrem o sucesso do projeto aos gestores.\n\nPortanto, as m√©tricas de avalia√ß√£o s√£o ferramentas essenciais para garantir que o modelo n√£o s√≥ funcione tecnicamente bem, mas tamb√©m gere resultados tang√≠veis e valor para a organiza√ß√£o.",
        "retrieval_context": [
            "ration systems:\n‚Ä¢It should be easy to specify a conÔ¨Åguration as a small change f rom a previous conÔ¨Åguration.\n‚Ä¢It should be hard to make manual errors, omissions, or oversi ghts.\n‚Ä¢It should be easy to see, visually, the difference in conÔ¨Ågur ation between two models.\n‚Ä¢It should be easy to automatically assert and verify basic fa cts about the conÔ¨Åguration:\nnumber of features used, transitive closure of data depende ncies, etc.\n‚Ä¢It should be possible to detect unused or redundant settings .\n‚Ä¢ConÔ¨Ågurations should undergo a full code review and be check ed into a repository.\n6\n7 Dealing with Changes in the External World\nOne of the things that makes ML systems so fascinating is that they often interact directly with the\nexternal world. Experience has shown that the external worl d is rarely stable. This background rate\nof change creates ongoing maintenance cost.\nFixed Thresholds in Dynamic Systems. It is often necessary to pick a decision threshold for a",
            "cloud platforms or on-premises infrastructure.\nIt is crucial to consider aspects such as scalability, security, and\nperformance during the deployment phase. Ensuring that the model can\nhandle multiple concurrent requests, protect sensitive data, and provide\nlow-latency responses is essential.\nMonit oring\nOnce the model is deployed, it is essential to monitor its performance\ncontinuously. Monit oring plays a vital role in identifying any degradation\nin model performance and detecting errors or anomalies in the\npredictions.\nSeveral metrics can be used to evaluate model performance, such as\naccuracy, precision, recall, and F1 score. Additionally, it is crucial to\nmonitor infrastructure-related metrics ‚Äì like latency, throughput, and\nresource consumption ‚Äì to guarantee the system‚Äôs stability and efficiency.\nBy actively monitoring the model and its surrounding infrastructure, it is\npossible to identify any issues early and swiftly address them. This process",
            "rather than ML-specific metrics alone like MAP (P5, P7, P15, P16,\nP11, P17, P18, P19). The need to evaluate product-critical metrics\nstemmed from close collaboration with other stakeholders, such\nas product managers and business operators. P11 felt that a key\nreason many ML projects fail is that they don‚Äôt measure metrics\nthat will yield the organization value:\nTying to the business‚Äôs KPIs (key\nperformance indicators) is really important. But it‚Äôs a\nprocess‚Äîyou need to figure out what are, and\nfrankly I think that‚Äôs how people should be doing AI. It\n like: hey, let‚Äôs do these experiments and\nget cool numbers and show off these nice precision-recall\ncurves to our bosses and call it a day. It should be like:\nhey, let‚Äôs actually show the same business metrics that\neveryone else is held accountable to to our bosses at the\nend of the day.\nSince product-specific metrics are, by definition, different for\ndifferent ML models, it was important for engineers to treat choos-",
            "end of the day.\nSince product-specific metrics are, by definition, different for\ndifferent ML models, it was important for engineers to treat choos-\ning the metrics as an explicit step in their workflow and align with\nother stakeholders to make sure the right metrics were chosen. For\nexample, P16 said that for every new ML project they work on, their\n‚Äúfirst task is to figure out, what are customers actually interested\nin, or what‚Äôs the metric that they care about.‚Äù P17 said that every\nmodel change in production is validated by the product team: ‚Äúif\nwe can get a statistically significant greater percentage people\nto subscribe to , then .‚Äù\nFor some organizations, a consequence of tightly coupling eval-\nuation to product metrics was an additional emphasis on important\ncustomers during evaluation (P6, P10). P6 described how, at their\ncompany, experimental changes that increased aggregate metrics\ncould sometimes be prevented from going to production:",
            "Following the training phase, model evaluation is conducted to assess the performance of the models on unseen data. Evaluation is critical to ensure that the models perform well in real-world scenarios. Metrics such as accuracy, precision, recall and fairness measures gauge how well the model meets the project objectives. These metrics provide a quantitative basis for comparing different models and selecting the best one for deployment. Through careful evaluation, data scientists can identify and address potential issues, such as bias or overfitting, ensuring that the final model is effective and fair.\n\nModel deployment"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.7555555555555555
            },
            {
                "llama3.2:latest": 0.9166666666666666
            },
            {
                "gpt-oss:20b-cloud": 0.45
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.14285714285714285
            },
            {
                "llama3.2:latest": 0.6666666666666666
            },
            {
                "gpt-oss:20b-cloud": 0.4166666666666667
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.6923076923076923
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.7692307692307693
            },
            {
                "llama3.2:latest": 0.55
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que √© um modelo baseline?",
        "expected_output": "√â um modelo simples usado como refer√™ncia para comparar modelos mais complexos.",
        "actual_output": "Nenhuma informa√ß√£o dispon√≠vel no contexto.",
        "retrieval_context": [
            "this phase, data engineers work together with data scientists to prepare\nand preprocess the data, performing featur e engineering to ensure the\ndata has the right format and structure.\nDuring model creation, various data pipelines are developed, enabling the\nsmooth flow of information between the different stages of the machine\nlearning process. T ools such as data engineering platforms can be used to\ndesign, test and maintain these pipelines.\nModel T raining\nOnce the model has been created, it is trained using a suitable dataset.\nModel training is an iterative process that involves feeding data into the\nmodel for it to learn and make predictions. The model is continually\nadjusted, and its performance is evaluated against a validation dataset to\nfine-tune its accuracy and effectiveness.\nSeveral techniques can be applied during the model training phase,\nincluding hyperparameter optimisation, cross-validation, and\nregularisation. Utilising the right combination of these methods helps",
            "‚Ä¢ Hyperparameters, including trials of automated hyperparameter tuning and model selection.\n‚Ä¢ Information about training, validation, and testing data splits that were used. \n‚Ä¢ Model evaluation metrics and the validation procedure that was used.\nIf there is no need to retrain the model on a regular basis, then the produced model at the end of the experimenta -\ntion is submitted to the model registry. The model is then ready to be reviewed, approved, and deployed to the target \n18\nserving environment. In addition, all the relevant metadata and artifacts \nthat were produced during model development are tracked in the metadata \ntracking repository.\nHowever, in most cases, ML models need to be retrained on a regular basis \nwhen new data is available or when the code changes. In this case, the \noutput of the ML development process is not the model to be deployed in \nproduction. Instead, the output is the implementation of the continuous",
            "Continuous monitoring of model performance for accuracy drift, bias and other potential issues plays a critical role in maintaining the effectiveness of models and preventing unexpected outcomes. Monitoring the performance and health of ML models ensures that they continue to meet the intended objectives after deployment. By proactively identifying and addressing these concerns, organizations can maintain optimal model performance, mitigate risks and adapt to changing conditions or feedback.",
            "Model development\n\nModel development is a core phase in the data science process, focusing on constructing and refining machine learning models. This phase starts with model training, where the prepared data is used to train machine learning models that use selected algorithms and frameworks. The objective is to teach the model to make accurate predictions or decisions based on the data it has been trained on.\n\nAn essential aspect of model development is maintaining and tracking experiments, which involves keeping detailed records of different model iterations, the hyperparameter configurations used and the outcomes of various experiments. Such meticulous documentation is critical for comparing different models and configurations, facilitating the identification of the most effective approaches. This process helps optimize model performance and ensures that the development process is transparent and reproducible.",
            "Following the training phase, model evaluation is conducted to assess the performance of the models on unseen data. Evaluation is critical to ensure that the models perform well in real-world scenarios. Metrics such as accuracy, precision, recall and fairness measures gauge how well the model meets the project objectives. These metrics provide a quantitative basis for comparing different models and selecting the best one for deployment. Through careful evaluation, data scientists can identify and address potential issues, such as bias or overfitting, ensuring that the final model is effective and fair.\n\nModel deployment"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.3666666666666667
            },
            {
                "llama3.2:latest": 0.9166666666666666
            },
            {
                "gpt-oss:20b-cloud": 0
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.21428571428571427
            },
            {
                "llama3.2:latest": 0.5675675675675675
            },
            {
                "gpt-oss:20b-cloud": null,
                "logs": "1 validation error for ContextualRelevancyVerdicts\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"verdicts\"...l.\"\\n    }\\n  ]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid"
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.0
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 0.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 1.0
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 0.0
            }
        ]
    },
    {
        "input": "O que significa monitorar lat√™ncia de infer√™ncia?",
        "expected_output": "Monitorar lat√™ncia garante que o modelo responda dentro dos limites necess√°rios para uso real.",
        "actual_output": "Monitorar lat√™ncia de infer√™ncia significa verificar o tempo que leva para um modelo de machine learning processar uma solicita√ß√£o e produzir uma resposta. √â uma m√©trica importante para garantir o desempenho e a efici√™ncia do sistema, especialmente em aplica√ß√µes que exigem respostas r√°pidas.",
        "retrieval_context": [
            ", in which thresholds are learned via simple evaluation on heldout validation data.\nMonitoring and Testing. Unit testing of individual components and end-to-end tests of running\nsystems are valuable, but in the face of a changing world such tests are not sufÔ¨Åcient to provide\nevidence that a system is working as intended. Comprehensiv e live monitoring of system behavior\nin real time combined with automated response is critical fo r long-term system reliability.\nThe key question is: what to monitor? Testable invariants ar e not always obvious given that many\nML systems are intended to adapt over time. We offer the follo wing starting points.\n‚Ä¢Prediction Bias. In a system that is working as intended, it should usually be t he case that\nthe distribution of predicted labels is equal to the distrib ution of observed labels. This is\nby no means a comprehensive test, as it can be met by a null mode l that simply predicts",
            "the distribution of predicted labels is equal to the distrib ution of observed labels. This is\nby no means a comprehensive test, as it can be met by a null mode l that simply predicts\naverage values of label occurrences without regard to the in put features. However, it is a\nsurprisingly useful diagnostic, and changes in metrics suc h as this are often indicative of\nan issue that requires attention. For example, this method c an help to detect cases in which\nthe world behavior suddenly changes, making training distr ibutions drawn from historical\ndata no longer reÔ¨Çective of current reality. Slicing predic tion bias by various dimensions\nisolate issues quickly, and can also be used for automated al erting.\n‚Ä¢Action Limits. In systems that are used to take actions in the real world, suc h as bidding\non items or marking messages as spam, it can be useful to set an d enforce action limits as a\nsanity check. These limits should be broad enough not to trig ger spuriously. If the system",
            "their control planes if at all possible.\nBecause external changes occur in real-time, response must also occur in real-time as well. Relying\non human intervention in response to alert pages is one strat egy, but can be brittle for time-sensitive\nissues. Creating systems to that allow automated response w ithout direct human intervention is often\nwell worth the investment.\n8 Other Areas of ML-related Debt\nWe now brieÔ¨Çy highlight some additional areas where ML-rela ted technical debt may accrue.\nData Testing Debt. If data replaces code in ML systems, and code should be tested , then it seems\nclear that some amount of testing of input data is critical to a well-functioning system. Basic sanity\nchecks are useful, as more sophisticated tests that monitor changes in input distributions.\n7\nReproducibility Debt. As scientists, it is important that we can re-run experiment s and get similar",
            "on values), P6 talked about monitoring completeness (i.e., fraction\nof non-null values) for features, P16 mentioned embedding their\npipelines with \"common sense checks,\" implemented as hard con-\nstraints on columns, and P8 described schema checks‚Äîmaking sure\neach data item adheres to an expected set of columns and their\ntypes.\nWhile rudimentary data checks were embedded in most systems,\nP6 discussed that it was hard to figure out what higher-order data\nchecks to compute:\nMonitoring is both metrics and then a predicate over\nthose metrics that triggers alerts. That second piece\ndoesn‚Äôt exist‚Äînot because the infrastructure is hard,\nbut because no one knows how to set those predicate\nvalues...for a lot of this stuff now, there‚Äôs engineering\nheadcount to support a team doing this stuff. This is\npeople‚Äôs jobs now; this constant, periodic evaluation of\nmodels.\nSome participants discussed using black-box data monitoring\nservices but lamented that their alerts did not prevent failures (P7,",
            "Setting up robust alerting and notification systems is essential to complement the monitoring efforts. These systems serve as an early warning mechanism, flagging any signs of performance degradation or emerging issues with the deployed models. By receiving timely alerts, data scientists and engineers can quickly investigate and address these concerns, minimizing their impact on the model's performance and the end-users' experience."
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.7928571428571429
            },
            {
                "gpt-oss:20b-cloud": 0
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.2647058823529412
            },
            {
                "llama3.2:latest": 0.6818181818181818
            },
            {
                "gpt-oss:20b-cloud": 0.0
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que √© valida√ß√£o cruzada?",
        "expected_output": "√â uma t√©cnica que divide os dados em m√∫ltiplos subconjuntos para avaliar melhor a generaliza√ß√£o.",
        "actual_output": "Nenhuma informa√ß√£o dispon√≠vel no contexto.",
        "retrieval_context": [
            "other systems. Without access controls, some of these consu mers may be undeclared , silently using\nthe output of a given model as an input to another system. In mo re classical software engineering,\nthese issues are referred to as visibility debt .\nUndeclared consumers are expensive at best and dangerous at worst, because they create a hidden\ntight coupling of model mato other parts of the stack. Changes to mawill very likely impact these\nother parts, potentially in ways that are unintended, poorl y understood, and detrimental. In practice,\nthis tight coupling can radically increase the cost and difÔ¨Å culty of making any changes to maat all,\neven if they are improvements. Furthermore, undeclared con sumers may create hidden feedback\nloops, which are described more in detail in section 4.\n2\nUndeclared consumers may be difÔ¨Åcult to detect unless the sy stem is speciÔ¨Åcally designed to guard",
            "ZoneId=3\u0000",
            "ZoneId=3\u0000",
            "different failure modes in the offline validation phase‚Äîe.g., perfor-\nmance drops in subpopulations users might care deeply about‚ÄîlikeP11 did, P8 offered a reactive strategy of spawning a new dataset\nfor each observed live failure: ‚ÄúEvery gets into\nthe same queue, and 3 of us sit down once a week and go through\nthe queue...then our collect more data.‚Äù This\nnew dataset was then used in the offline validation phase in future\niterations of the production ML lifecycle.\nWhile processes to dynamically update the validation datasets\nranged from human-in-the-loop to frequent synthetic data con-\nstruction (P6), we found that higher-stakes applications of ML (e.g.,\nautonomous vehicles), created separate teams to manage the dy-\nnamic evaluation process. P1 said:\nWe had to move away from only aggregate metrics like\nMAP towards the ability to curate scenarios of interest,\nand then validate model performance on them specifi-\ncally. So, as an example, you can‚Äôt hit pedestrians, right.",
            "(https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets#Holdout_dataset) to evaluate\nthe model quality. The output of this step is a set of metrics to assess the quality of\nthe model.\n6. Model validation: The model is confirmed to be adequate for deployment‚Äîthat its\npredictive performance is better than a certain baseline.\n7. Model serving: The validated model is deployed to a target environment to serve\npredictions. This deployment can be one of the following:\nMicroservices with a REST API to serve online predictions.\nAn embedded model to an edge or mobile device.\nPart of a batch prediction system.\n8. Model monitoring: The model predictive performance is monitored to potentially\ninvoke a new iteration in the ML process.\nThe level of automation of these steps defines the maturity of the ML process, which\nreflects the velocity of training new models given new data or training new models given"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.9166666666666666
            },
            {
                "gpt-oss:20b-cloud": 0
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.2608695652173913
            },
            {
                "llama3.2:latest": 0.5882352941176471
            },
            {
                "gpt-oss:20b-cloud": 0.058823529411764705
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.0
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 0.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 1.0
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 0.0
            }
        ]
    },
    {
        "input": "O que √© model drift detection?",
        "expected_output": "√â o processo automatizado de identificar mudan√ßas na distribui√ß√£o ou rela√ß√£o dos dados que afetam a performance do modelo.",
        "actual_output": "Model drift detection √© o processo de identificar quando um modelo de machine learning come√ßa a funcionar de maneira diferente do esperado ap√≥s sua implanta√ß√£o em produ√ß√£o. Isso acontece quando as caracter√≠sticas dos dados de entrada ou a rela√ß√£o entre eles mudam ao longo do tempo, fazendo com que o modelo produza resultados menos precisos ou imprecisos.\n\n### Principais tipos de drift:\n1. **Drift de dados (Data drift)**: Quando as caracter√≠sticas dos dados de entrada mudam, como formatos incorretos ou defini√ß√µes inconsistentes de palavras (como mencionado no contexto).\n2. **Drift de conceito (Concept drift)**: Quando a rela√ß√£o entre as caracter√≠sticas e o objetivo do modelo muda, por exemplo, devido a mudan√ßas no comportamento dos usu√°rios ou no ambiente.\n3. **Drift de precis√£o (Performance drift)**: Quando o desempenho do modelo come√ßa a deteriorar devido a mudan√ßas nos dados ou no conceito.\n\n### Como funciona a detec√ß√£o:\n- **Monitoramento cont√≠nuo**: Sistemas de monitoramento avaliam regularmente o desempenho do modelo, verificando mudan√ßas na precis√£o, erros ou comportamento inesperado.\n- **Alertas**: Quando um drift √© detectado, o sistema pode enviar alertas para que o modelo seja reavaliado ou atualizado.\n\n### Consequ√™ncias:\n- Modelos desatualizados podem gerar decis√µes ruins, como recomenda√ß√µes incorretas ou previs√µes imprecisas.\n- A detec√ß√£o ajuda a evitar esses problemas, garantindo que o modelo continue eficiente ap√≥s a implanta√ß√£o.\n\nEm resumo, a model drift detection √© essencial para manter a precis√£o e a confiabilidade dos modelos de machine learning no longo prazo! üòä",
        "retrieval_context": [
            "participants cited Covid as an example, but there are other\n(better) everyday instances of unnatural data drift. P6 de-\nscribed a bug where users had inconsistent definitions of the\nsame word, complicating the deployment of a service to a\nnew user. P7 mentioned a bug where data from users in a\ncertain geographic region arrived more sporadically than\nusual. P10 discussed a bug where the format of raw data was\noccasionally corrupted: ‚ÄúTables didn‚Äôt always have headers\nin the same place, even though they were the same tables.‚Äù\n‚Ä¢Natural data drift: Surprisingly, participants didn‚Äôt seem\ntoo worried about slower, expected natural data drift over\ntime‚Äîthey noted that frequent model retrains solved this\nproblem (P6, P7, P8, P12, P15, P16, P17). As an anecdote, we\nasked P17 to give an example of a natural data drift problem\ntheir company faced, and they could not think of a good\nexample. P14 also said they don‚Äôt have natural data drift\nproblems:",
            "asked P17 to give an example of a natural data drift problem\ntheir company faced, and they could not think of a good\nexample. P14 also said they don‚Äôt have natural data drift\nproblems:\n5Goldilocks and the Three Bears is a popular Western fairy tale. Goldilocks, the main\ncharacter, looks for things that are not too big or not too small, things that are ‚Äújust\nright.‚Äù\nOperationalizing Machine Learning: An Interview Study\nThe model gets retrained every day, so we don‚Äôt have the\nscenario of like: Oh, our models got stale and we need to re-\ntrain it because it‚Äôs starting to make mistakes because data\nhas drifted...fortunately we‚Äôve never had to deal with [such\na] scenario. Sometimes there are bad jobs, but\nwe can always effectively roll back to a different .\nHowever, a few engineers mentioned that natural data shift\ncould cause some hand-curated features and data quality\nchecks to corrupt (P3, P6, P8). P6 discussed a histogram used",
            "dation system has on click-throughs and on conversation rates. The results of online experimentation should be \nintegrated with the model registry capability to facilitate the decision about releasing the model to production. Online \nexperimentation enhances the reliability of your ML releases by helping you decide to discard ill-performing models \nand to promote well-performing ones. Key functionalities in online experimentation include the following:\n‚Ä¢ Support canary and shadow deployments.\n‚Ä¢ Support traffic splitting and A/B tests.\n‚Ä¢ Support multi-armed bandit (MAB) tests.\nModel monitoring\nThe model monitoring capability lets you track the efficiency and effectiveness of the deployed models in production \nto ensure predictive quality and business continuity. This capability informs you if your models are stale and need to \nbe investigated and updated. Key functionalities in model monitoring include the following:",
            "Continuous monitoring of model performance for accuracy drift, bias and other potential issues plays a critical role in maintaining the effectiveness of models and preventing unexpected outcomes. Monitoring the performance and health of ML models ensures that they continue to meet the intended objectives after deployment. By proactively identifying and addressing these concerns, organizations can maintain optimal model performance, mitigate risks and adapt to changing conditions or feedback.",
            "A pivotal aspect of MLOps is the maintenance and management of data, models and code. By maintaining distinct versions of these components, teams can effectively keep aware of changes over time, which is essential for troubleshooting issues, ensuring reproducibility of results and facilitating easier rollbacks when necessary. This approach aids in maintaining the integrity of the development process and enables auditability in ML projects.\n\nMonitoring the performance and health of ML models is critical to ensure that they continue to meet the intended objectives after deployment. This process involves regularly assessing for model drift, bias and other potential issues that could compromise their effectiveness. By proactively identifying and addressing these concerns, organizations can maintain optimal model performance, mitigate risks and adapt to changing conditions or feedback."
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.9166666666666666
            },
            {
                "gpt-oss:20b-cloud": null,
                "logs": "1 validation error for Verdicts\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"verdicts\"...n.\"\\n    }\\n  ]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid"
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.2962962962962963
            },
            {
                "llama3.2:latest": 0.7096774193548387
            },
            {
                "gpt-oss:20b-cloud": 0.3870967741935484
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 1.0
            },
            {
                "llama3.2:latest": 0.5714285714285714
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.75
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "Qual a import√¢ncia de um lineage tracker em MLOps?",
        "expected_output": "Lineage tracking permite rastrear dados, modelos, experimentos e artefatos para auditoria e reprodutibilidade.",
        "actual_output": "Ol√°! üòä Um **lineage tracker** (ou rastreador de linhagem) √© uma funcionalidade essencial dentro de um **ML Ops (MLOps)** e tem uma import√¢ncia crucial para o sucesso de projetos de Machine Learning. Vamos entender melhor!\n\n### üß™ O que √© um Lineage Tracker?\nEm termos simples, o **lineage tracker** √© como um \"genealogista\" do seu projeto de Machine Learning. Ele registra e rastreia o **caminho completo** de um dado ou artefato, desde sua origem at√© seu uso final. Isso inclui:\n\n- **Dados**: Onde os dados vieram, por quem foram modificados e como foram transformados.\n- **Modelos**: Como os modelos foram constru√≠dos, quais dados foram usados e qual foi o resultado.\n- **Experimentos**: Todas as etapas do desenvolvimento do modelo, desde a cria√ß√£o at√© a valida√ß√£o.\n\n### üåü Por que √© t√£o importante?\n\n1. **Reprodutibilidade** üîÑ  \n   Sem um lineage tracker, seria imposs√≠vel saber exatamente **como chegamos a um resultado**. Se voc√™ quiser reexecutar um experimento ou corrigir um problema, o tracker mostra exatamente o que foi usado antes.\n\n2. **Depura√ß√£o** üïµÔ∏è‚Äç‚ôÇÔ∏è  \n   Quando algo vai errado (por exemplo, o modelo come√ßa a dar resultados ruins), o lineage tracker ajuda a entender **onde o problema come√ßou**. Ele mostra o hist√≥rico completo do modelo e dos dados.\n\n3. **Governan√ßa e Conformidade** üßæ  \n   Em muitos setores (como sa√∫de, finan√ßas ou varejo), √© obrigat√≥rio seguir regras de conformidade. O lineage tracker ajuda a demonstrar **como os dados foram tratados** e como os modelos foram constru√≠dos, o que √© essencial para auditorias.\n\n4. **Colabora√ß√£o** üë•  \n   Em um time de Data Science, muitas pessoas trabalham juntas. O lineage tracker **centraliza a informa√ß√£o** e evita que cada um \"invente\" sua pr√≥pria forma de documentar os projetos.\n\n5. **Integra√ß√£o com o MLOps** üõ†Ô∏è  \n   O lineage tracker √© uma pe√ßa fundamental do MLOps, pois conecta todas as etapas do projeto (de dados at√© o modelo final) em um √∫nico lugar, facilitando a implementa√ß√£o e o monitoramento.\n\n### ‚úÖ Resumindo:\nUm **lineage tracker** √© como o \"DNI\" (Documento Nacional de Identidade) do seu projeto de Machine Learning. Ele garante que voc√™ saiba **exatamente o que aconteceu**, **quando**, e **como**. Isso √© essencial para garantir que seus modelos sejam confi√°veis, audit√°veis e reprodut√≠veis!\n\nSe voc√™ quiser, posso explicar melhor algum aspecto espec√≠fico! üòä",
        "retrieval_context": [
            "artifacts, including their location, types, properties, and associations to experiments and runs. The ML metadata and \nartifact tracking capability is foundational to all other MLOps capabilities. Such a capability enables reproducibility \nand debugging of complex ML tasks and pipelines. Key functionalities in ML metadata and artifact tracking include \nthe following:\n‚Ä¢ Provide traceability and lineage tracking of ML artifacts.\n‚Ä¢ Share and track experimentation and pipeline parameter configurations.\n‚Ä¢ Store, access, investigate, visualize, download, and archive ML artifacts.\n‚Ä¢ Integrate with all other MLOps capabilities.\nDeep dive of MLOps processes\nThis section describes each of the core MLOps processes in detail. It describes key tasks and flow of control be -\ntween tasks, the key artifacts created by the tasks, and the relationship of tasks to other upstream and downstream",
            "bridge the gap between machine learning development and the\nimplementation of ML systems in a production environment. The platform\nhelps streamline the process of building, deploying, and monitoring\nmodels, by providing a standardised and automated workflow.\nThe ML Ops platform typically includes multiple components such as:11/11/25, 9:50 PM MLOps Now - The MLOps Platform: Revolutionising Machine Learning Efficiency\nhttps://mlopsnow.com/blog/mlops-platforms-revolutionising-machine-learning/ 2/10\nAutomat ed Training : Automating the training of machine learning\nmodels on a scheduled basis to keep them updated with fresh data.\nModel V ersioning : Keeping track of different versions of models and\nsimplifying the management of those models.\nContinuous Int egration and Deployment : Ensuring continuous\nintegration (CI) and automatic deployment of ML models in the\nproduction environment.\nMonit oring : Tracking model performance, identifying drifts, and\nproviding alerts for potential issues.",
            "‚Ä¢ Support various data modalities, including tabular data, images, and text.\nML data assets can be managed at the entity features level or at the full dataset level. For example, a feature reposi -\ntory might contain an entity called customer, which includes features like age group, postal code, and gender. On the \nother hand, a dataset repository might include a customer churn dataset, which includes features from the customer \nand product entities, as well as purchase- and web-activity event logs.\nML metadata and artifact tracking\nVarious types of ML artifacts are produced in different processes of the MLOps lifecycle, including descriptive \nstatistics and data schemas, trained models, and evaluation results. ML metadata is the information about these \nartifacts, including their location, types, properties, and associations to experiments and runs. The ML metadata and",
            "Proper data management in an ML Ops platform ensures that datasets are:\nCleaned and preprocessed efficiently.\nEasily accessible and shareable among users or teams.\nVersion-controlled to keep track of changes and updates.\nCompliance and Monit oring\nCompliance and monitoring play crucial roles in maintaining governance\nand security in ML Ops platforms. Monitoring tools provide essential\ninsights to detect and mitigate potential security threats. These tools help\nin:\nIdentifying vulnerabilities and generating alerts in real-time.\nEnsuring data protection and regulatory compliance.\nTracking user activities, thus allowing auditing and accountability.\nMLOps platforms should implement policies and best practices to maintain\ncompliance with data protection laws and industrial regulations. This11/11/25, 9:50 PM MLOps Now - The MLOps Platform: Revolutionising Machine Learning Efficiency\nhttps://mlopsnow.com/blog/mlops-platforms-revolutionising-machine-learning/ 7/10",
            "processing, model training, model evaluation, model serving, online experimentation, model monitoring, ML pipeline, \nand model registry. Finally, two cross-cutting capabilities that enable integration and interaction are an ML metadata \nand artifact repository and an ML dataset and feature repository.\nFigure 4. Core MLOps technical capabilities\n11\nThe following sections outline the characteristics of each of the MLOps capabilities.\nExperimentation \nThe experimentation capability lets your data scientists and ML researchers collaboratively perform exploratory data \nanalysis, create prototype model architectures, and implement training routines. An ML environment should also let \nthem write modular, reusable, and testable source code that is version controlled. Key functionalities in experimenta -\ntion include the following:\n‚Ä¢ Provide notebook environments that are integrated with version control tools like Git."
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.6263625263625263
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": null,
                "logs": "1 validation error for Verdicts\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"verdicts\"...y.\"\\n    }\\n  ]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid"
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.45652173913043476
            },
            {
                "llama3.2:latest": 0.56
            },
            {
                "gpt-oss:20b-cloud": 0.4411764705882353
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.6666666666666666
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 0.9666666666666667
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.46153846153846156
            },
            {
                "llama3.2:latest": 0.6666666666666666
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que √© continuous training (CT)?",
        "expected_output": "CT √© a pr√°tica de treinar novamente modelos automaticamente com novos dados conforme regras definidas.",
        "actual_output": "O **Continuous Training (CT)** √© uma propriedade √∫nica das **MLops** que se refere √† automa√ß√£o do processo de **reten√ß√£o e servir dos modelos**. \n\nEle funciona da seguinte forma:\n\n1. **Execu√ß√£o repetida**: O pipeline de treinamento √© executado repetidamente, sempre que houver um gatilho, como:\n   - Nova disponibilidade de dados.\n   - Detec√ß√£o de decaimento do desempenho do modelo.\n   - Mudan√ßas no c√≥digo ou configura√ß√µes.\n\n2. **Sa√≠da**: O resultado √© um novo modelo reentrenado, que √© servido para operar em produ√ß√£o.\n\nEm resumo, o **CT** garante que os modelos sejam atualizados continuamente para manterem seu desempenho no ambiente real. üåü",
        "retrieval_context": [
            "flow:\n1. The core activity during this ML development phase is experimentation. As data scientists and ML research -\ners prototype model architectures and training routines, they create labeled datasets, and they use features \nand other reusable ML artifacts that are governed through the data and model management process. The \nFigure 3. The MLOps process\n9\nprimary output of this process is a formalized training procedure, which includes data preprocessing, model \narchitecture, and model training settings. \n2. If the ML system requires continuous training (repeated retraining of the model), the training procedure is \noperationalized as a training pipeline. This requires a CI/CD routine to build, test, and deploy the pipeline to \nthe target execution environment.\n3. The continuous training pipeline is executed repeatedly based on retraining triggers, and it produces a model \nas output. The model is retrained as new data becomes available, or if model performance decay is detected.",
            "testing, integration testing, and continuous delivery of the software module or the package.\nHowever, in ML, there are a few notable differences:\nCI is no longer only about testing and validating code and components, but also\ntesting and validating data, data schemas, and models.\nCD is no longer about a single software package or a service, but a system (an ML\ntraining pipeline) that should automatically deploy another service (model prediction\nservice).\nCT is a new property, unique to ML systems, that's concerned with automatically\nretraining and serving the models.\nThe following section discusses the typical steps for training and evaluating an ML model\nto serve as a prediction service.\nData science steps for ML\nIn any ML project, after you define the business use case and establish the success criteria,\nthe process of delivering an ML model to production involves the following steps. These\nsteps can be completed manually or can be completed by an automatic pipeline.",
            "The processes can consist of the following:\n‚Ä¢ ML development concerns experimenting and developing a robust and reproducible model training proce -\ndure (training pipeline code), which consists of multiple tasks from data preparation and transformation to \nmodel training and evaluation.\n‚Ä¢ Training operationalization concerns automating the process of packaging, testing, and deploying repeat -\nable and reliable training pipelines.\n‚Ä¢ Continuous training concerns repeatedly executing the training pipeline in response to new data or to code \nchanges, or on a schedule, potentially with new training settings.\n‚Ä¢ Model deployment concerns packaging, testing, and deploying a model to a serving environment for online \nexperimentation and production serving.\nFigure 2 . The MLOps lifecycle\n8\n‚Ä¢ Prediction serving is about serving the model that is deployed in production for inference.\n‚Ä¢ Continuous monitoring is about monitoring the effectiveness and efficiency of a deployed model.",
            "Continuous integration\nIn this setup, the pipeline and its components are built, tested, and packaged when new\ncode is committed or pushed to the source code repository. Besides building packages,\ncontainer images, and executables, the CI process can include the following tests:\nUnit testing your feature engineering logic.\nUnit testing the different methods implemented in your model. For example, you have\na function that accepts a categorical data column and you encode the function as a\none-hot¬†(https://en.wikipedia.org/wiki/One-hot) feature.\nTesting that your model training converges (that is, the loss of your model goes down\nby iterations and overfits¬†(https://en.wikipedia.org/wiki/Overfitting) a few sample records).\nTesting that your model training doesn't produce NaN¬†(https://en.wikipedia.org/wiki/NaN)\nvalues due to dividing by zero or manipulating small or large values.\nTesting that each component in the pipeline produces the expected artifacts.",
            "pipeline; this lets you achieve continuous delivery of model prediction service. To automate\nthe process of using new data to retrain models in production, you need to introduce\nautomated data and model validation steps to the pipeline, as well as pipeline triggers and\nmetadata management.\nThe following figure is a schematic representation of an automated ML pipeline for CT.11/13/25, 11:39 PM MLOps: Continuous delivery and automation pipelines in machine learning | Cloud Architecture Center | Google Cloud Do‚Ä¶\nhttps://docs.cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning 8/18\nFigure 3. ML pipeline automation for CT.\nCharacteristics\nThe following list highlights the characteristics of the MLOps level 1 setup, as shown in\nFigure 3:\nRapid experiment: The steps of the ML experiment are orchestrated. The transition\nbetween steps is automated, which leads to rapid iteration of experiments and better"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.7555555555555555
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 0.95
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.23333333333333334
            },
            {
                "llama3.2:latest": 0.6666666666666666
            },
            {
                "gpt-oss:20b-cloud": 0.25925925925925924
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.7142857142857143
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.625
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 0.8333333333333334
            }
        ]
    },
    {
        "input": "Como funciona um sistema de AB testing para modelos?",
        "expected_output": "O sistema divide tr√°fego entre modelos diferentes para comparar performance estatisticamente.",
        "actual_output": "Um sistema de A/B testing para modelos funciona da seguinte forma:\n\n1. **Introdu√ß√£o Gradual**: Voc√™ implementa o novo modelo ao lado do modelo antigo, mas n√£o o substitui imediatamente. Isso permite comparar o desempenho do novo modelo com o existente em condi√ß√µes reais.\n\n2. **Coleta de Dados**: Durante o per√≠odo de teste, voc√™ coleta m√©tricas relevantes para ambos os modelos. Isso inclui precis√£o, recall, taxa de erro, e outros indicadores que s√£o relevantes para o seu caso de uso.\n\n3. **An√°lise Estat√≠stica**: Ap√≥s coletar dados suficientes, voc√™ realiza uma an√°lise estat√≠stica para determinar se as diferen√ßas no desempenho entre os modelos s√£o significativas. Ferramentas como testes t ou an√°lise de vari√¢ncia podem ser √∫teis aqui.\n\n4. **Decis√£o**: Com base nos resultados, voc√™ decide se o novo modelo √© melhor que o antigo. Se os resultados forem favor√°veis, voc√™ pode implementar o novo modelo completamente. Caso contr√°rio, voc√™ pode manter o modelo antigo ou fazer ajustes no novo modelo e repetir o processo.\n\n5. **Monitoramento Cont√≠nuo**: Ap√≥s a implementa√ß√£o, √© importante continuar monitorando o desempenho do modelo para garantir que ele mantenha a qualidade esperada ao longo do tempo.\n\nO A/B testing √© uma abordagem poderosa para garantir que os modelos selecionados para produ√ß√£o realmente funcionem bem no mundo real, minimizando o risco de falhas ap√≥s o deploy.",
        "retrieval_context": [
            "Typical assets produced in this \nprocess include the following:\n‚Ä¢ Notebooks for experimentation \nand visualization\n‚Ä¢ Metadata and artifacts of the \nexperiments\n‚Ä¢ Data schemas\n‚Ä¢ Query scripts for the training data\n‚Ä¢ Source code and configurations for \ndata validation and transformation\n‚Ä¢ Source code and configurations for \ncreating, training, and evaluating \nmodels\n‚Ä¢ Source code and configurations for \nthe training-pipeline workflow\n‚Ä¢ Source code for unit tests and \nintegration tests\nCore MLOps capabilities:\n‚Ä¢ Dataset & feature repository\n‚Ä¢ Data processing\n‚Ä¢ Experimentation\n‚Ä¢ Model training\n‚Ä¢ Model registry\n‚Ä¢ ML metadata & artifact repository\n19\nA pipeline typically goes through a series of testing and staging environ -\nments before it is released to production. The number of testing and stag -\ning environments varies depending on standards that are established in a \ngiven organization. Most organizations have at least one testing environ -\nment before production; some have more.",
            "Data changes\nModel training code changes\nApplication code changes.\nAutomated testing helps you discover problems early for fast error Ô¨Åxes and learnings. Automation is more\neÔ¨Écient with infrastructure as code (IaC). You can use tools to deÔ¨Åne and manage infrastructure. This helps\nensure it's reproducible and can be consistently deployed across various environments.\nRead about IaC ¬ª\nContinuous X\nThrough automation, you can continuously run tests and deploy code across your ML pipeline.\nIn MLOps, continuous refers to four activities that happen continuously if any change is made anywhere in\nthe system:\nContinuous integration extends the validation and testing of code to data and models in the pipeline\nContinuous delivery automatically deploys the newly trained model or model prediction service\nContinuous training automatically retrains ML models for redeployment\nContinuous monitoring concerns data monitoring and model monitoring using metrics related to\nbusiness\nModel governance",
            "testing, integration testing, and continuous delivery of the software module or the package.\nHowever, in ML, there are a few notable differences:\nCI is no longer only about testing and validating code and components, but also\ntesting and validating data, data schemas, and models.\nCD is no longer about a single software package or a service, but a system (an ML\ntraining pipeline) that should automatically deploy another service (model prediction\nservice).\nCT is a new property, unique to ML systems, that's concerned with automatically\nretraining and serving the models.\nThe following section discusses the typical steps for training and evaluating an ML model\nto serve as a prediction service.\nData science steps for ML\nIn any ML project, after you define the business use case and establish the success criteria,\nthe process of delivering an ML model to production involves the following steps. These\nsteps can be completed manually or can be completed by an automatic pipeline.",
            "Continuous Integration.¬†\nMonit oring identifies model drif t over time. Without model monitoring,\nproduction systems are flying blind. By monitoring for model drift the data\nscience team is able to proactively work rather than reactively.¬†\nTesting ensur es the accuracy and r eliability o f models. Validating both\nthe model‚Äôs predictions and the data sets used is a fundamental step in\ngreenlighting models for production.¬†\nUse A/B t esting t o identif y best models. A/B testing is sometimes\noverlooked in Machine Learning but is a great way to introduce new\nmodels. Rather than swapping models out straight away you can introduce\nthe new model alongside the old. This weighted approach allows you to\nsee the efficacy of the new model in production before committing to it.\n4. Version Contr ol\nVersion control is a significant aspect of ML Ops. It allows teams to track",
            "Verifying that models meet the predictive performance targets before they are\ndeployed.\nAutomated deployment to a test environment, for example, a deployment that is\ntriggered by pushing code to the development branch.\nSemi-automated deployment to a pre-production environment, for example, a\ndeployment that is triggered by merging code to the main branch after reviewers\napprove the changes.\nManual deployment to a production environment after several successful runs of the\npipeline on the pre-production environment.\nTo summarize, implementing ML in a production environment doesn't only mean deploying\nyour model as an API for prediction. Rather, it means deploying an ML pipeline that can\nautomate the retraining and deployment of new models. Setting up a CI/CD system lets you\nautomatically test and deploy new pipeline implementations. This system lets you cope\nwith rapid changes in your data and business environment. You don't have to immediately"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.8083333333333332
            },
            {
                "gpt-oss:20b-cloud": 0.25
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.4666666666666667
            },
            {
                "llama3.2:latest": 0.6666666666666666
            },
            {
                "gpt-oss:20b-cloud": 0.08695652173913043
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.8
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.6153846153846154
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    },
    {
        "input": "O que √© um modelo champion vs challenger?",
        "expected_output": "O modelo champion √© o atual em produ√ß√£o, enquanto challengers s√£o avaliados como potenciais substitutos.",
        "actual_output": "Ol√°! O conceito de **modelo Champion vs Challenger** √© uma estrat√©gia comum no Machine Learning para gerenciar a introdu√ß√£o e valida√ß√£o de novos modelos antes de substituir o modelo atual.\n\n### O que s√£o?\n\n*   **Modelo Champion (Campe√£o):**\n    *   √â o modelo atualmente em produ√ß√£o, usado para fazer as previs√µes no sistema principal.\n    *   √â o \"modelo de refer√™ncia\" ou o modelo que est√° sendo usado para tomar decis√µes no dia a dia.\n    *   Geralmente, √© um modelo est√°vel e confi√°vel, mas pode ser antigo ou n√£o estar mais otimizado.\n\n*   **Modelo Challenger (Desafiante):**\n    *   √â um novo modelo que est√° sendo desenvolvido ou testado para substituir o Champion.\n    *   O objetivo do Challenger √© demonstrar que ele tem melhor desempenho ou que √© mais adequado para o problema em quest√£o.\n    *   O Champion permanece em produ√ß√£o at√© que o Challenger seja oficialmente aprovado e substitu√≠do.\n\n### Como funciona?\n\n1.  **Introdu√ß√£o do Challenger:** O novo modelo (Challenger) √© implementado, mas ainda n√£o √© usado diretamente no sistema principal. Ele faz previs√µes paralelamente ao Champion.\n2.  **Monitoramento:** As previs√µes dos dois modelos s√£o comparadas continuamente (geralmente usando m√©tricas relevantes para o neg√≥cio, como precis√£o, recall, ou impacto direto nos resultados).\n3.  **Avalia√ß√£o:** Se o Challenger demonstrar consistentemente melhor desempenho do que o Champion, ele pode ser promovido.\n4.  **Promo√ß√£o:** O Challenger substitui o Champion no sistema principal. O modelo antigo (Champion) pode ser mantido como um hist√≥rico ou usado para refer√™ncia.\n5.  **Novo Challenger:** Se um novo modelo melhor for desenvolvido, ele se torna o novo Challenger, e o modelo atual Champion passa por esse processo novamente.\n\n### Por que √© √∫til?\n\n*   **Garante que o modelo em produ√ß√£o √© o melhor poss√≠vel:** N√£o h√° necessidade de mudar o modelo em produ√ß√£o diretamente, o que pode ser arriscado e complicado (como mencionado no contexto sobre \"deadlock\" e custos de corre√ß√£o cascada).\n*   **Reduz o risco:** Se o novo modelo (Challenger) n√£o funcionar como esperado, o sistema continua operando com o modelo Champion sem interrup√ß√£o.\n*   **Permite testes controlados:** √â poss√≠vel validar o desempenho do novo modelo em condi√ß√µes reais antes de cometer mudan√ßas na produ√ß√£o.\n\nEm resumo, a abordagem Champion vs Challenger √© uma forma segura e eficaz de introduzir e validar novos modelos no ambiente de produ√ß√£o, minimizando os riscos associados a mudan√ßas diretas.",
        "retrieval_context": [
            "more expensive to analyze improvements to that model in the f uture. The cost increases when\ncorrection models are cascaded, with a model for problem A‚Ä≤‚Ä≤learned on top of m‚Ä≤\na, and so on,\nfor several slightly different test distributions. Once in place, a correction cascade can create an\nimprovement deadlock, as improving the accuracy of any indi vidual component actually leads to\nsystem-level detriments. Mitigation strategies are to aug mentmato learn the corrections directly\nwithin the same model by adding features to distinguish amon g the cases, or to accept the cost of\ncreating a separate model for A‚Ä≤.\nUndeclared Consumers. Oftentimes, a prediction from a machine learning model mais made\nwidely accessible, either at runtime or by writing to Ô¨Åles or logs that may later be consumed by\nother systems. Without access controls, some of these consu mers may be undeclared , silently using\nthe output of a given model as an input to another system. In mo re classical software engineering,",
            "other systems. Without access controls, some of these consu mers may be undeclared , silently using\nthe output of a given model as an input to another system. In mo re classical software engineering,\nthese issues are referred to as visibility debt .\nUndeclared consumers are expensive at best and dangerous at worst, because they create a hidden\ntight coupling of model mato other parts of the stack. Changes to mawill very likely impact these\nother parts, potentially in ways that are unintended, poorl y understood, and detrimental. In practice,\nthis tight coupling can radically increase the cost and difÔ¨Å culty of making any changes to maat all,\neven if they are improvements. Furthermore, undeclared con sumers may create hidden feedback\nloops, which are described more in detail in section 4.\n2\nUndeclared consumers may be difÔ¨Åcult to detect unless the sy stem is speciÔ¨Åcally designed to guard",
            "Continuous Integration.¬†\nMonit oring identifies model drif t over time. Without model monitoring,\nproduction systems are flying blind. By monitoring for model drift the data\nscience team is able to proactively work rather than reactively.¬†\nTesting ensur es the accuracy and r eliability o f models. Validating both\nthe model‚Äôs predictions and the data sets used is a fundamental step in\ngreenlighting models for production.¬†\nUse A/B t esting t o identif y best models. A/B testing is sometimes\noverlooked in Machine Learning but is a great way to introduce new\nmodels. Rather than swapping models out straight away you can introduce\nthe new model alongside the old. This weighted approach allows you to\nsee the efficacy of the new model in production before committing to it.\n4. Version Contr ol\nVersion control is a significant aspect of ML Ops. It allows teams to track",
            "this phase, data engineers work together with data scientists to prepare\nand preprocess the data, performing featur e engineering to ensure the\ndata has the right format and structure.\nDuring model creation, various data pipelines are developed, enabling the\nsmooth flow of information between the different stages of the machine\nlearning process. T ools such as data engineering platforms can be used to\ndesign, test and maintain these pipelines.\nModel T raining\nOnce the model has been created, it is trained using a suitable dataset.\nModel training is an iterative process that involves feeding data into the\nmodel for it to learn and make predictions. The model is continually\nadjusted, and its performance is evaluated against a validation dataset to\nfine-tune its accuracy and effectiveness.\nSeveral techniques can be applied during the model training phase,\nincluding hyperparameter optimisation, cross-validation, and\nregularisation. Utilising the right combination of these methods helps",
            "Following the training phase, model evaluation is conducted to assess the performance of the models on unseen data. Evaluation is critical to ensure that the models perform well in real-world scenarios. Metrics such as accuracy, precision, recall and fairness measures gauge how well the model meets the project objectives. These metrics provide a quantitative basis for comparing different models and selecting the best one for deployment. Through careful evaluation, data scientists can identify and address potential issues, such as bias or overfitting, ensuring that the final model is effective and fair.\n\nModel deployment"
        ],
        "context_precision_score": [
            {
                "llama3.1:latest": 0.5
            },
            {
                "llama3.2:latest": 0.9166666666666666
            },
            {
                "gpt-oss:20b-cloud": null,
                "logs": "1 validation error for Verdicts\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"verdicts\"...n.\"\\n    }\\n  ]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid"
            }
        ],
        "context_relevancy_score": [
            {
                "llama3.1:latest": 0.21621621621621623
            },
            {
                "llama3.2:latest": 0.7586206896551724
            },
            {
                "gpt-oss:20b-cloud": 0.1111111111111111
            }
        ],
        "answer_relevancy_score": [
            {
                "llama3.1:latest": 0.6875
            },
            {
                "llama3.2:latest": 0.5
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ],
        "faithfulness_score": [
            {
                "llama3.1:latest": 0.5384615384615384
            },
            {
                "llama3.2:latest": 1.0
            },
            {
                "gpt-oss:20b-cloud": 1.0
            }
        ]
    }
]